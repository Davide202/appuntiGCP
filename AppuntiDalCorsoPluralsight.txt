GOOGLE CLOUD PLATFORM FUNDAMENTALS - CORE INFRASTUCTURE
	
	ORGANIZZAZIONE
Le risorse sono organizzate gerarchicamente in:
- org node: company
- folders
- projects: test, prod
- resources: vm, storage

Projects hanno tre identificativi:
1 - Project ID (unico globalmente, scelto, immutabile)
2 - Project Name (scelto, mutabile)
3 - Project Number (unico, immutabile)

	IAM
	who can do what on which resource???
who: google account, service account, google group, cloud identity or g suite domain
predefined roles: owner, editor, viewer, billing administrator
custom roles
oss: i ruoli dipendono dal tipo di risorsa a cui ci stiamo riferendo

il service account permette di accedere ad una risorsa da una vm

	VPC
vpc è globale ed ha subnetwork regionali
ci sono 5 tipi di load balancing:
1 global http(s), based on load, can route differente urls to differente back ends
2 global ssl proxy, 
3 global tcp proxy 
4 regional 
5 regiona internal 

Interconnection  options:
vpn, secure multi-gbps connection over vpn tunnels
direct peering, private connection between you and google for your hybrid cloud workloads
carrier peering, connection through the largest partner network of service providers
dedicated interconnect, connect N x 10G transport circuits for private cloud traffic to google cloud at google pops

	COMPUTE ENGINE
	
- Creare una VM dalla cloud shell
per vedere la lista delle zone:
>gcloud compute zones list | grep us-central1
per settare una zona:
>gcloud config set compute/zone us-central1-c
creare una vm
>gcloud compute instances create "my-vm-2" \
>--machine-type "n1-standard-1" \
>--image-project "debian-cloud" \
>--image "debian-9-stretch-v20170918" \
>--subnet "default"

la vm così creata avrà ip interno ed esterno
nel lab entra in ssh di vm1
>ssh my-vm-1
e fa il ping della vm2:
>ping my-vm-2
e viceversa

	STORAGE
	
	Cloud Storage: binary large-object storage
ogni oggetto ha una url
gli oggetti non si possono modificare ma solo creare, scaricare, cancellare e versionare
cloud storage è organizzato in buckets
che hanno nome unico e possono essere controllati con IAM policies
è possibile definire un life cycle per gli oggetti
oss: se il versionamento non è abilitato, l'upload sovrascrive
ci sono 4 classi: (CLOUD STORAGE CLASSES)
- multi-regional per accesso frequente
- regional per accesso frequente dalla stessa region
- nearline per una frequenza di accesso inferiore ad 1accesso/mese (backups)
- coldline per una frequenza di accesso inferiore ad 1accesso/anno (archiving, disaster recovery)
il costo è decrescente per gb, crescente per utilizzo dei trasferimenti.
Cloud Storage è integrato con tutti gli altri servizi:
è possibile importare ed esportare tables di BigQuery
startup scripts, images, and general object of Compute Engine
object storage, logs, datastore backups of App Engine 
import and export tables of Cloud SQL 
e può essere gestito dall'utente.

	Cloud BigTable is managed NoSQL
wide-column database service for  terabyte applications
è ideale per analytical applications, iot, user analytics, finacial data analysis
native compatibility with big data, hadoop ecosystems
è possibile accedere ai dati utilizzando Application API, 
può ricevere i dati in stream
da processi batch.

	Cloud SQL is managed RDBMS
offers mysql and postgresql databases as a service
ammette le transazioni
automatic replication
managed backups
vertical (read and write) and horizontal scaling (read)
google security
è possibile utilizzarlo con Compute Engine e con App Engine e con servizi esterni.

	Cloud Spanner is a horizonatally scalable RDBMS
Strong global consistency
managed instances with high availability
SQL queries
Automatic replication
transactional consistency

	Cloud Datastore is a horizonatally scalable NoSQL DB
Designed for application backends
Support transactions (BigTable NO)
Includes a free daily quota
SQL-like queries



	COMPARING  STORAGE SERVICES 
	
Cloud Datastore:	NoSQL document	
					Transactions
					NO Complex queries
					Terabytes+ of Capacity
					Unit size: 1MB/entity
					best for: semi-structured application data, durable key-value data
					use cases: getting started, App Engine applications
Cloud Bigtable:	NoSQL wide column
				transactions: single-row
				No complex queries
				Petabytes+ of capacity
				Unit size: 10MB/cell, 100MB/row
				best for: "flat" data, heavy read/write, events, analytical data
				use cases: AdTech, Financial and IoT data
Cloud Storage:	immutable object / Blobstore
				No transactions
				No complex queries
				Petabytes+ of capacity
				Unit size: 5TB/object
				best for: structured and unstructured binary or object data
				use cases: images, large media files, backups.
Cloud SQL:	Relatioal SQL for OLTP
			Transactions
			Complex queries
			Terabytes of Capacity
			Unit size determined by DB engine
			best for: web frameworks, exixting applications
			use cases: user credentials, customer orders
Cloud Spanner:	Relatioal SQL for OLTP
				Transactions
				Complex queries
				Terabytes of capacity
				Unit size: 10MB10.240Mb/row
				best for: large-scale database applications (>2TB)
				use cases: whenever high I/O, global consistency is needed
BigQuery:	Relatioal SQL for OLAP
			NO Transactions
			Complex queries
			Petabytes of capacity
			Unit size: 10MB/row
	big data analysis and interactive querying capabilities
			best for: interactive querying, offline analytics
			use cases: data warehousing.
			
	CREARE UN BUCKET
dopo aver creato una VM linux con Compute Engine da UI, nella zona us-central1-a,
con script di installazione:
> apt-get update
apt-get install apache2 php-mysql -y
service apache2 restart

procediamo a creare un Cloud Storage bucket da Cloud Shell, multiregion:
utilizzando come nome del bucket il project id:
>gsutil mb -l US gs://$DEVSHELL_PROJECT_ID
copiamo un'immagine da un'altro bucket:
>gsutil cp gs://cloud-training/gcpfci/my-excellent-blog.png my-excellent-blog.png
adesso posso vedere l'immagine tra i file della directory corrente con il comando:
>ls
adesso posso copiare l'immagine nel mio bucket:
>gsutil cp my-excellent-blog.png gs://$DEVSHELL_PROJECT_ID/my-excellent-blog.png
>gsutil ls gs://$DEVSHELL_PROJECT_ID

posso anche visualizzare l'immagine entrando da UI in Cloud Storage e nel bucket.
se flaggo share publicly all'immagine, cliccando su Public link posso copiare il link dell'immagine.



	CREARE UN DB Cloud SQL
da interfaccia grafica basta scegliere il tipo di VM, nome, la zona, pwd....
dopo aver creato il db bisogna creare almeno un user
configuro il db per essere contattato solo dalla mia VM,
copio l'IP pubblico della VM 35.226.78.114 e lo incollo nelle authorizazioni del db,
creo una nuova network con subnet 35.226.78.114/32.

entro in ssh della vm e la configuro per utilizzare l'istanza di cloud sql.



---------------------------------------------------------
	CONTAINERS in the Cloud GKE

Compute Engine is Infrustructure as a Service 
App Engine is Platform as a service 

Kubernetes Engine si trova a metà strada tra IaaS e PaaS

IaaS: ogni App ha un OS(sistema operativo)
 replicando le App replichiamo anche il SO
 
PaaS: le risorse vengono replicate "insieme" 
e non singolarmente in base alle esigenze

Containers: abbiamo un'unico SO, 
le App hanno solo le dipendenze (Libs)
le App vengono replicate similmente alla PaaS ma indipendentemente l'una dall'altra
i container sono portabili, senza bisogno di re-build
e facilmente replicabili

Kubernetes orchestra i container e rende semplice la gestione
Cloud Build rende semplice la creazione di container

	ESEMPIO DI CREAZIONE DI CONTAINER CON PYTHON
---
app.py

from flask import Flask
app= Flask(__name__)

@app.route("/")
def hello():
	return "Hello World! \n"
	
@app.route("/version")
def version():
	return "Helloworld 1.0 \n"
	
if __name__ == "__main__":
	app.run(host='0.0.0.0')

---	
requirements.txt

Flask==0.12
uwsgi==2.0.15

---
Dockerfile

FROM ubuntu:18.10
RUN apt-get update -y && \
	apt-get install -y puthon3-pip python3.dev
COPY requirements.txt /app/requirements.txt
WORKDIR /app
RUN pip3 install -r requirements.txt
COPY . /app
ENTRYPOINT ["python3", "app.py"]

---
	BUILD AND RUN
> docker build -t py-server .
> docker run -d py-server

---

Kubernetes è organizzato in cluster
ogni cluster contiene il master e i nodi
il comando per creare il cluster:

> gcloud container clusters create k1

i nodi contengono i pod
ogni pod contiene:
	- Virtual Ethernet
	- uno o più container
	- uno o più volume

OSS: di solito un pod contiene più container quando questi sono strettamente correlati
altrimenti conviene avere un container per ogni pod

Supponiamo un esempio in cui abbiamo un cluster Kubernetes
formato dal master, un nodo che contiene un pod

> kubectl container clusters create k1
> kubectl run nginx --image=nginx:1.15.7
> kubectl get pods
> kubectl expose deployments nginx --port=80 --type=LoadBalancer

Gli sviluppatori accedono ad un API che permette di accedere al master che gestisce i deployments
Il Network Load Balancer espone un indirizzo IP pubblico per accedere ai servizi dall'esterno (end users)
	esso inoltre ha un IP fisso con cui si connette al service che si trova all'interno del cluster,
	il quale è connesso ai nodi e quindi ai pod che contengono i servizi.
	Il service rappresenta quindi un gruppo di pod, che rispondono all'end point.
IL service è necessario, perchè espone un IP fisso,
mentre i pod che sono replicabili, hanno indirizzo IP effimero.
	
> kubectl get service
NAME	TYPE			CLUSTER-IP		EXTERNAL-IP			PORT(S)	AGE
nginx	LoadBalancer	10.0.65.118		104.198.149.140		80/TCP	5m
	
	SCALING IN Kubernetes-GKE
	
> kubectl scale nginx --replicas=3
In questo modo il POD viene replicato in altri due nodi,
quindi ora il cluster contiene il master(che contiene/gestisce il deployment)
e tre nodi, ciascun nodo contiene un POD con nginx all'interno.

Possiamo anche utilizzare l'AUTOSCALING:	
> kubectl autoscale nginx --min=10 --max=15 --cpu=80
> kubectl get pods -l "app=nginx" -o yaml
abbiamo specificato il num min, il num max di pod e il criterio di scaling up

Possiamo anche utilizzare un comando non imperativo ma dichiarativo,
cioè scrivendo i comandi in specifici file di configurazione in formato yaml.

	DEPLOYMENTS
apiVersion: v1
kind: Deployment
metadata:
	name: nginx
	labels:
		app: nginx
spec:
	replicas: 3
	selector:
		matchLabels:
		app: nginx
	template:
		metadata:
		labels:
			app: nginx
	spec:
		containers:
		- name: nginx
			image: nginx:1.15.7
		ports:
		- conainerPort: 80

---
Questi file posso essere memorizzati e versionati
questo deployment getisce i deployment dei pod che l'etichetta specificata.

Per utilizzare questo file di configurazione utilizziamo il comando:
> kubectl apply -f nginx-deployment.yaml

	REPLICASET
Un'altro modo di gestire la vita dei container, oltre al deployment,
sono i replicaset.

> kubectl get replicasets
> kubectl get pods
> kubectl get deployments
> kubectl get services

	UPDATE DELLA VERSIONE DELL'APPLICAZIONE
Kubernetes ha delle strategie per effettuare l'aggiornamento delle versioni dell'applicazione
e quindi l'aggiornamento dei pod, senza però creare un disservizio,
cioè i pod simili non vengono stoppati e riavviati tutti contemporaneamente,
ma uno più di uno alla volta a seconda della strategia scelta.

DEPLOYMENTS
apiVersion: v1
kind: Deployment
metadata:
	name: nginx
	labels:
		app: nginx
spec:
	replicas: 5
	strategy:
		
	rollingUpdate:
		maxSurge: 1
		maxUnavailable: 0
	type: RollingUpdate
	
	selector:
		matchLabels:
		app: nginx
	template:
		metadata:
		labels:
			app: nginx
	spec:
		containers:
		- name: nginx
			image: nginx:1.15.7
		ports:
		- conainerPort: 80

---


ANTHOS is Google's modern solution for hybrid and multi-cloud systems
			and services managament
	- Kubernetes and GKE On-Prem create the foundation
	- On-premises and Cloud environments stay in sync
	- A rich set of tools is provided for:
		. managing services on-premises and in the Cloud
		. monitoring systems and services
		. migrating applications from VMs into clusters
		. maintaining consestent policies across all clustes,
			wheather on-premises or in the Cloud
			
BUILDING A MODERN HYBRID INFRASTRUCTURE, STEP BY STEP

Google Kubernetes Engine (GKE) on the Cloud and GKE On-Prem
are integrated with GCP Marketplace.

Cloud Interconnect:
On the Cloud: Anthos Service Mesh (service mesh)
On-Prem: Isto Open Source (service mesh)

Stackdriver (logging and monitoring) 
comunicates with both GKE on the Cloud and On-Prem.

Anthos Configuration Managements (sync policy)
comunica attraverso Cloud Interconnect,
è presente in GKE e in GKE On-Prem,
per GKE On-Prem è disponibile Polici Repository (GIT)(store policy)

LAB: GETTING STARTED WITH KUBERNETES ENGINE
https://app.pluralsight.com/course-player?clipId=97cdd40e-3979-4dc0-9f8d-b31044fe4a28
Creazione di un cluster Kubernetes con un LoadBalancer.

Per prima cosa andiamo nella sezione 
APIs and Services per verificare che l'API è abilitato.
Cerchiamo:
Kubernetes Engine API
Container Registry API

Quindi eseguiamo lo start del cluster da Cloud Shell:
> export MY_ZONE=us-central1-f
> gcloud container clusters create webfrontend --zone $MY_ZONE --num-nodes 2
> kubectl version

In Compute Engine possiamo verificare la presenza di due VM e 
in Kubernetes Engine possiamo verificare la presenza del cluster e del size (2)

> kubectl run nginx --image=nginx:1.10.0
> kubectl get pods

> kubectl expose deployment nginx --port 80 --type LoadBalancer
> kubectl get services
adesso è assegnato un IP address

> kubectl scale deployment nginx --replicas 3
> kubectl get pods
> kubectl get services


	APP ENGINE
	
Nessun controllo sull'architettura PaaS
LoadBalancer e autoscaling
especially suited for building scalable web applications and mobile backends

App Engine offre due ambienti: Flexible e Standard

App Engine standard environment
	easily deploy your applications
	autoscale workloads
	free daily quota
	usage based pricing
	SDKs for development, testing, deployment
	specifica versione of Java Python PHP Go 
	SANDBOX constraints: no writing to local files, all requests time out at 60secs, limits on third-party software.
	Each project App Engine contains App Servers and Application instances,
		Memcache, Task queues, Sheduled tasks, Search, Logs.
	App Engine automatically scales and reliably serves your web application
				can access a variety of services using dedicated APIs
	
App Engine flexible environment
	build and deploy containerized apps with a click
	NO SANDBOX constaints
	can access App Engine resources
	
COMPARAZIONE STANDARD-FLEXIBLE 
					standard environment			flexible environment
instance startup	milliseconds					minutes
SSH access			no								yes (although not by default)
write to local disk	no								yes (but writes are ephemeral)
support for 		no								yes
 3rd-party binaries
network-access		via app engine services			yes

pricing model		after free daily use, pay		pay for resources allocation per hour
					per instance class,				no automatic shutdown
					with automatica shutdown		

	Kubernetes Engine 
language support: any, 
service model: hybrid
primary use case: container based workloads

	App Engine Flexible
language support: any
service model: PaaS
primary use case: web and mobile applications, container-based workloads

	App Engine Standard
language support: Java Python Go PHP
service model: PaaS
primary use cases: web and mobile applications


	CLOUD ENDPOINTS AND APIGEE EDGE

Cloud Endpoints helps you create and maintain APIs 
	distribuited API management through an API console
	expose your API using RESTful interface
	control access and validae calls with JSON Web Tokens and Google API Keys
	identify web, mobile users with auth0 and firebase authentication
	generate client libraries
	supports: App Engine Flexible Env, Kubernetes Engine, Compute Engine runtime environments and
			Android, iOS, Javascript clients.
			
Apigee Edge is also a platform for developing and managing API proxies
	it has a focus on business problems like rate limiting, quotas, analytics...
	
LAB APP ENGINE
creare un'istanza di app engine standard

> git clone https://github.com/GoogleCloudPlatform/appengine-guestbook-python 
> cd appengine-guestbook-python 
> ls -l 
> cat app.yml
> dev_appserver.py ./app.yaml 
> gcloud app deploy ./index.yaml ./app.yaml 



Cloud Source Repositories: 
	fully featured Git repositorues hosted on Google Cloud Platform
	
Cloud Functions:
	create single purpose functions that respond to events without a server or runtime.
	written in Javascript, execute in managed Node.js environment on GCP 
	
Deployment Manager:
	provides repeatable deployments
	create a .yaml template describing your environment and use Deployment Manager to create resources.
	
Monitoring:
	Proactive instrumentation
	
Stackdriver:
	Monitoring
		platform, system and application metrics
		uptime/health checks
		dashboards and alerts
	Logging
		platform, system and application logs
		log search, view, filter and export
		log-based metrics
	Debugger
		debug applications
	Error Reporting
		error notifications
		error dashboard 
	Trace
		latency reporting and sampling
		per-URL latency and statistics
	Profiler
		continuous profiling of CPU and memory consumption 

LAB deployment-manager and stack driver 

> export MY_ZONE=us-central1-f
> echo $DEVSHELL_PROJECT_ID
> nano mydeploy.yaml
---
resources:
- name: my-vm-1
	type: compute.v1.instance
	properties:
		zone: ZONE
	machineType: zones/ZONE/machineTypes/n1-standard-1
	metadata:
		items:
		- key: startup-script
			value: "apt-get update"
	disks:
		- deviceName: boot
			type: PERSISTENT
			boot: true
			autoDelete: true
			initializeParams:
			sourceImage: https://www.googleapis.com/.../debian...
		networkInterfaces:
		- network: https://www.googleapis.com/.../default
			accessConfigs:
		- name: External NAT
			type: ONE_TO_ONE_NAT
--- 
> sed -i -e 's/PROJECT_ID/'$DEVSHELL_PROJECT_ID/mydeploy.yaml
> sed -i -e 's/ZONE/'$MY_ZONE/mydeploy.yaml 
> gcloud deployment-manager deployments create my_first_depl --config mydeploy.yaml 
> gcloud deployment-manager deployments list 


	BIG DATA AND MACHINE LEARNING
	
	Google Cloud's big data services are fully managed and scalable
	
Cloud Dataproc:
	managed Hadoop MapReduce, Spark, Pig, Hive service 
	create clusters in 90 seconds or less on average
	scale clusters up and down even when jobs are running
	easily migrate on-premises Hadoop jobs to the cloud
	save money with preemptible instances 
	use Spark Machine Learning Libraries (MLlib) to run classifications algorithms
	
Cloud Dataflow:
	stream and batch processing, unified and simplified pipelines
	processes data using Compute Engine instances.
		clusters are sized for you
		automated scaling, no instance provisioning required
	Spurce (BigQuery) -> Transforms( Dataflow) -> Sink (Cloud Storage)
	ETL (exctract/transform/load) pipelines to move, filter, enrich, shape data 
	Data analysis: batch computation or continuous computation using steaming
	Orchestation: create pipelines that coordinate services, including external services
	Integrates with GCP services like Cloud Storage, Cloud Pub/Sub, BigQuery, BigTable, Open source Java and Python SDKs.
	
BigQuery:
	Analytics database, stream data at 100000 rows per second
	is a fully managed data warehouse
	provides near rea-time interactive analysis of massive datasets (hundreds of TBs) using SQL syntax (SQL 2011)
	no cluster maintenance is required
	can load from Cloud Storage or Cloud Datastore or steam into BigQuery up to 100000 rows per second
	in additional to SQL queries, you can read and write data from Cloud Dataflow, Hadoop and Spark 
	
	BigQuery runs on Google's high performance infrastructure, 
	it is a globa service
	
	compute and storage are separated with a terabit network in between
	you only pay for storage and processing used
	automatic discount for long-term data storage
	
	
Cloud Pub/Sub:
	scalable and flexible enterprise messaging 
	is scalable, reliable messaging
	supports many-to-many asynchronous messaging 
	application components make push/pull subscriptions to topics
	includes support for offline consumers
	Pub=publisher,Sub=subscriber
	builing block for data ingestion in Dataflow, IoT, Marketing Analytics
	Foundation for Dataflow streaming
	push notifications for cloud-based applications
	connect applications across Google Cloud Platform (push/pull between Compute Engine and App Engine)
	
	
Cloud Datalab:
	interactive data exploration
	interactive tool for large-scale data exploration, transformation, analysis, visualizatioon
	integrated, open source, built on Jupyter (formerly Python)
	analyze data in BigQuery, Compute Engine, Cloud Storage using Python, SQL, Javascript
	Easily deploy models to BigQuery



Google Cloud Machine Learning Platform

	TensorFlow, Cloud ML, Machine Learning APIs
	Open source toool to build and run neural network models
		wide platform support: CPU or GPU, mobile server or cloud
	Fully managed machine learning service
		familiar notebook-based developer experience
		optimized for Google infrastructure, integrates with BigQuery and Cloud Storage
	Pre-trained machine learning models bilt by Google
		speech: stram results in real time, detects 80 languages
		vision: identify object, landmarks, text, content
		translate: language translation including detection
		natual language: structure, meaning of text 
	
	Cloud Machine Learning platform:
	- for structured data
		classification and regression
		recommendation
		anomaly detection
	- for unstructured data
		image and video analytics
		text analytics 
	
	Cloud Vision API
		analyze images with a simple REST API 
		logo detection, label detection etc
		with Cloud Vision API you can:
		gain insight from images
		detect inappropriate content
		analyze sentiment
		extract text
		
	Cloud Natural Language API (voice and text???)
		can return text in real time
		highly accurate, even in noisy evironments
		access from any device
		80 languages 
		uses machine learning models to reveal structure and meaning of text
		extract information about items mentioned in text documents, news articles, blog posts.
		
	Cloud Translation API
		translate arbitrary strings between thousands of language pairs
		programmatically detect a document's language 
		support for dozens of languages 
	
	Cloud Video Intelligence API
		annotate the contents of videos
		detect scene changes
		flag inappropriate content
		support for a cariety of video formats 
		
		
	LAB BigQuery
	
Dalla dashboard di BigQuery creiamo un nuovo dataset specificando Dataset ID, location, Data espiration.
Nella tab Create Table, dal menu File upload scegliamo Google Cloud Storage e specifichiamo il path del bucket/file da importare.
Specificare il nome della table di destinazione, inseriamo il flag per il riconoscimento automatico dello schema.
Dopo aver creato la table è possibile esplorarla da UI, cliccando sul pulsante Compose Query è possibile comporre la query per interrogare la table.

select int64_field_6 as hour, count(*) as hitcount
from logdata.access....
group by hour order by hour ;

Possiamo scrivere le query anche da Shell:
> bq query "select string_field_10 as request, count(*) as requestcount from logdata.accesslog group by request order by requestcount desc" 




	COMPARING COMPUTE OPTIONS

Compute Engine:
	service model: IaaS
	use cases: general computing workloads
	
Kubernetes Engine:
	service model: Hybrid
	use cases: container-based workloads
	
App Engine Flexible:
	service model: PaaS
	use cases: web and mobile applications; container-based workloads
	
App Engine Standard:
	service model: PaaS
	use cases: web and mobile applications
	
Cloud Functions:
	service model: Serverless
	use cases: ephemeral functions responding to events 
	

	COMPARING LOAD-BALANCING OPTIONS
	
Global HTTP(S):
	layer 7 load balancing based on load
	can route different URLs to different back ends
	
Global SSL Proxy:
	layer 4 load balancing of non-HTTPS SSL traffic based on load
	supported on specific port numbers
	
Global TCP Proxy:
	layer 4 load balancing of non-SSL TCP traffic 
	supported on specific port numbers 
	
Regional:
	load balancing of any traffic (TCP, UDP)
	supported on any port number 
	
Regiona internal:
	load balancing of traffic inside a VPC 
	use for the internal tiers of multi-tier applications 
	

	COMPARING INTERCONNECT OPTIONS
	
VPN: secure multi-Gbps connection over VPN tunnels 

Dirent Peering: private connection between you and Google for your hybrid cloud workloads 

Carrier Peering: connection through the largest partner network of service providers

Dedicated Interconnect: Connect N x 10G transport circuits for private cloud traffic to Google Cloud at Google POPs 

------------------------------------------------
ESSENTIAL GOOGLE CLOUD INFRASTUCTURE - FUNDATION 

IaaS								PaaS								SaaS
CPUs, Memory			Servers				Clusters					Serverless 
Disk, Interfaces		VM instances		Cluster Management 			Autoscaling
IT Ops				SysOps			DevOps					LowOps			NoOps 

LAB - creare un bucket

da interfaccia grafica:
Storage -> Create bucket
specificare name (globally unique) e storage class 

da Google Cloud Shell:
> gsutil mb gs://<BUCKET_NAME> 
mb=make bucket 
> gsutil cp myfile.txt gs://<BUCKET_NAME> 

> gcloud compute regions list 
mostra tutte le region disponibili 

> INFRACLASS_REGION=us-centra1
> echo $INFRACLASS_REGION 

> mk infraclass 
> touch infraclass/config 
> echo INFRACLASS_REGION=$INFRACLASS_REGION >> tilde/infraclass/config 
> INFRACLASS_PROJECT_ID=myprojectid
> echo INFRACLASS_PROJECT_ID >> tilde/infraclass/config

> nano .profile



	PROJECTS
	
Le risorse possono essere create ed utilizzate solo all'interno di un progetto.

> gcloud config list

> gcloud config list | grep project

> export PROJECT_ID = project_id
> gcloud config set project $PROJECT_ID


	VIRTUAL NETWORKS: global instrastructure

Regions with 3 zones , PoPs (point of presence), network,
Edge point of presence 

	VPC (virtual private cloud)
VPC objects:
- Projects
- Networks (default, auto mode, custom mode)
- Subnetworks
- Regions 
- Zones
- IP addesses (internal, external, range)
- Virtual machines (VMs)
- Firewall rules

A project:
- Asosciates objects and services with billing
- Contains networks (up to 5) that con be shared/peered

A network:
- has no IP address range
- is global and spans all available regions 
- contains subnetworks 
- is available as default, auto or custom 

	3 VPC network types:
1- Default
	every project
	one subnet per region
	default firewall rulse
2- Auto Mode
	default network
	one subnet per region
	regional IP allocation
	fixed /20 subnetwork per region
	expandable up to /16
3- Custo Mode 
	no default subnets created
	full control of IP ranges
	regional IP allocation
	expandable to any RFC 1918 size 
	
You can conver Auto Mode network in Custom Mode network
to take advantage of the control that customer networks provide.
However, this conversion is one way, meaning that 
Custom Networks cannot be changed to Auto Mode networks.

VMs in the same network can communicate using their internal IP address,
even though they are in different regions.

VMs in different network must communicate using their external IP address,
even though they are in the same region.
The traffic isn't through the public internet,
but it is going through the Google Edge routers.
	
	Google's VPC is global
Possiamo comunicare con le VM presenti anche in differenti region,
ma nella stessa Cloud VPC Network, passando attraverso lo stesso VPN Gateway.
Questo riduce costi e complessità di gestione della rete.

	Subnetworks cross zones
VMs can be on the same subnet but in different zones.
A single firewall rule can apply to both VMs.
Because the region contains several zones,
subnetworks can cross zones.

subnet 10.0.0.0/20?
.0 and .1 are reseved for the network in these subnets gateway respectively
.2 and .3 are assigned to the VMs instances.

Every subnet has 4 reserved IP addesses in its primary IP range.

	Expand subnets without re-creating instances 

Abbiamo un progetto che contiene una Network,
nella Region A ci sono due subnet: 172.16/24 (2 VMs) e 10.128/16 (5 VMs)
nella Region B c'è la subnet 10.130/16 (5 VMs)
nella Region C c'è la subnet 10.132/16 (5 VMs) 

Cannot overlap with other subnets 
Must be inside the RFC 1918 address spaces 
can expand but not shrink 
Auto Mode can be expanded from /20 to /16 
	per espandere ulteriormente dobbiamo passare a Custom Mode network.
avoid large subnets.

	How to Expand a Subnet
	
ho creato una subnet con /29 mask che contiene 8 indirizzi
4 sono riservati da GCP, quindi abbiamo disponibili 4 IP per le VMs 

proviamo a creare un'altra VM instance nella subnet, 
otteniamo un errore.

per espandere la subnet andiamo nella tab della subnet,
selezioniamo edit e andiamo a modificare il campo
IP address range da 10.0.0.0/29 a 10.0.0.0/23 

quindi possiamo cliccare su Retry della VM che è fallita.

Abbiamo espanso la maschera della subnet senza alcuno shutdown delle macchine già presenti.

	VMs can have internal and external IP addresses
	
Internal IP
	Allocated from subnet range to VMs by DHCP
	DHCP lease is renewed every 24 hours
	VM name + IP is registered with network-scoped DNS 

External IP
	Assigned from pool (ephemeral) default
	Reserved (static) and billed more when not attached to a running VM 
	VM doesn't know external IP, it is mapped to the internal IP 
	
When you create a VM in GCP, 
it's symbolic name is registered with an internal DNS service 
that translates the name to the internal IP addess.

DNS is scoped to the network so it can translate web urls and VM names
of hosts in the same network,
but it can't translate host names from VMs in a different network.

the external IP address is optional,
you can assign an external IP address if your device or your machine 
is externally facing.

Contestualmente alla creazione di una VM,
è possibile scegliere per il Primary Internal IP lo opzioni:
Ephemeral (Automatic) e Ephemeral (Custom) e Reserve static internal IP address.
Per quanto riguarda l' External IP address le opzioni sono:
None e Ephemeral e Create IP address.


	DNS resolution for internal addresses
Each instance has a hostname that can be resolved to an internal IP address:
- the hostname is the same as the instance name
- FQDN is [hostname].[zone].c.[project_id].internal

Example: my-server.us-central1-a.c.guestbook-151617.internal

Name resolution is handled by internal DNS resolver:
- provided as part of Compute Engine (169.254.169.254)
- configured for use on instance via DHCP 
- provides answer for internal and external addresses.

	DNS resolution for external addresses
Instances with external IP addresses can allow connections from hosts outside the project.
	users connect directly using external IP address
	admins can also publish public DNS records pointing to the instance.
	public DNS records are not publishe automatically.
DNS records for external addresses can be published using existing DNS servers (outside of GCP)
DNS zones can be hosted using Cloud DNS.
	
	Host DNS zones using Cloud DNS
Google's DNS service
translate domain names into IP address
low latency
high availability (100% SLA)
create and update millions of DNS records 
UI, command line or API 

	ALIAS IP RANGES
	Assign a range of IP addresses as aliases to a VM's network interface using alias IP ranges
	
rete esterna alla VM: VM primary IP 10.1.0.2 - subnet: primary CIDR range 10.1.0.0/16
rete interna alla VM: Container in VM, VM Alias IP range: 10.2.1.0/24 - subnet: secondary CIDR range 10.2.0.0/20


	ROUTES ND FIREWALL RULES

By default every network has:
- routes that let instances in a network send traffic directly to each other, even across subnet.
- a default route that directs packets to destinations that are outside the network 

Firewall rules must also allow the packet. 

Routes map taffic to destination networks 
- apply to traffic egressing a VM (traffico in uscita)
- forward traffic to most specifica route (inoltrare)
- are created when a subnet is created.
- enable VMs on same network to communicate 
- destination is in CIDR notation 
traffic is delivered only if it also matches a firewall rule.

			VM Routing Table
Internet: 	0.0.0.0/0
VM 1 : 		192.168.5.0/24
VM 2 e 3 : 	10.128.1.0/20
...

	Instance routing tables
	
vpngateway		10.100.0.0/16 -> default-route-78...
				0.0.0.0/0 -> default-route-6807...

vm1				10.100.0.0/16 -> default-route-78...
				0.0.0.0/0 -> default-route-6807...
				172.12.0.0/16 -> vpngateway 
				
vm2				10.100.0.0/16 -> default-route-78...
				0.0.0.0/0 -> default-route-6807...
				172.12.0.0/16 -> vpngateway 

Each route in the routes collection may apply to one or more instances.
A route applies to an instance if he network and instance tax match.
If the network matches and there are no instance tax specified,
the route applies to all instances in that network.
Compute Engine uses the routes collection to create individual read-only table for each instance.

Every virtual machine instance in the network is directly connected to this router
and all packets leaving a virtual machine instance 
our first handle at slayer before they are forwarded to the next stop(hop?).

The original network router selects the next hop
for a packet by consulting the routing table for that instance.

	Firewall rules protect your VM instances from unapproved connections

- VPC network functions as a distributed firewall 
- firewall rules are applied to the network as a whole 
- connections are allowed or denied at the instance level 
- firewall rules are stateful (firewall rules allow bidirectional communication once a session is established)
- impied deny all ingress and allow all egress by default.

Firewall rules are applied between individual instances in the same network.

	Routes map traffic to destination networks 

Parameter		Details

direction		Inbound connections are matched against ingress rules only
				Outbound connections are matched against egress rules only 
				
source or		For the ingress direction, sources can be specified as part of the rule with IP addresses, source tags, or a source service account.
destination 	For the egress direction, destinations can be specified as part of the rule with one or more ranges of IP addresses.

protocol		Any rule can be stricted to apply to specifica protocols only or specifica combinations of protocols and ports only.
and port 

action			To allw or deny packets that match the direction, protocol, port, and source or destination of the rule.

priority		Governs the order in which rules are evaluated, the first matching rule is applied.

Rule assignment		All rules are assigned to all instances, but you can assign certain rules to certain instances only.

	GCP firewall use case: Egress
	
Conditions:
- destination CIDR ranges 
- Protocols 
- Ports 
Actions:
- Allow: permit the matching egress connection 
- Deny: block the matching egress connection.

	GCP firewall use case: Egress
Conditions:
- Source CIDR ranges
- Protocols 
- Ports 
Actions:
- Allow: permit the matching ingress connection 
- Deny: block the matching ingress connection 


LAB - create an Auto Mode VPC network with firewall rules and two VM instances,
	then convert the Auto Mode network to a Custom Network and create other Custom Mode networks.

Ogni progetto ha la default network,
che ha una subnet per ogni differente region.
Ogni subnet ha un IP addresses range e un Gateway IP.
In VPC network, nella tab Routes possiamo esplorare la rules table.

Nella tab Firewall rules cancelliamo tutte le rules presenti e 
cancelliamo anche la default network.

Verifichiamo che da Compute Engine non riusciamo 
a creare una nuova VM senza la default network, 
cioè senza alcuna network.

Creiamo mynetwork di tipo Automatic Mode, con ip range /20 per ogni region.

Le Firewall rules che vengono create in automatico sono:

- mynetwork-allow-icmp 			ingress			apply to all	priority:65534
- mynetwork-allow-internal		ingress			apply to all	priority:65534
- mynetwork-allow-rdp			ingress			apply to all	priority:65534
- mynetwork-allow-ssh			ingress			apply to all	priority:65534

- mynetwork-deny-all-ingress	ingress			apply to all	priority:65535
- mynetwork-allow-egress		egress			apply to all	priority:65535

Le ultime due regole non sono deselezionabili, quindi vengono aggiunte di default alla nuova rete.
Le prime 4 le aggiungiamo noi.

Andiamo in Compute Engine e creiamo una VM nella network che abbiamo creato
specificando una region e zone.

Per ogni VM creata vediamo internal ed external IP address.
Cliccando su (nic0) della riga della VM possiamo verificare la subnet in cui essa è creata,
quindi verificare che il suo IP interno si trova nella maschera della subnet.

Abbiamo 2 VM in due subnet della stessa network,
dall'SSH di ciascuna VM riusciamo a fare il ping dell'altra utilizzando l'IP interno:
> ping -c 3 10.132.0.2 
oppure possiamo utilizzare il nome dell'istanza della VM:
> ping -c 3 mynet-eu-vm 
questo grazie al DNS, è importante perchè l'IP interno è effimero, cioè può cambiare al restart della VM.

Il ping funziona anche con l'ip esterno, grazie alle impostazioni delle Firewall rules.

Convertiamo la nostra network Auto Mode in una Custom Network.
Entrare nella VPC network e andare in edit, è selezionato Auto, basta selezionare Custom e salvare.

Creiamo una un'altra custom network.

da Shell:

> gcloud cmpute networks create networkName 
	--subnet-mode=custom 
	--project=projectid 
	
> gcloud compute networks subnets create subnetName 
	--network=networkName 
	--region=us-centra1 
	--range=172.16.0.0/20 
	--project=projectid 
	
> gcloud compute networks list 

> gcloud compute networks subnets list 
	--sort-by=NETWORK 

Creiamo le Firewall rules:

> gcloud compute firewall-rules create firewallRuleName
	--direction=INGRESS 
	--priority=1000 
	--network=networkName 
	--action=ALLOW 
	--rules=tcp:22.tcp:389,icmp 
	--source-ranges=0.0.0.0/0 
	--project=projectId 
	
> gcloud compute firewall-rules list 
	--sort-by=NETWORK 
	
> gcloud compute instances create vmName
	--zone=us-centra1-c 
	--machine-type=f1-micro 
	--subnet=subnetName 
	
> gcloud compute instances list
	--sort-by=ZONE 	
	
Andiamo a fare il ping con External IP addess 
tra due VM nella stessa Zone ma in Network differenti:

> ping -c 334.68.94.34 
il ping funziona grazie alle impostazioni delle firewall rules.
ovviamente, a maggior ragione il ping funziona tra vm nella stessa network.


	INCREASED AVAILABILITY WITH MULTIPLE ZONES 
IF your application needs increased availability 
you can place two virtual machines into multiple zones,
but within the same SUBnetwork.

Using a single subnetwork allows you to create a firewall rule against the sub network.

Per esempio possiamo avere 2 VM nella stessa region:us-west1 :
	vm1 in zone:us-west-1a  10.2.0.2
	vm2 in zone:us-west-1b  10.2.0.3
sotto la stessa subnet 10.2.0.0/16
Therefore, by allocating VMs on a single subnet to separate zones
you get improved availability without additional security complexity.

A regioal managed instance group contains instances from multiple zones 
across the same region, which provides increased availability. 

Questa configurazione conferisce isolamento per le infrastrutture,
hardware e software failures. 

	GLOBALIZATION WITH MULTIPLE REGIONS 
Abbiamo due VM in differenti region, quindi in differenti subnet:
	vm1 in zone:us-east-1a 10.2.0.4 in subnet:10.2.0.0/16
	vm2 in zone:us-west-1b 192.168.0.2 in subnet:192.168.0.0 

Putting resources in different regions, provides an even higher degree of failure independence,
this allows you to design robust system with resources spread across different failure domains,
when using a global Load Balancer, like HTTP Load Balancer, 
you can route traffic to the region that is closest to the user.
This can result in better latency for users and lower network traffic costs for your project.

	GENERAL SECURITY BEST PRACTICE
Only assign internal IP address to VM instances whenever is possible.

Cloud NAT provides internet access to private instances (inside VPC)

Cloud NAT does not implement INBOUND NAT:
hosts outside your VPC network cannot directly access any of the private instances behind the cloud NAT Gateway.
this help you keep your VPC networks isolated and secure.

Private Google Access to Google APIs and services 

similarly, you should enable Private Google Access 
to allow VM instances that ONLY have IP addresses to reach the external IP
of Google APIs and services.

For example,
if your private VM instances needs to access a Cloud Storage Bucket,
you need to enable Private Google Access,

Le VM, anche da regioni diverse si collegano allo stesso VPC Routing,
il quale si collega all'Internet Gateway che è collegato ad Internet.
Supponiamo ci siano nella region us-west1, 
	subnet-a: Private Google Access ON
	VM A1 con IP interno
	VM A2 con IP interno ed esterno
Nella region us-east1, 
	subnet-b: Private Google Access OFF 
	VM B1 con IP interno 
	VM B2 con IP interno ed esterno
	
This allow VM A1 to access Google apps and services,
even though it has no external IP address.
Private Google Access has no effect on instances that have external IP addresses,
that's why VMs A2 and B2 can access Google apps and services.
The only VM that can't access those apps and services is VM B1,
this VM has no public IP address and it is in a subnet where Google Private Access is disabled.

LAB - Implement Private Google Access and Cloud NAT Gateway

Creiamo una VM, una VPC Custom network con una subnet e le Firewall rules.
Lasciamo Private Google Access disabilitato nelle impostazioni nella crezione della network.
Creiamo le Firewall Rules, specificando la network, il Targets: All instances in the network,
specifichiamo anche il Source IP ranges, Protocols and ports: tcp: 22,
Infine creiamo la VM nella network che abbiamo precedentemente creato e dobbiamo specificare
anche la subnet. 
Per l'External IP address, possiamo scegliere tra le opzioni: None, Ephemeral, Create IP address,
scegliamo l'opzione None.
Entriamo in Cloud Shell,

> gcloud config project myprojectid
> gcloud compute ssh vmName
	--zone us-centra1-c
	--tunnel-through-iap
> ping -c 2 www.google.com 
this command is not working because the vm doen't have the external IP address.

Possiamo utilizzare Private Google Access per raggiungere l'IP esterno di google.com 

Creiamo un bucket da Google Storage
torniamo in Cloud Shell e cerchiamo di raggiungere il buchet.
Copiamo qualcosa nel bucket e cerchiamo di raggiungerla dalla VM.

> gsutil cp gs://internal-url/img.svg gs://bucketName 
> gsutil cp gs://bucketName/img.svg .

torniamo nell'SSH della VM:
> gcloud compute ssh vm-internal 
	--zone us-central1-c --tunnel-through-iap 
> gsutil cp gs://bucketName/img.svg .
it doesn't working

Andiamo in VPC networks, la nostra network 'privatenet',
entriamo nella scheda della sua subnetwork ed andiamo in edit,
modifichiamo il flag Private Google access da Off ad On 
Adesso il comando 
> gsutil cp gs://bucketName/img.svg .
dalla ssh della vm funziona!

	Gonfigurare Cloud NAT Gateway
per accedere ad internet dalla vm sprovvista di external IP address

con il comando exit usciamo dall'ssh della vm e torniamo nella Cloud Shell.
> exit
> sudo apt-get update 
> gcloud compute ssh vm-internal 
	--zone us-central1-c --tunnel-through-iap 
> sudo apt-get update 
il comando ovviamente non funziona, perchè la vm non ha accesso ad internet.

Andiamo in Netowork services -> Cloud NAT 
Create a NAT Gateway,
Gateway name: nat-config, VPC network: privatenet(la nostra network)
Region:us-centra1
Cloud Router: create 
NAT mapping possiamo assegnare un IP statico, ma lasciamo i valori di default.

Riproviamo il comando sudo dall'ssh della vm:
> sudo apt-get update 
adesso il comando funziona!


	VIRTUAL MACHINE 

GCP compute and processing options 

	COMPARING COMPUTE OPTIONS

Compute Engine:
	language support: any
	service model: IaaS
	scaling: server autoscaling 
	use cases: general computing workloads
	
Kubernetes Engine:
	language support: any
	service model: Hybrid (IaaS and PaaS) 
	scaling: cluster 
	use cases: container-based workloads
	
App Engine Standard:
	language support: Python, Node.js, Go, Java, PHP.
	service model: PaaS
	scaling: autoscaling managed servers 
	use cases: web and mobile and backend scalable applications
	
App Engine Flexible:
	language support: Python, Node.js, Go, Java, PHP, Ruby, .NET, Custom Runtimes.
	service model: PaaS
	scaling: autoscaling managed servers 
	use cases: web and mobile applications; container-based workloads
	
Cloud Functions:
	language support: Python, Node.js, Go 
	usage model: microservices architecture 
	scaling: serverless 
	use case: lightweight event actions.

	Compute Engine:
Infrastructure as a Service (IaaS)

Predefined or custom machine types:
- vCPUs(cores) and Memory(RAM) 
- Persistent disks: HDD, SSD, Local SSD 
- Networking 
- Linux or Windows 

Compute Engine features:

Instance metadata 
Strartuo scripts 

Machine rightsizing:
- recommentation engine for optimum machine size 
- Stackdrivers statistics 
- New recommendation 24hrs after VM create or resize 

Global load balancing:
- Multiple regions for availability 

Availability policies:
- Live migrate 
- Auto restart 

Per-second billing 
Sustained use discount 
Committed use discounts 

Preemptible:
- Up to 80% discount 
- No SLA 

Several Machine types:
- Network throughtput scales 2 Gbps per vCPU (small exceptions)
- Theoretical max of 32 Gbps with 16 vCPU or 
					100 Gbps with T4 or V100 GPUs.
A vCPU is equals to 1 hardware hyper-thread.

Disks:
- Standard, SSD, or Local SSD 
- Standard and SSD PDs scale in performance for each GB of space allocated.
Resize disks or migrate instances with no downtime.

Networking:
Robust networking features 
- Default, custom networks 
- Inbound/outbound firewall rules 
	. IP based
	. Instance/group tags 
- Regional HTTPS load balancing 
- Network load balancing 
	. Does not require pre-warming
- Global and multi-regional subnetworks.

Create a VM
Compute Engine -> Create 
specificare nome, regione, zone, machine type, 
	Boot disk (immagine, dimensione, tipologia, anche più di uno)
	Identity and API access, 
	networking

	VM access 
Linux:
	- SSH from GCP Console or CloudShell via Cloud SDK 
	- SSH from computer or third-party client and generate key pair 
	- Requires firewall rule to allow tcp:22 (not required in default network)
Windows: RDP
	- RDP clients 
	- Powershell terminal 
	- Requires setting the Windows password 
	- Requires firewall rule to allow tcp:3389 (not required in default network)
	
	VM lifecycle
1. Provisioning <-> Restart
	- Virtual CPUs + Memory
	- Root disk and Persisten disk 
	- Additional disks 
2. Staging
	- IP addresses (internal and external) (Virtual Private Cloud) 
	- System Image (Cloud Storage)
	- Boot 
3. Running <-> Reset
	- Startup Script <-> set/get metadata 
	- Access SSH | RDP 
	- Modify Use <-> 	Export system image 
						Snapshot persistent disk 
						Move VM to different zone 
	- Live migrate
4. Stopping 
	- Shutdown Script 
	- Terminated
		. Delete
		. Availability Policy 

Changing VM state from running:
reset
	from console, gcloud, API, OS 
	state: remains running 
restart
	from console, gcloud, API, OS 
	state: terminated -> running 
reboot
	from: OS:sudo reboot 
	state: running -> running 
stop 
	from: console, gcloud, API 
	state: running -> terminated 
shutdown 
	from: OS:sudo shutdown 
	state: running -> terminated 
delete 
	from: console, gcloud, API 
	state: running -> N/A 
preemption
	method: automatic 
	state: N/A 
	
	Availability policy: Automatic changes
	Called "scheduling options" in SDK/API 

Automatic restart 
	automatic VM restart due to crash or maintenance event
	not preemption or a user-initiated terminate 
On host maintenance 
	determines whether host is live-migrated or terminated due to a maintenance event.
	Live migration is the default.
Live migration 
	during maintenance event, VM is migrated to different hardware without interruption
	metadata indicates occurrence of live migration.

	Stopped (Terminated) VM 
when a VM is terminated:
No charge for stopped VM
	charged for attached disks and IPs 
Actions:
	change the machine type
	migrate the VM instance to another network 
	add or remove attached disks, change auto-delete settings 
	modify instance tags 
	modify custo VM or project-wide metadata 
	remove or set a new static IP 
	modify VM availability policy
	can't change the image of a stopped VM 
Not all of the actions listed here require you to stop a virtual machine.
For example: VM availability policy can be changed while the VM is running.

LAB - Creating Virtual Machines 
	
Compute Engine -> Create 
	Name: utility-vm,
	Region us-central1 and zone us-centra1-c
Andando in edit, nella VM possiamo verificare le impostazioni
modificabili mentre la VM è running.

Per le VM linux in SSH:
> free			per vedere lo spazio libero e altre info 
> sudo dmidecode -t 17 		per vedere info sulla RAM 
> nproc 		numero processori
> lscpu			info sui processori 

Abbiamo tre opzioni per creare e configurare le VM:
1- console.google.com 
2- command line including Cloud Shell 
	> gcloud compute instances create vmName 
2- RESTful API 
	
	Machine types

Predefined machine types: 
	Ratio of GB of memory per vCPU 
	- standard 
	- high-memory 
	- high-cpu 
	- memory-optimized 
	- compute-optimized 
	- shared-core 

Custom machine types:
	- you specify the amount of memory and number of vCPUs. 
creating custom machine types:
when to select custom:
- requirements fit between the predefined types 
- need more memory or more CPU 
customize the amount of memory and vCPU for your machine:
- either 1vCPU or even number of vCPU 
- 0.9 GB per vCPU, up to 6.5 GB per vCPU (default) 
- total memory must be multiple of 256 MB 
choose region and zone.

Pricing:
- per-second billing, with minimum of 1 minute 
	vCPUs, GPUs, and GB of memory 
- resource-based pricing 
	each vCPU and each GB of memory is billed separately 
- discoounts 
	. sustained use 
	. committed use 
	. preemptible VM instances 
- recommendation engine 
	notifies you of underutilized instances 
- free usage limits

Preemptible VM (for batch processing):
- lower price for interruptible service (up to 80%)
- VM might be terminated ad any time 
	no hcarge if terminated in the first minute 
	24 hours max 
	30-second terminate warning, but not guaranteed (time for a shutdown script)
- no live migrate, no autorestart 
- you can request that CPU quota for a region be split between regular and preemption 
	default: preemptible VMs count against region CPU quota. 


	SOLE-TENANT NODES PHYSICALLY ISOLATE WORKLOADS 

Normal host vs Sole tenant host.

A sole-tenant node is a physical compute engine server 
that is dedicated to hosting VM instances only for your specific project,
you sold tennant nodes to keep your instances physically 
separated from instances in other projects or to group your instances 
together on the same host hardware.
For example, if you have a payment processing workload that needs 
to be isolated to meet compliance requirements.

	SHIELDED VMs OFFER VERIFIABLE INTEGRITY 
- secure boot 
- virtual trusted platform module (vTPM)
- integrity monitorin 
Requires shielded image.

	IMAGES 
When creating a Virtua Machine,
you can choose the boot disk image.
This image includes: 
- boot loader 
- operating system 
- file system structure 
- software 
- customizations 

You can select:
- Public base images 
	. Google, third-party vendors, and community; Premim images(=p) 
	. Linux (CentOS, CoreOS, Debian, RHEL(p), SUSE(p), Ubuntu, openSUSE, FreeBSD.
	.Windows
		Windows Server 2019(p), 2016(p), 2012-r2(p) 
		SQL Server pre-installed on Windows(p) 
- CUstom images 
	. Create new image from VM: pre-configured and installed SW 
	. Import from in-prem, workstation, or another cloud 
	. Management features: image sharing, image family, deprecation 


	MACHINE IMAGES (when machine images can be used) 

SCENARIOS				MACHINE  		PERSISTENT 		CUSTOM IMAGE 	INSTANCE 
						IMAGE			DISK SNAPSHOT					TEMPLATE 

Single disk backup 		yes 			yes 			yes				no 
Multiple disk backup 	yes 			no 				no				no 
Differential backup 	yes 			yes 			no 				no 
Instance cloning 		yes 			no 				yes 			yes 
	and replication 
VM instance				yes 			no 				no 				yes 
	configuration

	DISK OPTIONS 
Boot disk:
- VM comes with a single root persistent disk. 
- Image is loaded onto root disk during first boot:
	Bootable: you can attach to a VM and boot from it.
	Durable: can survive VM terminate.
- Some OS images are customized for Compute Engine.
- Can survive VM deletion if "Delete boot disk when instance is deleted" is disabled.

	PERSISTENT DISKS (not attached to the VM)
Network storage appearing as a block device:
- attached to a VM through the network interface 
- durable storage: can survive VM terminate 
- bootable: you can attach to a VM and boot from it 
- snapshots: incremental backups 
- performance: scales with size 
- HDD(magnetic) or SSD(faster, solid-state) options.
- Disk resizing: even runnng and attached.
- can be attached in read-only mode to multiple VMs,
	is cheaper to replicating data for each VM.
- zonal or regional (
	pd-standard: backed by a standard hard disk drive, 
	pd-balanced: backed by a solid state drives, 
		balanced performance and cost as a standard persistent disks
	pd-ssd:backed by solid state drives. 
	) 
- encryption keys (Google-managed, Customer-managed, Customer-supplied).
	by default Google encryot all data for you ad REST.

	LOCAL SSD DISKS are physically attached to a VM.
- more IOPS, lower latency and higher throughput than persistent disk 
- 375-GB disk up to eight, total of 3 TB 
- data survives a reset, but not a VM stop or terminate 
- VM-specific: cannot be reattached to a different VM.

	RAM DISK 
- tmpfs (store data in memory)
- faster than local disk, slower than memory 
	. use when your application expects a file system structure 
		and cannot directly store its data in memory 
	. fast scratch disk, or fast cache
- very volatile, erase on stop or restart 
- may need a larger machine type if RAM was sized for application 
- consider using a persistent disk to back up RAM disk data.


	SUMMARY OF DISK OPTIONS 

				Persistent disk HDD		Persistent disk SSD 	Local SSD disk 		RAM disk 
																(ephemeral)			(ephemeral) 					
Data 			yes						yes						no					no
redundancy

Encryption		yes						yes						yes					N/A 
at rest 

Snapshotting	yes						yes						no 					no	

Bootable		yes 					yes 					no 					not 	

use case 	General, bulk file storage. Very random IOPS.		High IOPS and low latency. 	Low latency and risk of data loss.

	
	Maximum persistent disks 

Machine Type			Disk number limit 

Shared-core 			16

Standard 				128
High-memory 
High-CPU 
Memory-optimized 
Compute-Optimized


Persistent disk management differences:

Cloud Persistent Disk:
- single file system is best 
- resize (grow) disks 
- resize file system 
- built-in snapshot service 
- automatic encryption 

Computer Hardware Disk:
- Partitioning 
- Repartitioning disk 
- Reformat 
- Redundant disk arrays 
- subvolume management and snapshots
- encrypt files before write to disk.



	COMMON ACTIONS WITH COMPUTE ENGINE 

- Metadata and scripts

time: Boot ->  Run ->   Maintenance -> Shutdown
			Metadata Metadata Metadata       Metadata
			startup-script-url=URL 					 shutdown-script-url=URL

For example,
you can write a startup script that gets the metadata key value pair for an instance external IP address
and use that IP address in your script to set up a database.

Because the default Metadata keys are the same on every instance,
you can reuse your script without having to update it for each instance.
This helps you to create less brittle code for your applications.

Storing and retrieving instance metadata is a very common compute engine action. 

I recommend storing the startup and shutdown scripts in cold storage.

- Move an instance to a new zone
	
	. Automated process (moving within region):
		- > gcloud compute instances move 
		- Update references to VM, not automatic
	
	. Manual process (moving between regions):
		- Snapshot all persistent disks on the source VM.
		- Create new persistent disks in destinatione zone restored from shapshots.
		- Create new VM  in the destination zone and attach new persistent disks.
		- Assign static IP to new VM.
		- Update references to VM.
		- Delete the snapshots, original disks, and original VM. 

Snapshot can be used for: 

- Back up critical data
	data -> Snapshot Service(Cloud Storage)

- Migrate data between zones
	data in zone1 -> Snapshot Service -> data in zone2

- Transfer to SSD to improve performance 
	(trasferring data to a different disk type)
	Persistent Disk HDD -> Snapshot Service -> Persistent Disk SSD 

Persistent disk snapshots:
- Snapshot is not available for local SSD
- Creates an incremental backup to Cloud Storage
		Not visible in your buckets, managed by the snapshot service.
		Consider cron jobs for periodic incremental backup.
- Snapshots can be restored to a new persistent disk.
		New disk can be in another region or zone in the same project.
		Basis of VM migration: "moving" a VM to a new zone.
			Snapshot doesn't backup VM metadata, tags, etc.

Another common Compute Engine action is to:

Resize persistent disk

you can grow disks, but never shrink them!!!


LAB - working with Virtual Machine

Creiamo una VM "mc-server"

Access scopes: Set access for each API
Storage: Read Write (this allow the VM to read and write Cloud Storage bucket)
Disks: Add a new disk "minecraft-disk"
type: SSD persistent disk 
Networking: Network tags: minecraft-server 
External IP: Reserve a new static IP address
name: mc-server-ip 

When the instance of the VM is up and running,
open the SSH of the VM:

> sudo mkdir -p /home/minecraft

> sudo mkfs.ext4 -F -E lazy_itable_init=0, lazy_jurnal_init=0,discard /dev/disk/by-id/google-minecraft-disk

> sudo mount -o discard,defaults /dev/disk/by-id/google-minecraft-disk /home/minecraft 

> sudo apt-get update 

> sudo apt-get install -y default-jre-headless 

> cd /home/minecraft 

> sudo wget https://launcher.mojang.com/v1/objects/algrkjbfnaebnfaiul/server.jar 

> sudo java -Xmx1024M -Xms1024M -jar server.jar nogui 

> sudo ls -l 

> sudo nano eula.txt 
		andiamo a modificare il file: eula=true 

> sudo apt-get install -y screen 

> sudo screen -S mcs java _Xmx1024M -Xms1024M -jar server.jar nogui

Nei log: Starting Minecraft server on *:25565 

Aggiorniamo le Firewall Rules per abilitare il traffico sulla porta.

VPC network -> Firewall rules -> Create Firewall rule 
name: minecraft-rule, Target tags: minecraft-server(la stessa della VM)
Source IP ranges: 0.0.0.0/0 (from anywhere) 
Specified protocols and ports: tcp:25565

andando su mcsrvstat.us possiamo inserire l'external IP della VM per verificarne lo stato.

Adesso che il server è up and running, vogliamo scheduralare 
backup.

Rientramo in SSH della VM

> export YOUR_BUCKET_NAME=projectID

> echo $YOUR_BUCKET_NAME 

> gsutil mb gs://$YOUR_BUCKET_NAME-minecraft-backup 

> cd /home/minecraft

> sudo nano /home/minecraft/backup.sh 
		gnòeakjdfvn.eshgfnoòàriJSLEKFMROahgnkla.ghq	adksn.

> sudo chmod 755 /home/minecraft/backuop.sh 

> . /home/minecraft/backup.sh 

> sudo chrontab -e
	con nano possiamo impostare nel file:
	*/4 * * * /home/minecraft/backup.sh (backup automatico ogni 4 ore)

OSS: per non accumulare i backup impostare il lifecycle per l'autocancellazione da Cloud Storage.

Adesso stoppiamo la VM, entriamo nella tab della VM e andiamo in edit.
Custom metadata: 

key:startuoo-script-url, 
value: the location of the file backup.sh (https://storage.googleapis.com/cloud-training.../startup.sh)

key: shutdown-script-url
value: http://..../shutdown.sh 

Al restart troveremo la stessa configurazione.

That's the end of the laaaaab!

-------------------------------------------------------------
ESSENTIAL GOOGLE CLOUD INFRASTRUCTURE: CORE SERVICES


	CLOUD IDENTITY AND ACCESS MANAGEMENT ( CLOUD IAM )	

IAM is a way of identifying who can do what on which resource.

WHO: person, group, application

WHAT: refers to specific privileges or actions 

RESOURCES: any Google Cloud service

	Cloud IAM objects:

Organization, Folders, Projects, Resources, Roles, Members.

Cloud IAM policies:
- a policy consists of a list of bindings
- a binding binds a list of members to a role.

Cloud IAM resource hierarchy 

Organization > Folders > Projects > Resources 

Each resource has exactly one parent.
L'organizzazione può rappresentare l'azienda,
i Folder rappresentano i dipartimenti, 

A Policy is a cllection of access statements attached to a resource.
Each policy contains a set of roles and role members.
Resources inheriting plicies from their parent.

Resource policies are union of parent and resource,
where a less restrictive parent policy will always override a more restrictive resource policy

The Cloud IAM policy hierarchy always follows the same path 
as the Google Cloud resource hierarchy,
which means that if you chiange the resource hierarchy
the policy hierarchy also changes.

For example,
moving a project into a different organizzation
will update the projects Cloud IAM policy to inherit from the new organization Cloud IAM policy.

Also child policies cannot restrict access granted at the parent level. 

For example,
if we grant you the editor role for department X
and we grant you the viewer role at the bookshelf project level,
you still hav the editor role for that project.
Therefore it is a best practive to follow the principle of
least privilege.
The principle applies to identities, roles and resources
always select the smallest scope that is necessary for the task, in order to reduce your exposure to risk.

	Organization Policies

An organization policy is:
- A configuration of restrictions 
- defined by configuring a contraint with desired restrictions 
- applied to the organization node, folders or project,
descendants of the targeted resourcehierarchy node inherit policy orgatization
that's been applied to their parents,

Exceptions to these policies can be made,
but only if the user ha the organization policy admin role.

	IAM Conditions

IAM conditions allow you to define and enforce conditional attribute based access control for Google Cloud Resources.
With IAM conditions you can choose to grant resource access to identities members 
if configured conditions are met.
For example,
this can be done by configuring temporary access for users
in the event of a production issue or
to limit access to resources only for employe's making request from your corporate office. 
Conditions are specified in the role bindings of a resource IAM policy.
When a condition exists, the access request is only granted
if the condition expression evaluates to true. 
Each condition expression is defined as a set of logic statements,
allowing you to specify one ore more attributes to check.

Enforce conditional, attribute-based access control for Google Cloud resources.
- Grant resource access to identities (members) only 
	if configured conditions are met.
- Specified in the role bindings of a resource's IAM policy.

	 Organization node

- An organization node is a root node for Google Cloud resources
- Organizarion roles:
	. Organization Admin:
		Control over all cloud resources, useful for auditing.
	. Project Creator:
		Controls project creation, control over who can create projects.

	Creating and managing organizations

- Created when a Google Workspace or Cloud Identity account creates a Google Cloud Project

- Workspace or Cloud Identity super administrator:
	. Assign the Organization admin role to some users
	. Be the point of contact in case of recovery issues
	. Control the lifecycle of the Workspace or Cloud Identity account and Organization resource

- Organization admin:
	. Define IAM policies
	. Determine the structure of the resources hierarchy
	. Delegate responsibility over critical componenets such as Networking, Billing, Resource Hierarchy 
		through IAM roles.

	Folders

	Additional grouping mechanism and isolation boundaries between projects:
	- Differente legal entities
	- Departments
	- Teams
	Folders allow delegation of administration rights.

	Resource manager roles:

	- Organization:
		. Admin: Full control over all resources
		. Viewer: View access to all resources
	- Folder:
		. Admin: Full control over folders
		. Creator: Browse hierarchy and create folders
		. Viewer: view folders and projects below a resource
	- Project:
		. Creator: Create new projects (automatic owner) and 
			migrate new projects into organization
		. Deleter: Delete projects



	Roles:
	- Basic role
	- Predefined role
	- Custom role

++IAM basic roles apply across all Google Cloud services in a project
	can do what on all resources

IAM basic roles offer fixed, coarse-grained levels of access:

- Owner role (includes Editor role and Viewer role)
	. invite members
	. remove members
	. delete projects
- Editor role (includes Viewer role)
	. deploy applications
	. modify code
	. configure services
- Viewer
	. read-only access

- Billing Administrator
	. manage billing
	. add and remove administrators

++IAM predefined roles apply to a particular GCP service in a project
	can do what on Compute Engine resources in this projects, or folder, or org

IAM predefined roles offer more fine-grained permissions 
	on a particular services

	Google Group
		InstanceAdmin Role -> List of Permissions
			project_a

List of Permissions:
	compute.instances.delete
	compute.instances.get
	compute.instances.list
	compute.instances.setMachineType
	compute.instances.start
	compute.instances.stop

	Compute Engine IAM roles
Compute Engine has serveral predefined IAM roles:

Compute Admin:		Full control of all Compute Engine resources (compute.*), 
		which means that every action for any type of compute engine resource is permitted.
Network Admin: 		Permissions to create, modify and delete neworking resources,
		except for firewall rules and SSL certificates.
Storage Admin:		Permissions to create, modify and delete disks, images and snapshots.

++IAM custom roles let you define a precise set of permissions

	Google Group
		Instance Operator Role -> Listo of Permissions
			project_b

List of Permissions:
	compute.instances.get
	compute.instances.list
	compute.instances.start
	compute.instances.stop


	How to create a Custo Role in GCP

create an instance operator role that allows some users
to start and stop Compute Engine Virtual Machines,
but not reconfigure them.

From GCP Console -> IAM & admins -> Roles -> 
 select "Access Approval Config Editor" 
	in the right panel you can see the assigned permissions,
 click "Create Rome From Selection"
 or click "Create Role"

 Let's click "Create Role"
 name: Instance Operator
 description: create on current date

 ID: CustomRole
 Role launch stage: Alpha, Beta, General Availability, Disabled (choose Alpha)
 +Add permissions: compute.instances.get, compute.instances.list, compute.instances.resume, compute.instances.reset,compute.instances.start, compute.instances.stop, compute.instances.suspend

 Create!




 Members defines the who part 
of who can do wht on which resource

There are 5 different types of Members:
1- Google Account
2- Service Account
3- Google Group
4- Gsuite Domain
5- Cloud Identity Domain

Google Account represents a developer and administrator or
any other person who interacts with the GCP.
Any email address that is assoicated with a Google Account can be an identity including gmail.com or other domains.
New users can sign uo for a Google Account by going to the Google Account sign up pag without receiving mail through gmail.

A Service Account is an account that belongs to your application instaad of to an individual end user.
When you run code that is hosted on GCP,
you specify the account that the code should run as,
you can create as many service accounts as needed to represent the different logical components of your application.

A Google Group is a named collection of Google Accounts and Service Accounts,
every group has a unique email address that is associated with the group.
Google Groups are a convenient way to apply an access policy to a collection of users,
you can grant and change access controls for a whole group at once, instead of granting or changing access controls one at a time 
for indivifual users or service accounts.

G Suite domains represent your organization's internet domain name,
such as example.com.
And when you add a user to your G Suite domain, 
a new Google Account is created for the user inside this virtual group, souch as username@example.com.

GCP customer who are not G Suite customers can get these same capabilities through Cloud Identity.
Cloud Identity lets you manage users and groups using the Google admin console,
but you do not pay for or receive G Suite Collaboration products such as Gmail, Docs, Drive and Calendar.
Cloud Identity is available in free and premium editions.

The Premium edition adds capabilities for mobile device management,
you cannot use Cloud IAM to create or manage your user or groups
in stead you can use Cloud Identity or Gsuite to create and manage users.

		Using Google Cloud Directory Sync
your administrators can log in and manage GCP resources 
using the same user names and passwords they already use.
This tool synchronizes users and groups from your existing active directory
or held up system with the users and groups in your Cloud Identity domain. 
The synchronization in one way only,
which means that no information in your active directory or held
at map is modified. 
Google Cloud directory is designed to run scheduled synchronizationwithout supervision, 
after its synchronization rules are set up.

What if I already have a different corporate directory?

Microsoft Active 						Google Cloud
Directory or LDAP 					Directory Sync 						Users and groups in your 
										----------------------------->>> 	Cloud Identity domain
Users and groups in 		scheduled one-way sync
your existing 
directory service


	Single sign-on (SSO)
GCP also provide single sign on authentication.
- Use Cloud Identity to configure SAML SSO
- If SAML2 isn't supported, use a third-party solution (ADFS, Pin, or Okta)

IF you have your identity system,
you can continue using your own system and processes with
SSO configured, when user authentication is required
Google will redirect to your system.

If the user is authenticated in your system,
access to Google Cloud Platform is given.
Otherwise the user is prompted to sign in.
This allow you to also revoke access to GCP, 
if your existing authentication system supports SAML to SSO configuration
is as simple as three links and a certificate.
Otherwise you can use a third party solution like ADFS, Ping or Okta.

If you want to use a Google Account but are not interested
in receiving mail through Gmail,
you can still create an account without gmail.

	Service Account
Service accounts provide an identity for carrying out
server-to-server interactions

- Programs running within Compute Engine instances can automatically acquire access tokens with credentials.
- Tokens are used to access any service API in your project and any other services that granted access to that service account.
- Service accounts are convenient when you're not accessing user data.

For example,
if you write an application taht interacts with Google Cloud Storage,
you must first authenticate either Google Cloud Storage 
XML API or JSON API.
You can enable Service Accounts and grant read-write access to
the account on the instance where you plan to run your application.
Then program the application to obtain credentials from the Service Account.
Your application authenticates seamlessly to the API 
without embedding any secret keys or credentials in your instance image or application code. 

Service accounts are identified by an email address
- 123455555556-compute@project.gserviceaccount.com
- Three types of service accounts:
	- User-created (custom)
	- Built-in
		Compute Engine and App Engine default service accounts
	- Google APIs service account
		Runs internal Google processes on your behalf.

By default, all projects come with a built-in Compute Engine,
default Service Account.

Apart from the default Service Account,
all projects come with a Google Cloud APIs Service Account,
identifiable by 
the email project - number @ Cloud Services . G Service Account .com
this is an account designed specifically to run internal Google Processes 
on your behalf and is automatically granted the editor role on the project.

Alternatively,
you can also start an instance with a Custom Service Account,
that provide more flexibility tham the Default Service Account,
but they require more management from you.
You can create many service accounts as you need, 
sign any arbitrary access scopes or Cloud IAM roles to them
and assigne the service accounts to any virtual machine instance.

	Default Compute Engine Service Account

- Automatically created per project with auto-generated name and email address:
	name has -compute suffix
	56478103875151-compute@developer.gserviceaccount.com
- Automatically added as a project Editor
- By default, enabled on all instances created using gcloud or Cloud Console

You can override this behavior by specifing another service account
or by disabling service accounts for the instance.

	Scopes

Authorization is the process of determining
what permissions an auhenticated identity has
on a set of specified resources.
Scopes are used to determine whether an authenticated identity
is authorized.

Each application contais authenticated identities or service account.
If the application want to use a Cloud Storage bucket.
They each request access from the Google Authorization Server, 
and in return they receive an access token.
Each application receives an access token with a specific scope.

	Customizing scopes of a VM
- Scopes can be changed after an instance is created
- For user-created service accounts, use Cloud IAM reles instead.

Scope can be customized when you create an instance
using the default Service Account.
Access scopes are actually a legacy method of specifying permissions for your VM.
Before the existence of IAM roles access scopes were 
the only mechanism for granting permissions to service accounts.
For user created Service Accounts use Cloud IAM roles instead
to specify permissions.

	Service account permissions
- Default Service Accounts: basic and predefined roles
- User-created Service Accounts: predefined roles
- Roles for service accounts can be assigned to groups or users

First you create a Service Account that has 
the InstanceAdmin Role,
which has some permissions to create, modify and delete 
Virtual Machine instances and disks.
Then you treat the Service Account as a resource and
decide who can use it by provsioning users or a group
with the Service Account user role.
This allows those users to act as that Service Account 
to create, modify and delete Virtual Machine instances and disks.
Users who are Service Account users for a Service Account
can access all of the resources that the Service Account has access to.
Therefore be cautious form granting the Service Account user role
to a user or group.

	Example: Service Accounts and Cloud IAM

- in project_a there are component_1 and component_2
- VMs running component_1 are granted Editor access 
	to project_b using Service Account_1
- VMs running component_2 are granted ObjectViewer access to
	bucket_1(inside project_a) using Service Account_2
- Service account permissions can be changed without re-created VMs.

Essentially, Cloud IAM lets you slice up a project
into different microservices, each when access to different
resources by creating service accounts to represent each one.
You assign the Service Accounts to the VMs when they are created
and you don't need to ensure the credentials have been 
managed correctly because Google Cloud manages security 
for you.

	There are two types of Google Service Accounts:
1- Google-managed service accounts:
	. all service accounts have Google-managed keys
	. Google stores both the public and private portion of the key
	. each public key can be used for signing for a maximum of two weeks
	. private keys are never directly accessible

2- User-managed service accounts:
	. Google only stores the public portion of a user-managed key
	. users are responsible for private key security
	. can create up to 10 user-managed service account keys per service
	. can be administered via Cloud IAM API, gcloud, or the Console.

Keeping your user-managed keys safe is vital
and is the creator's responsibility.
Remember: Google does not save your user-managed private keys,
if you lose them, Google cannot help you recover them.
And also you are responsible for performing KEY ROTATION.

User managed keys should be used as a last resort.
Consider the other alternatives,
such as Short Lived Service Account credentials, tokens 
or Service Account impersonation.

	Use the "gcloud" command-line tool to quickly list
		all of the keys associated with a Service Account:
> gcloud iam service-accounts keys list
		--iam-ccount user@email.com

		Cloud IAM best practices

1- Leverage and understand the resource hierarchy
	- use projects to group resources that share the same trust bundary
	- check the policy granted on each resource and make sure you understand the inheritance
	- use "principles of least privilege" when granting rols
	- audit policies in Cloud audit logs: setiampolicy
	- audit membership of groups used in policies 

2- Grant roles to Google groups instead if individuals
	- update group membership instead of changing Cloud IAM policy
	- audit membership of groups used in policies
	- control the ownership of the Google group used in Cloud IAM policies

	Example:
_______________
|Network 			|			-> Group_1 needing view_only role to a Cloud Storage bucket
|Admin Group 	|			-> Group_2 needing read_ony role to a Cloud Storage bucket
_______________

Therefore groups are not only associated with job roles
but can exist for the purpose of role assignment.

3- Service Accounts
	- Be very careful granting "serviceAccountUser" role
		because it provides access to all resources of the service account has access to
	- when you create a service account, give it a display name
		that clearly identifies its purpose
	- establish a naming convention for service accounts
	- establish key rotation policies and methods
	- audi with "serviceAccount.keys.list()" method 

4- Cloud Identity-Aware Proxy (Cloud IAP)
	enforce access control policies for applications and resources:
	- identity-based access control
	- central authorization layes for applications accessed by HTTPPS
	Cloud IAM policy is applied after authentication.

--------------------------------------
					_______________________
					|			IDENTITY 				|GCP
					|				|							|
					|				V 						|
Users  ---|--> 	 IAP 	-> 	ERP 	|
					|						->	CRM 	|
					|											|
					_______________________
---------------------------------------
Cloud IAP lets you establish a central authorization layer 
for applications access by HTTPS,
so you can use an application level access control model,
instead of relying on network level firewalls.

Applications and resources protected by Cloud IAP
can only be accessed through the proxy by users and groups
with the correct Cloud IAM role.
When you grant a user access to an application ore resources by Cloud IAP,
thes're subject to the fine grained access controls 
implemented by the product in use without requiring a VPN.

Cloud IAP performs authentication and authorizaton checks,
when a user tries to access a Cloud IAP secured resource.


LAB - Cloud IAM 

Abbiamo due utenti, con due username.
Facciamo quindi il login in due tab diverse con i due diversi account.

In username_1 tab, 
	IAM & admin -> IAM 
	roles: 	App Engine Admin
					BigQuery Admin
					Edior
					Owner
					Viewer

	Cloud Storage -> Storage -> create bucket with project_id
	upload file
	rename: sample.txt


In username_2tab, 
	IAM & admin -> IAM 
	roles: 	Viewer

	Cloud Storage -> Storage -> Browser 
	verifichiamo la visibilità del bucket creato con username_1 e del suo file all'interno


Rimuoviamo il "project viewer role" per lo username_2
In username_1 tab, 
	IAM & admin -> IAM -> edit username_2
	remove the role and save

entrando come username_2 non vedremo più il bucket.

Adesso concediamo a username_2 il ruolo: Storage Object Viewer

Dalla Shell di username_2:

> gsutil ls gs://bucketname

---
In username_1 tab, 
	IAM & admin -> Service Accounts -> create
name: read-bucket-objects
role: Storage Object Viewer

	IAM & admin -> IAM
selezioniamo il Service Account creato
e clicchiamo su +ADD
consideriamo l'esempio
New members: altostrat.com
role: Service Account User

+ADD
consideriamo l'esempio
New members: altostrat.com
role: Compute Engine - Compute instance admin v1

Creiamo una VM con il Service Account appena creato.

Dall'SSH della VM:

> gcloud compute instances list
	
> gsutil cp gs://bucketname/sample.txt
> mv sample.txt sample2.txt 
> gsutil cp sample2.txt gs://bucketname


--------------------------------------------------------

		STORAGE AND DATABASE SERVICES

Unstructured data:
	Object:	Cloud Storage 
		good for binary or object data
		such as images, media serving, backups
	File: Filestore 
		file system
		good for: network attached storage (NAS)
		such as latency sensitive workloads

Relational:	
	Cloud SQL 
		good for web frameworks
		such as CMS, ecommerce
	Cloud Spaner
		horizonal scalability
		good for RDBMS + scale, HA, HTAP
		such as user metadata, Ad/Fin/MarTech

Non-relational
	Firestore
		good for hierarchical, mobile, web
		such as user profiles, game state
	Cloud Bigtable
		analytics
		update and low latency
		good for heavy read + write, events
		such as financial, IoT

Warehouse: BigQuery 
	analytics
	good for enterprise data warehouse
	souch as analytics, dashboards

	Scope

Infrastucture Track
- Service differentiators
- when to consider using each service
- set uo and connect to a service

Data Engineering Track
- How to use a database system
- Design, organization, structure, schema,
	and use for an application
- Details about how a service stores and retrieves
	structured data

Cloud Storage is an object storage service
- website content
- storing data for archiving and disaster recovery
- distribuiting large data objects to users via direct download

Cloud Storae key features
- Scalable to exabytes
- Time to first byte in milliseconds
- Very high availability across all storage classes
- Single API across storage classes 

Cloud Storage is not a File System
instead it is a collection of buckets that you place objects into.

Classes:

Standard
	- use cases: frequently access, most expensive, hot data and or stored for only bief periods of time like data-intensive computations
	- minimum storage duration: none
	- retrieval cost: none
	- availability SLA: 99.95%(multi/dual) 99.90%(region)
	- durability: 99.999%

Nearline
	read less than once per 30 days
	- use cases: infrequently accessed data like data backup, long-tail multimedia content, and data archiving
	- minimum storage duration: 30 days 
	- retrieval cost: $0.01 per GB 
	- availability SLA: 99.90%(multi/dual) 99.00%(region)
	- durability: 99.999%

Coldline
	read less than once per 90 days
	- use cases: infrequently accessed data that you read or modify at most once a quarter 
	- minimum storage duration: 90 days 
	- retrieval cost: $0.02 per GB 
	- availability SLA: 99.90%(multi/dual) 99.00%(region)
	- durability: 99.999%

Archieve
	read less than once per year
	- use cases: Data archiving, online backup, and disaster recovery 
	- minimum storage duration: 365 days 
	- retrieval cost: $0.05 per GB 
	- availability SLA: None
	- durability: 99.999%

Choose a regional class to help optimize latency
and network bandwidth for data consumers such as analytics and pipelines
that are grouped in the same region.

Use a dual region when you want similar performance advantages as regions
but you also want the high availability that comes
with being geo redundant.

Use a multi region when you want to serve content
to data consumers that are outside of the google network and disributed across large geographic areas,
or when you want the higher data availability that comes with being geo redundant.



	Overview
Buckets
- naming requirements
- cannot be nested
Objects
- inherit storage class of bucket when created
- no minimum size, unlimited storage
Access
- gsutil
- RESTful JSON API or XML API 

	Changing default storage classes
- default class ia applied to new objects
- regional bucket can never changed to multi-region/dual-region
- multi-regional bucket can never be changed to regional
- Objects can be moved from bucket to bucket
- Object Lifecycle Management can manage the classes of objects

You can change the storage class of a bucket.
you can also change the storage class of an object without moveing into another bucket.

	Access control
Object -> Bucket -> Project

- Cloud IAM (Cloud Identity and Access Management)
- Access Control Lists (ACLs)
- Signed URL (signed and timed cryptographic key)
- Signed policy document (controls file upload policy)
Can be used together.

	Access Control Lists (ACLs)
ACLs is a mechanism you use to define
who has access to your buckets and objects
as well as what the level of access they have.
The maximum number of cells and tissues can create for a bucket or object is 100.
Each ACL consists of one or more entrie and these entrie consists of two pieces of information:
1- A Scope which defines who can perform the specified actions.
2- A Permission which defines what actions can be performed.

	Signed URLs
- Valet key access to buckets and objects via ticket:
	. Ticket is a cryptographically signed URL
	. Time-limited
	. Operations specified in ticket: HTTP GET, PUT, DELETE (not POST)
	. any user with URL can invoke permitted operations
- Example using private account key and gsutil:
	gsutil signurl -d 10m path/to/privatekey.p12 gs://bucket/object

	Cloud Storage features 
- Customer-supplied encryption key (CSEK)
		use your own key instead of Google-managed keys
- Object Lifecycle Management
		automatically deleted or archive objects
		examples
			downgrade storage class on objects older than a year
			delete objects created before a specific date
			keep only the 3 most recent versions of an object
		object inspection occurs in asynchronous batches
		changes can take 24 hours to apply
- Object Versioning
		mintain multiple versions of objects
		objects are immutable
		maintain a history of modifications of objects
		list archived versions of an object, restore an objets an older state or delete
		turning versioning off leaves existing object versions in place and causes the bucket to stop accumulating new archived object versions
- Directory synchronization
		synchtonizes a VM directory with a bucket
- Object change notification
- Data import
- Strong consistency

Object change notification can be used to notify
an application when an object is updated or added to a bucket.
Recommendd: Pub/Sub Notifications for Cloud Storage
Notification Channel

	Data import services:
- Trasfer Appliance
	rack, capture and then ship your data to Google Cloud
- Storage Transfer Service
	import online data (another bucket, an S3 bucket, or web service)
- Offline Media Import
	third-party provider uploads the data from physical media

Cloud Storage provides strong global consistency
- read-after-write
- reaf-after-metadata-update
- read-after-delete
- bucket listing
- object listing


	Filestore is a managed file storage service
for applications that require a file system interface
and a shared file system for data.

- fully managed network attached storage (NAS) for 
	Compute Engine and GKE instances.
- predictable performance
- full NSFv3 support
- scales to 100s of TBs for high-performance workloads

use cases:
- application migration
- media rendering
- Electronic Design Automation (EDA)
- Data analytics
- genomics processing
- web content management


LAB - Cloud Storage


Creiamo un bucket
nome projectId, multi-regional
access control model:
	set object-level and bucket-level permissions

Da Cloud Shell
> export BUCKET_NAME_1=...
> curl http://hadoop.apache.org/docs/current/hadoop-project-dist/hadoop-common/ClusterSetup.html

> cp setup.html setup2.html
> cp setup.html setup3.html

> gsutil cp setup.html gs://$BUCKET_NAME_1 

> gsutil acl get gs://$BUCKET_NAME_1/setup.html > acl.txt
> cat acl.txt

> gsutil acl set private gs://$BUCKET_NAME_1/setup.html
> gsutil acl get gs://$BUCKET_NAME_1/setup.html > acl2.txt

> cat acl2.txt

> gsutil acl ch -u AllUsers:R gs://$BUCKET_NAME_1/setup.html

> gsutil acl get gs://$BUCKET_NAME_1/setup.html > acl3.txt

> cat acl3.txt

> rm setup.html
> ls

> gsutil cp gs://$BUCKET_NAME_1/setup.html setup.html

		CREATE THE KEY
> python -c 'import base64; import os; print(base64.encodestring(os.urandom(32)))'
		copy the key

> gsutil config -n 
> nano .boto
		paste the key
		encryption_key=...

> gsutil cp setup2.html gs://$BUCKET_NAME_1

> rm setup*
> gsutil cp gs://$BUCKET_NAME_1/setup* ./
.
.
.

		ENABLE VERSIONING
> gsutil lifecycle set lige.json gs://$BUCKET_NAME_1

> gsutil lifecycle get gs://$BUCKET_NAME_1

> gsutil versioning get gs://$BUCKET_NAME_1

> gsutil versioning set on gs://$BUCKET_NAME_1

.
.
.


		CLOUD SQL
Build your own database solution or use a managed service in Compute Engine

Cloud SQL is a fully managed database service (MySQL, PostgreSQL, Microsoft SQL Server	)
- Patches and updates automatically applied
- you administer MySQL users
- Cloud SQL supports many clients:
	. gcloud sql
	. App Engine, Google Workspace scripts
	. Applications and tools
		- SQL Workbench, Toad
		- External applications using standard MySQL drivers

Performance:
- 30 TB of storage
- 40000 IOPS
- 416 GB of RAM
- scale out with read replicas
- scale up: machine capacity

Choice:
- MySQL 5.6, 5.7(default), 8.0
- PostgreSQL 9.6, 10, 11, 12(default)
- Microsoft SQL Server 2017

Cloud SQL services:
- HA configuration
- backup service
- import/export
- scaling up(machine capacity) and out(read replicas)

Può essere conssesso all'interno della stessa Google Cloud region
attraverso il suo Private IP,
dall'esterno con l'utilizzo dei certificati SSL oppure Authorized Networks oppure Cloud SQL Proxy.

LAB - CLOUD SQL

nella VPC ci sono due region:
europe-west1 con Wordpress
us-central1 con Wordpress e Cloud SQL

Possiamo connettere Wordpress e Cloud SQL all'interno della stessa region
mediante il loro Private IP.
Per connettere Wordpress e Clodu SQL in due region differenti
dobbiamo utilizzare gli External IP address.
Entrambe le connessioni sono encrypted.
La VPC è cmq globale comprende entrambe le region.

Entro in SSH della VM che si trova nella region europe-west1
e scarico il Cloud SQL Proxy e lo rendo eseguibile

> wget https_//dl.google.com/cloudsql/cloud_sql_proxy.linux.amd64 -O cloud_sql_proxy && chmod +x cloud_sql_proxy

> export $SQL_CONNECTION=incollo la instance connection name dell'istanza di Cloud SQL creata, che trovo all'interno delle info dell'istanza

> ./cloud_sql_proxy -instances=$SQL_CONNECTION=TCP:3306 &

> curl -H "metadata-Flavor: Google" http://169.254.169.254/computeMetadata/v1/network-interfaces/0/access-configs/0/external-ip && echo
visualizzo l'ip esterno della vm
attraverso il quale posso accedere alla vm
e visualizzare wordpress a cui è connesso il db

Ora provo con la VM che si trova in us-central1
mi copio il private IP dell'istanza di Cloud SQL
che utilizzerò nel campo datbase host in fase di configurazione
di word presso senza la necessità di abilitare il proxy

"so I created a direct connecion to a private IP 
instead of configuring proxy and that connection is private"


		CLOUD SPANNER
Se Cloud SQL non è sufficiente perchè hai bisogno di scalabilità orizzontale,
puoi considerare Cloud Spanner.

Cloud Spanner combines the benefits of relational database structure
with non-relational horizontal scale
- scale to petabytes
- transactional consistency
- global scale
- strong consistency
- high availability
- used for financial and inventory applications
- monthly uptime
		multi-regional: 99.999%
		regional: 99.99%

		Characteristics

				Cloud Spanner		|	Relational DB		|	Non-Relational DB
___________________________________________________________________________
Schema				yes			|		yes				|	no
SQL					yes			|		yes				|	no	
Consistency			strong		|		strong			|	eventual
availability		high			|		failover			|	high
scalability			horizontal	|		vertical			|	horizontal
replication			automatic	|		configurable	|	configurable



		CLOUD FIRESTORE
Cloud Firestore is a NoSQL document database

- simplifies storing, syncing, querying data
- mobile, web, IoT apps at globale scale
- live synchronization and offline support
- security features
- ACID transactions
- Multi-region replication
- Powerful query engine

Cloud Firestore is the next generation of Cloud Datastore

Datastore mode (new server projects):
- compatible with Datastore applications
- strong consistency
- no entity group limits

Native mode (new mobile and web apps):
- strongly consistent storage layer
- collection and document data model
- real-time updates
- mobile and web client libraries

----	CLOUD BIGTABLE
if you don't require transactional consistency 
you might want to consider Cloud BigTable

It is a NoSQL big data database service (fully managed)
- petabyte-scale
- consistency sub-10ms latency
- seamless scalability for throughput
- learn and adjust to access patterns
- ideal for Ad Tech, Fintech and IoT
- storage engine for ML applications
- easy integration with open source big data tools

Processing is separated from storage
	ci sono più Bigtable node che si occupano del processo
	che accedono ad un unico Colossus file system


	CLOUD MEMORYSTORE 
is a fully managed Redis service

- in memory data store service
- focus on building great apps
- high availability, failover, patching and monitoring
- sub-millisecond latency
- instances up to 300GB
- network throughput of 12 gbps
- easy lift-and-shift


	RESOURCE MANAGEMENT

 Resource Manager lets you hierarchically manage resources

 - Identity and Access Management (IAM)
 GCP -> Organization -> Folders -> Projects -> Resources -> VM instance

 	Child policies cannot restrict access granted at the parent level

 - Billing and Resource Monitoring
 GCP <- Organization <- Folders <- Projects <- Resources <- VM instance
 
 	Oragnization contains all billing accounts

	 Project is associated with one billing account

 	A resource belongs to one and only one project

---

Organization node is root node for GCP resources

Project accumulates the consumption of all its resources
- track resource and quota usage
	. enable billing
	. manage permissions and credentials
	. enable services and APIs
- projects use three identifying attributes
	. project name
	. project number
	. project ID, also known as Application ID

---

	Resource Hyerarchy

Resources are global, regional or zonal

Examples:
Global: iamges, snapshots, networks
Regional: external IP addresses
Zonal: Instances, Disks

Regardless of the type, each resource is organized into a project.
This enables each project to have its own billing and reporting.

__QUOTAS

All resources are subject to project quotas or limits
- how many resources you can create per project (5 VPC networks/project)
- how quickly you can make API requests in a project: rate limits (5 admin actions/second Cloud Spanner)
- how many resources you can create per region (24 CPUs region/project)

Project quotas prevent runaway consumption in case of an error or malcius attac

Prevent billing spikes or surprises

Forces sizing consideration and periodic review

___LABELS

- Attached to resources: VM, disks, snapshot, image 
	(by GCP console, gcloud command line, API)
- Example uses of labels:
Inventory
Filter resources
In scripts:
	help analyze costs
	run bulk operations

Use labels for

- Team oc Cost Center 	
	team:marketing
	team:research
- Components
	component:redis
	component:frontend
- Environment or stage
	environment:prod
	environment:test
- Owner or contact
	owner:gaurav
	contact:opm
- State
	state:inuse
	state:readyfordeletion

___BILLING
Because the consumption of all resources under a project
accumulates into one billing account.
Let's talk billing to help project planning and controlling costs you can set a budget.

Setting a budget lets you track how your spend is growing 
towards that amount.

Labels can help you optimize GCP spend

SELECT
	TO_JSON_STRING(labels) as labels,
	sum(cost) as cost
FROM 'project.dataset.table'
GROUP BY labels;

Per creare un avviso,
andare in Billing -> Budgets & alerts -> Create budget
Nella tab Transactions possiamo vedere le transazioni relative ad un Billing Account
Nella tab Billing export è possibile esportare in BigQuery oppure in CSV o JSON file in un bucket di Cloud Storage.


Possiamo analizzare con BigQuery i dati esportati da Billing export.


	RESOURCE MONITORING

Google Cloud's operations suite( preciously Stackdriver)
- integrated monitoring, logging, diagnostics
- manages across platforms:
	. Google Cloud and AWS
	. dynamic discovery of Google Cloud with smart defaults
	. Open-source agents and integrations
- Access to powerful data and analytics tools
- collaboration with third-party software

Offers Services:
	Monitoring, Logging, Error Reporting, Trace, Debugger

Site Reliability engineering

Product -> Development -> Capacity Planning -> Testing + Release Procedures ->
	-> Postmortem/Root Cause Analysis -> Incident Response -> Monitoring

	Monitoring

- dynamic config and intelligent dafaults
- platform, system and application metrics
	. ingests data: Metrics, events, metadata
	. generates insights through dashboards, charts, alerts
- Uptime/health checks
- Dashboards
- Alerts

Workspae is the root entity that holds monitoring and configuration information.
Each workspace can have between one and one hundred monitored projets including one or more GCP projects 
and any number of AWS accounts.
You can have as many workspaces as you want
but GCP projects and AWS accounts can't be monitored by more than one workspace.

A workspace contains custom dashboards, alerting policies, uptime checks, notification channels and 
group definitions that you use with your monitored projects.

A workspace can access metric data from its monitored projects
but the metric data and log entries remain in the individual projects.

The first monitored GCP projct in a workspac is called The Hosting Project, and it must be specified when you create the workspace,
the name of that project is the name of the workspace.

To access to the AWS account you must configure a project in GCP to hold the AWS connector.
Because workspaces can monitor all of your GCP projects in a single place,
a Workspace is a single pane of glass through which you can view resources from multiple GCP projects and AWS accounts.

All Stackdriver users who have access to that workspace have access to all data by default.
This means that a stack driver role assigned to one person on one project applies equally to all projects monitored by that workspace.
In order to give people different roles per projects
and to control visibility to data,
consider placing the monitoring of those projects in separate workspaces.

Stackdriver monitoring allows you to create custom dashboards
that contain charts of the metrics that you want to monitor.
For example, you can create charts that display: 
- your instances CPU utilization, 
- the packets or bytes sent and received by those instances,
the packets are bites dropped by the firewall of those instances.

Charts provide visibility into the utilization
and network traffic of your VM instances,
these charts can be customized with filters to remove noise, groups to reduce the number of time series and
aggregates to group multiple time series together.

Although charts are extremely useful,
they can only provide insight when someone is looking at them.
but what if your server goes down in the middle of the night or over the weekend?
You want to create alerting policies then notify you 
when specific conditions are met.

For example, you can create an alerting policy when
the network egress of your VM instance goes above a centain threshold for a specific time frame.
When this condition you or someone else can be automatically notified through email, sms or other channels, in order to troubleshoot this issue.

You can also create an alerting policy that monitors your stackdriver usage
and alerts you when you approach the threshold for billing.

Uptime checks can be configured to test the availability
of your public services form locations around the world.
The type of uptime check can be set to http, https or TCP.
The resource to be checked can be
	App Engine application,
	Compute Engine instance,
	URL of a host
	AWS instance
	loadbalancer.
For each uptime check, you can create an alerting policy
and view the latency of each global location.

	Monitoring agent can access to VM instances.

Stackdriver monitoring can access some metrics without the monitoring agent including CPU utilization, some disk traffic metrics,
network traffic and up time information.

However, to access additional system resources and application services,
you should install the monitor agent.
The monitor agent is supported for Compute Engine and 
EC2(AWS) instances.
The monitor agent can be installed with these two simple commands
wich you could include in your startup script:

> curl -sSO https://dl.googl.com/cloudagents/add-monitoring-agent-repo.sh
> sudo bash add-monitoring-agent-repo.sh

This assume that you have a VM instance running Linux
that is being monitored by a workspace
and that your instance has the proper credentials for the agent.

If the standard metrics provided by Stackdriver Monitoring (now Cloud Monitoring), do not fit your needs,
you can create custom metrics.

For example, imagine a game server that has a capacity of 50 users,
what metric indicator might you use to trigger scaling events?
From an infrastructure perspective,
you might consider using CPU load or perhaps network traffic load as values that are somewhat correlated
with the number of users.
But with custo metrics, you could actually pass
the current number of users directly from your application into stack driver to get.


LAB - Stackdriver Monitoring -> Cloud Monitoring

Abbiamo 3 VM instance nginxstack-1/2/3

- Da Stackdriver, Monitoring Overview page -> Create dashboard
scegliere nome, Add Chart 
Find resource type and metric:
	Resource type: GCE VM instance
	Metric: CPU utilization
View options: X-Ray mode

- Resources -> Metric Explorer 
Find resource type and metric:
	Resource type: GCE VM instance
	Metric: CPU utilization

- Alerting -> Create policy
Find resource type and metric:
	Resource type: GCE VM instance
	Metric: CPU utilization
Condition: is above threshold (20) for (1 minute)

Find resource type and metric:
	Resource type: GCE VM instance
	Metric: Reserved cores
Condition: is above threshold (1) for (1 minute)

Policy Triggers:
	Trigger when: ALL conditions are met

Notifications: Email

Groups -> Create Group
	name contains nginx

- Uptime Checks -> Uptime Check Overview -> Add Uptime Check
	Title
	Check type: HTTP
	Resource Type: Instance / URL ....
	Applies to: Single / Group
	Path: ...
	Check every: 1 minute


	LOGGING (previously Stackdriver Logging)
- Platform, systems and applicaion log
	. API to write to logs
	. 30-day retention
- log seach/view/filter
- log-based metrics
- monitoring alerts can be set on log events
- Data can e exported to Cloud Storage, BigQuery and Pub/Sub

Analyze logs in BigQuery and visualize in Data Studio

Installing Logging agent:

> curl -sSO https://dl.google.com/cloudagents/install-logging-agent.sh
> sudo bash install-logging-agent.sh

It's best practice to install logging agent on all of your VM instances.
The logging agent can be installed with these two simple commands.
which you could include in your startup script.
This agent is supported for Compute Engine and EC2 instances(AWS).


	ERROR REPORTING

Agregate and display errors for running cloud services:
- Error notifications
- Error dashboard
- App Engine, Apps Script, Compute Engine, Cloud Functions, Cloud Run, GKE, Amazon EC2
- Go, Java, .NET, Node.js, PHP, Python, and Ruby

	TRACING: Cloud Trace (previously Stackdriver Trace)

Tracing system
- Display data in nead real-time
- Latency reporting
- Per-URL latency sampling
Collects latency data:
- App Engine
- Google HTTPS(S) load balancers
- Applications instrumented with the Cloud Trace SDKs


	DEBUGGING: Cloud Debugger (previously Stackdriver Debugger)
- Inspect an application without stopping it or slowing it down significantly
- Debug snapshots:
	Capture call stack and local variables of a running application
- Debug logpoints:
	Inject logging into a service without stopping it
- Java, Python, Go, Node.js, Ruby, PHP, .NET Core


LAB
Sviluppiamo un'app App Engine con un bug
al fine di sfruttare Error Reporting & Debugging per individuarlo

Dalla Shell andiamo a creare un folder e a copiarci una hello world application

> mkdir appengine-hello
> cd appengine-hello
> gsutil cp gs://cloud.training/archinfra/gae-hello/* .
> dev_appserver.py $(pwd)

tale web app risponde all'endpoint https://8080-dot-.....

Adesso procediamo al deply dell'application su App Engine

> gcloud app deploy app.yaml
> gcloud app browse
la Shell risponde con la url cliccabile per raggiungere l'endpoint dell'app

Andiamo ora a modificare il file principale dell'app al fine di inserire un bug

> cat main.py
import webapp2

class MainPage(webapp2.RequestHandler):
	def get(self):
		self.response.headers['Content-Type']='text/plain'
		self.response.write('Hello, World!')

app=webapp2.MSGIApplication([
	('/',MainPAge),
	],debug=True)

		---> modifichiamo l'import in 22:

import webapp22

class MainPage(webapp2.RequestHandler):
	def get(self):
		self.response.headers['Content-Type']='text/plain'
		self.response.write('Hello, World!')

app=webapp2.MSGIApplication([
	('/',MainPAge),
	],debug=True)

> sed -i -e 'a/webapp2/webapp22' main.py (comando di modifica)
> cat main.py (per verificare che la modifica sia avvenuta correttamente)

> gcloud app deploy app.yaml --quiet
> gcloud app browse

andando a cliccare sul link fornitoci incappiamo in un errore

Andiamo in Error Reporting
vediamo l'errore
possiamo cliccare sull'errore e vedremo la stack driver
correggiamo l'errore e rifacciamo il deploy della web app

-----------------------------------------------------
ELASTIC GOOGLE CLOUD INFRASTRUCTURE: SCALING AND AUTOMATION

Cloud VPN securely connects your on-premises network to your GCP VPC network
- useful for low-volume data connections
- 99.9% SLA
- supports:
	. site-to-site VPN
	. static routes
	. dynamic routes (Cloud Router)
	. IKEv1 and IKEv2 cipher

Traffic traveling between the two networks is encrypted by one VPN gateway 
then decrypted by the other VPN gateway.
This protect your data as it travels over the public internet
and that's why Cloud VPN is useful for low volume data connections.

As a managed service, Cloud VPN provides an SLA of 99.9%

Cloud VPN doesn't support use cases where client computers need to dial in VPN using client VPN software.

Also dynamic routes are configured with Cloud Router.

	VPN topology

On-premises Network 
	On-premises VPN Gateway 
	-> On-premises External IP 
	-> Internet

GCP, VPC Network
	-> GCP Regional External IP 
	-> Cloud VPN Gateway 
	-> Routing Table 
	-> VPC Routing 
	-> Internal IP addresses -> Resources

In order to connect to your on premises network and its resources you need to configure your Cloud VPN gateway, on-premise VPN gateway and two VPN tunnels.

The Cloud VPN gateway is a regional resource that uses a regional external IP address.

Your On-premises VPN gateway can be a physical device in your data center or a software based VPN offering in another Cloud provider network,
this VPN gateway also has an external IP address.

A VPN tunnel then connects your VPN gateways and servers
as the virtual medium through which encrypted traffic is passed.

In order to create a connection between two VPN gateways
you must establish two VPN tunnels.
Each tunnel defines the connection from the perspective of its gateway and traffic can only oass when the pair of tunnels established.

One thing to remember:
when using Cloud VPN is that the maximum transmission unit or MTU for your on premises VPN gateway cannot be grater than 1460 bytes.
This is because the encryption and encapsulation of packets.


	DYnamic routing with Cloud Router

GCP
	- resources
	- Google VPN Gateway
	- Cloud Router

Peer Network
	- Peer Gateway
	- Racks

The connection between Cloud Router and Peer Gateway(on-premises) is:
BGP link-local IP addresses

Cloud VPN supports static and dynamic routes.
In order to use dynamic routes you need to configure Cloud Router.

Cloud Router can manage routes from a Cloud VPN tunnel
using border Gateway protocol or BGP(?)
This routing method allows for routes to be updated and
exchanged without changing the tunnel configuration.

In addition to Classic VPN, Google Cloud also offers a second type of Cloud VPN gateway, HA
VPN. HA VPN is a high availability Cloud VPN solution that lets you securely connect your
on-premises network to your Virtual Private Cloud (VPC) network through an IPsec VPN
connection in a single region. HA VPN provides an SLA of 99.99% service availability.
For more information on HA VPN, refer to the documentation Cloud VPN topologies:
https://cloud.google.com/network-connectivity/docs/vpn/concepts/topologies

For information on moving to HA VPN, see Moving to HA VPN:
https://cloud.google.com/network-connectivity/docs/vpn/how-to/moving-to-ha-vpn

-----------------------------------------------------
LAB - Virtual Private Networks

In this lab, you establish VPN tunnels between two networks in separate regions such that a VM in one network can ping a VM in the other network over its internal IP address.

Il ping con l'ip esterno va a buon fine:
> ping -c 3 <ext IP add>

Il ping con l'ip interno fallisce!!!

	Create the VPN gateways and tunnels
Establish private communication between the two VM instances by creating VPN gateways and tunnels between the two networks.

Reserve two static IP addresses
Reserve one static IP address for each VPN gateway.

In the Cloud Console, on the Navigation menu (Navigation menu), click VPC network > External IP addresses.

Click Reserve static address.

Specify the following, and leave the remaining settings as their defaults:

Property	Value (type value or select option as specified)
Name	vpn-1-static-ip
IP version	IPv4
Region	us-central1
Click Reserve.

Repeat the same for vpn-2-static-ip.

Click Reserve static address.

Specify the following, and leave the remaining settings as their defaults:

Property	Value (type value or select option as specified)
Name	vpn-2-static-ip
IP version	IPv4
Region	europe-west1
Click Reserve.

Note both IP addresses for the next step. They will be referred to us [VPN-1-STATIC-IP] and [VPN-2-STATIC-IP].

+Create the vpn-1 gateway and tunnel1to2

In the Cloud Console, on the Navigation menu (Navigation menu), click Hybrid Connectivity > VPN.

Click Create VPN Connection.

If asked, select Classic VPN, and then click Continue.

Specify the following in the VPN gateway section, and leave the remaining settings as their defaults:

Property	Value (type value or select option as specified)
	Name	vpn-1
	Network	vpn-network-1
	Region	us-central1
	IP address	vpn-1-static-ip
Specify the following in the Tunnels section, and leave the remaining settings as their defaults:

Property	Value (type value or select option as specified)
	Name	tunnel1to2
	Remote peer IP address	[VPN-2-STATIC-IP]
	IKE pre-shared key	gcprocks
	Routing options	Route-based
	Remote network IP ranges	10.1.3.0/24
Make sure to replace [VPN-2-STATIC-IP] with your reserved IP address for europe-west1.

Click command line.
The gcloud command line window shows the gcloud commands to create the VPN gateway and VPN tunnels and it illustrates that three forwarding rules are also created.

Click Close.
Click Create.



+Create the vpn-2 gateway and tunnel2to1

Click VPN setup wizard.

If asked, select Classic VPN, and then click Continue.

Specify the following in the VPN gateway section, and leave the remaining settings as their defaults:

Property	Value (type value or select option as specified)
	Name	vpn-2
	Network	vpn-network-2
	Region	europe-west1
	IP address	vpn-2-static-ip
Specify the following in the Tunnels section, and leave the remaining settings as their defaults:

Property	Value (type value or select option as specified)
	Name	tunnel2to1
	Remote peer IP address	[VPN-1-STATIC-IP]
	IKE pre-shared key	gcprocks
	Routing options	Route-based
	Remote network IP ranges	10.5.4.0/24
Make sure to replace [VPN-1-STATIC-IP] with your reserved IP address for us-central1.

Click Create.
Click Cloud VPN Tunnels.
Click Check my progress to verify the objective.

Create the 'vpn-2' gateway and tunnel
Wait for the VPN tunnels status to change to Established for both tunnels before continuing.


 Verify VPN connectivity

 Verify server-1 to server-2 connectivity
In the Cloud Console, on the Navigation menu, click Compute Engine > VM instances.

For server-1, click SSH to launch a terminal and connect.

To test connectivity to server-2's internal IP address, run the following command:

ping -c 3 <insert server-2's internal IP address here>
Exit the server-1 SSH terminal.

For server-2, click SSH to launch a terminal and connect.

To test connectivity to server-1's internal IP address, run the following command:

ping -c 3 <insert server-1's internal IP address here>
Remove the external IP addresses
Now that you verified VPN connectivity, you can remove the instances' external IP addresses. For demonstration purposes, just do this for the server-1 instance.

On the Navigation menu, click Compute Engine > VM instances.
Select the server-1 instance and click Stop. Wait for the instance to stop.
Instances need to be stopped before you can make changes to their network interfaces.
Click on the name of the server-1 instance to open the VM instance details page.
Click Edit.
For Network interfaces, click the Edit icon (Edit).
Change External IP to None.
Click Done.
Click Save and wait for the instance details to update.
Click Start.
Click Start again to confirm that you want to start the VM instance.
Return to the VM instances page and wait for the instance to start.
Notice that External IP is set to None for the server-1 instance.
Feel free to SSH to server-2 and verify that you can still ping the server-1 instance's internal IP address. You still be able to SSH to server-1 from the Cloud Console using Cloud IAP as described here.

External IP addresses that don’t fall under the Free Tier will incur a small cost. Also, as a general security best practice, it’s a good idea to use internal IP addresses where applicable and since you configured Cloud VPN you no longer need to communicate between instances using their external IP address.
Task 4: Review
In this lab, you configured a VPN connection between two networks with subnets in different regions. Then you verified the VPN connection by pinging VMs in different networks using their internal IP addresses.

You configured the VPN gateways and tunnels using the Cloud Console. However, this approach obfuscated the creation of forwarding rules, which you explored with the command line button in the Console. This can help in troubleshooting a configuration if needed.

--- Overview:

In questo lab abbiamo configurato una connessione VPN 
tra due network con subnet in regioni differenti.
Successivamente abbiamo verificato la connessione mediante ping con ip interno delle VM.

Nel Deployment Manager del nostro progetto
possiamo verificare le reti e sottoreti, 
le istanze di VM e le firewall rules.

Da VPC network possiamo vedere la default network
e le due custom networks vpn-network-1/2 con rispettivamente
le due subnet us-central1 e europe-west1.

In Firewall rules possiamo vedere le firewall rules create di default con gli ingress abilitati.

Da Compute Engine possiamo vedere le due istanze
e le network e subnet a cui appartengono.

Possiamo effettuare il ping reciproco delle VM,
possiamo verificare che funziona solo con IP esterno.

Andiamo in VPC network -> Externa IP addresses
riserviamo un IP esterno statico per ciascuna delle custom network:
	vpn-1-static-ip
	vpn-2-static-ip

	CREAZIONE PRIMA VPN
Andiamo in Hybrid Connectivity -> VPN -> Create a VPN
scegliamo Classic VPN

Google Compute Engine VPN gateway
name: vpn-1
network: vpn-network-1
IP address: vpn-1-static-ip

Tunnels
name: tunnel1to2
remote peer IP address: vpn-2-static-ip
routing options
	rout-based
		remote network IP ranges: 10.1.3.0/24

	CREAZIONE SECONDA VPN
Google Compute Engine VPN gateway
name: vpn-2
network: vpn-network-2
IP address: vpn-2-static-ip

Tunnels
name: tunnel2to1
remote peer IP address: vpn-1-static-ip
routing options
	rout-based
		remote network IP ranges: 10.5.4.0/24

Dopo aver creato queste due VPN 
siamo in grado di effettuare il ping dalle due VM
reciprocamente sfruttando l'indirizzo IP interno.




	CLOUD INTERCONNECT AND PEERING SERVICES

There are different cloud interconnect and peering services available to connect your infrastructure to 
Google's network.

_________________________________________________
...........	Dedicated			Shared

Layer 3 		Direct Peering		Carrier Peering

Layer 2 		Dedicated 			Partner 
				Interconnect 		Interconnect
_________________________________________________

Cloud VPN is between Direct Peering and Carrier Peering, this service ses the public internet 
but traffic is encrypted and provides access to internal IP addresses,
that's why Cloud VPN is a useful addition to direct peering and carrier peering.


---Dedicated Interconnect 
provides direct physical connections
between your on premises network (on-premises router) and Google's network (Google Router).
This enables you to transfer large amount of data
between networks which can be more cost effective than purchasing additional bandwith over public internet.

VPC <-> Cloud Router <->  Google Peering Edge(in colocation facility) <-> On-premises router

In order to use dedicated interconnect,
you need to provision a cross connect
between the Google network and your own router in a common collocation facility.
To exchange routes between the networks,
you configure a BGP session over the interconnect between the Gloud Router and the on-premises router.
This will allow user traffic from the on-premises network
to reach GCP resources on the VPC network and ....

Dedicated Interconnect can be configured to offer a 99.9% or 99.99% up time SLA.

In order to use Dedicated Interconnect,your network must physically meet Google's network in a supported collocation facility.


---Partner Interconnect 
provides connectivity through a supported service provider between your on-premises network and your
VPC network.
This is useful if your data center is in the physical location that cannot reach a dedicated interconnect collocation facility or
if your data needs don't warrant a dedicated interconnect.
In order to use Partner Interconnect,
you work with the supported service provider to connect your VPC and on-premises netwrks.
These service providers have existing physical connections to Google's network
that they make available for their customers to use.
After you establish connectivity with the service provider,
you can request a Partner interconnect connection from your service provider,
then establish a BGP session between your cloud router and on-premises router
to start passing traffic between your network.

Partner Interconnect can be configured to offer a 99.9% or 99.99% uptim SLA between Google and the service provider.



VPC subnet <-> Cloud Router <->  Google Peering Edge(in colocation facility) <-> Service provider peering edge <-> On-premises router


---Comparison of Interconnect options

Ipsec VPN tunnel connection:
	provides encrypted tunnel to VPC networks through the public internet
	capacity: 1.5(public internet)-3(direct peering link) gbps per tunnel
	requirements: on-premises VPN gateway
	access type: internal IP addresses

Dedicated Interconnect connection:
	provides dedicated, direct connection to VPC networks
	capacity: 10 gbps per link (100 gbps BETA)
	requirements: connection in colocation facility
	access type: internal IP addresses

Partner Interconnect connection:
	provides dedicated bandwidth, connection to VPC network through a service provider
	capacity: 50 mbps - 10 gbps per connecion
	requirements: service provider
	access type: internal IP addresses


-----------------------------------------------
	CLOUD PEERING SERVICES

---Direct Peering 
provides a direct connection between you business network and Google's network
- broad-reaching edge network location
- exchange BGP routes
- reach all of Google's services
- peering requirements
- no SLA

GCP Edge Points of Presence (PoPs) are where
Google's network connects to the rest of the internet reappering.
PoPs are present on over 90 Internet exchanges and at over 100 interconnection facilities aroutnd the world.

---Carrier Peering 
provides connectivity through a supported partner
- Carrier Peering partner
- reach all of Google's services 
- Partner requirements
- NO SLA 


Comparison od Peering Options:

Direct Peering connection:
	provides dedicated, direct connection to Google's network
	capacity: 10 gbps per link
	requirements: connection in GCP PoPs
	access type: public IP addresses

Carrier Peeerign connection:
	provides: peering through service provider to Google's public network
	capacity: varies based on partner offering
	requirements: service Provider
	access type: public IP addresses


Choosing a network connection option
	
	Interconnect
Direct access to RFC1918 IPa in your VPC with SLA
- Dedicated Interconnect
- Partner Interconnect
- Cloud VPN

	Peering
Access to Google public IPs only without SLA
- Direct Peering
- Carrier Peering

----------------------------------------------
	SHARING VPC NETWORKS

Let's move out attention from Hybrid connectivity
to share VPC networks.
In the simples Cloud environment a single project might have one VPC network
spanning many regions with VM instances hosting very large and complicated applications.
However, many organizations commonly deploy multiple isolated projects with multiplic VPC networks and subnets.
We are going to cover two configuarions for sharing VPC networks across GCP projects.

Shared VPC allows an arganization to connect resources from multiple projects 
to a common VPC network. This allows the resources to communicate 
with each other securely and efficiently using internal IPs from that network.

When you use Shared VPC, 
you desighate a project as a host project and attach one or more other service projects to it.


	VPC NETWORK PEERING,
in contrast, allows private RFC1918 connectivity across two VPC networks.
Regardless of whether they belong on the same project or the same organization.

Remember that each VPC network will have firewall rules that define what traffic is allowed or denied between the networks.

For example, there are two organizations that represent
a consumer and a producer respectively.
Each organization has its own organization node,
VPC network, virtual machine instances, network admin and instance admin.

In order for VPC network peering to be established successfully,
the producer network admin needs to peer the producer network with the consumer network,
and the consumer network admin needs to peer the consumer network with the producer network.
When both peering connections are created,
the VPC network peering session becomes active and routes are exchanged.
This allows the virtual machine instances to communicate privately using their internal IP addresses.

VPC network peering is a decentralized or distributed approach
to multi-project networking because each VPC network may remain under the control of separate administrator groups 
and maintains its own global firewall and routing tables.

Historically, such projects would consider external IP addresses
or VPNs to facilitate private communication between VPC networks.
However, VPC network peering does not incur the network latency, security, and cost drawbacks
that are present when using external IP addresses or VPNs.


	SHARED VPC VS. VPC PEERING
__________________________________________________
Consideration				Shared VPC 		VPC network Peerign

Across organizations 	NO 				YES

Withing project 			NO 				YES

Network administation 	Centralized 	Decentralized
__________________________________________________

The biggest difference between the two configurations is the network administration models.

Shared VPC is a centralized approach to multi-project networking,
because security and network policy occurs in the single designated VPC network.

In contrast, VPC network peering is a decentralized approach,
because each VPC network can remain under the control of separate administrator groups
and maintains its own global firewall and routing tables.

--------------------------------------------------

	LOAD BALANCING AND AUTOSCALING

_________________________________________________
|	GLOBAL 				REGIONAL
|	- HTTP(S)			- Internal TCP/UDP
|	- SSL PROXY 		- Network TCP/UDP
|	- TCP PROXY 		- Internal HTTP(S)
|_________________________________________________

GCP offers different types of load balancers that
can be divided into two categoris: Global and Regional

The global loadbalancers are the http, https, ssl proxy load balancers.
These load balancers leverage the Google front ends which are software defined distributed 
systems that sit in Google's point of presence and are dstributed globally.

Therefore, you want to use a global load balancer when your users and instances are globally distributed,
your users need access to the same application and content and you want to provide access using a single any cost IP address. 

The regional load balancers are the internal and network load balancers and
they dstribute traffic to instances that are in a single GCP region.

The internal load balancer uses Andromeda,
which is GCP software defined network virtualization stack
and the network load balancer uses Maglec,
which is a large distributed software system.

There is also another internal load balancer for http, https traffic.
The sixth load balancer is a proxy based regional layer 7 load balancer,
that enables you to run and scale your services behing a private load balancing IP address that is accessible only in the load balancers region in your VPC network.


++ Managed Instance Groups
- Deploy identical instances based on instance template
- INstance group can be resized
- Manager ensures all instances are RUNNING
- Typically used with autoscaler
- Can be single zone or regional

A managed instance group is a collection of identical VM instances that you control as a single entity using an Instance Template.

You can easily update all the instances in a group by specifying a new template in a rolling update.

Also when your applications require additional compute resources,
managed instance groups can scale autoatically to the number of instances in the group.

Managed instance groups can work with load balancing services to distribute traffic to all of the instances in the group.

If an instance in the group stops, crashes or is deleted by an action other than the instance groups commands, the managed instance group automatically recreates the instance so it can resume irs processing tasks.

The recreated instance uses the same name and the same instance template as the previous instance.

Managed instance groups can automatically identify and recreate unhealthy in a group to ensure that all instances are running optimally.

Regional managed instance groups are generally recommended
over zonal managed instance groups 
because they allow you to spread the application load across multiple zones, instead of confining your application to a single zone or having you manage multiple instance groups across different zones.

This replication protects against zonal failures
and unforeseen scenario (scenario imprevisto)
or an entire group of instances in a single zone malfunctions.
IF that happens, your application can continue serving traffic from instances running in another zone in the same region.

In order to create a manage instance group,
you first need to create an instance template.
Next, you are going to create a managed instance group of an spcified instances.
The instance group manager then automatically populates the instance group,
based on the instance template.
You can easily create instance templates using the Cloud Console.
The instance template dialog looks and works exactly like creating an instance,
except that the choices are recoded so they can repeated.
When you create an instance group,
you define the specific rules dor that instance group.

First you decide what type of managed instance group you want to create,
you can use managed instance groups 

- for stateless serving or batch workloads such as website front end or image processing from IQ 
- or for statefull applications, such as databases or legacy applications.

Second, provide a name fro the instance group.

Third, decide whether the instance group is going to be single or multi zoned and where those locations will be.
You can optionally provide port name mapping details.

Fourth, select the instance template that you want to use.

Fifth, deciding whether you want to autoscale and under what circumstances.

Finally, consider creating a health check to determine which instances are healthy and should receive traffic.

Essentially, you're creating virtual machines but you're applying more rules to that instance group.


Managed instance group offer autoscaling caabilities
- Dybanucally add/remove instances:
	. Increases in load
	. Decreases in load
- Autoscaling policy:
	. CPU utilization
	. Load balancing capacity
	. Monitoring metrics
	. Queue-based workload

A health check is very similar to an up time check in Stackdriver,
you just define a protocol, port and health criteria, based on this configuration, 
GCP computes a health state for each instance.

The health criteria defines how often to check whether an instance is healty (check interval).
How long to wait for a response (time out).
How many successful attempts are decisive,
that's the healthy threshold.
How many failed attempts are decisive,
that's the unhealthy threshold.


	HTTP(S) LOAD BALANCING
- global load balancing
- anycast IP address
- HTTP or port 80 or 8080
- HTTPs on port 443
- IPv4 or IPv6
- autoscaling
- URL maps (serve per indirizzare le richieste alle instance group a seconda della provenienza)

Architecture of an HTTP(S) load balancer

Internet -> Global Forwarding rule -> Target Proxy -> Url Map -> 
Backend Service
	-> Backend istance Group
	-> Health Check

Backend Services
- Health check
- Session affinity (optional)
- Time out setting (30-sec default)
- One or more backends
	. An instance group (managed or unmanaged)
	. A balancing mode (CPU utilization or RPS(requests-per-seconds))
	. A capacity scaler (ceiling % of CPU/Rate targets)

Http(s) load balancing (Cloud Load Balancing)
- target HTTP(S) proxy
- one signed SSL certificate installed (minimum)
- Client SSL session terminates at the load balancer
- support the QUIC transport layer protocol


	SSL CERTIFICATES
- required for HTTP(S) load balancing
- up to 10 SSL certificates (per target proxy)
- create an SSL certificate resource


	NETWORK ENDPOINT GROUPS (NEG)
A network endpoint group (NEG) is a configuration object 
that specifies a group of backend endpoints or services.

There are four types of NEGs:

- Zonal
- Internet
- Serverless (don't contain endpoints)
- Hybrid connectivity

A common use case for this configuration
is deploying services in containers.

You can also distribute traffic and grandeur version to 
application running on your backend instances.
You can use NEGs as backends for some load balancers
and with traffic director. 

Zonal and internet NEGs define how endpoints should be reached whether they are reachable and where they are located.

Unlike these neck types,
serverless NEGs don't contain endpoints.

A zonal NEG contains one or more endpoints that can be:
Compute Engine VMs or services running on the VMs.
Each endpoint is specified by either an IP address or 
an IP colin port combination.

An internet NEG contains a single endpoint that is hosted outside Google Cloud.
This endpoints is specified by host name, FQDN (fully qualified domain name) colin port or IP port.

A hybrid connectivity NEG points to traffic director services running outside of Google Cloud.

A serverless NEG points to Cloud Run, App Engine, Cloud Function services residing in the same regio as the NEG.


LAB - Configuring an HTTP Load Balancer with Autoscaling

_____________________________________________________________
Internet -> 
Cloud Load Balancing(
Anycast IP -> Global Forwarding Rule -> Target Proxy -> 
URL Map -> Backend Service
			-> Health Check
) ->
Nework: Default{
Firewall Rules -> Virtual Private Cloud
-> network:us-central1, subnet:us-central1,
	backend instance group: us-centra1-mig (Compute Engine)
-> network: europe-west1, subnet: europe-west1,
	backend instanc group: europe-west1-mig (Compute Engine)
	
}
____________________________________________________________

Creiamo delle firewall rule che accettano tutte le chiamate http in ingresso: 
name: default-allow-http
target tags: http-server
IP ranges: 0.0.0.0/0
protocols and ports: tcp: 80

Creiamo le firewall rule per l'healt check:
name: default-allow-health-check
target tas: http-server
source IP ranges: 130.211.0.0/22, 35.191.0.0/16
protocols and ports: tcp: all

Creiamo una custom image in Compute Engine
Create VM
name: webserver
Boot disk: debian
zone: us-central1-a
nelle opzioni avanzate andiamo a togliere la spunta:
Delete boot disk when instance is deleted ****
networking: 
	network tags: http-server
	network interfaces: default
Quando la VM è "up and running"
entriamo in SSH per installare al suo interno un software:

> sudo apt-get update

> sudo apt-get install -y apache2

> sudo service apache2 start

> sudo update-rc.d apache2 enable

A questo punto facciamo il reset della VM

> sudo service apache2 status

A questo punto se eliminiamo la VM senza cancellare il Boot Disk, andando in Compute Engine -> Disks 
vediamo che il disco è ancora presente.

Andiamo in Compute Engine -> Images -> +Create an image
name: mywebserver
Source: Disk
	Source disk: webserver
Location: multi-regional

Adesso possiamo creare il template in Compute Engine -> Instance templates -> Create
name: mywebserver-template
Boot Disk: mywebserver (in Custom images tab)
network tags: http-server

Una volta creato il template possiamo creare la nostra instance group in Compute Engine -> Instance Group -> Create -> New managed instance group
name: us-centra1-mig
Location: multiple zones
Region: us-central1
Autoscaling: on
autoscaling policy: HTTP load balancing usage
target HTTP load balancing usage: 80
min #: 1, max #: 10
Cool down period: 60
Create health check:
	name: http-healt-check
	protocol: TCP, port: 80
	Proxy protocol: NONE
	Health criteria: check interval: 10 seconds, timeout: 5 seconds,
	Healthy threshold: 2 consecutive successes
	Unhealthy threshold: 3 consecutive failures

Possiamo creare la stessa instance group in un'altra region,
name: europe-west1-mig

A questo punto abbiamo una VM per ogni instance group.

Configuriamo l'HTTP load balancer
Network services -> Load balancing -> Create
-> HTTP(S) Load Balancing -> From internet to my VMs
name: http-lb

Backend services -> Create 
name: http-backend
backend type: instance groups

	instance group: us-central1-mig
	port numbers: 80
	balancing mode: rate
	maximum: 50 rps
	capcity: 100%

	instance group: europe-west1-mig
	port numbers: 80
	balancing mode: utilization
	maximum CPU utilization: 80%
	capcity: 100%

health check: http-health-check(TCP)

Host and path rules:
	backends: http-backend

Frontend configuration:
	IPv4 + IPv6

Creiamo un'altra istanza per generare traffico nel load balancer e verificare l'autoscaling.

Compute Engine -> Crate instance
name: stress-test
zone: us-west1-b
Boot disk: mywebserver

Entriamo in SSH della VM stress-test:
copiamo l'IP-Port HTTP del Frontend dell'Http Load Balancer che abbiamo creato e lo memorizziamo in una variabile

> export LB_IP=34.96.86.147

> ab -n 500000 -c 1000 http://$LB_IP/
	-n numero totale di richieste
	-c numero di richieste concorrenti

Possiamo visualizzare 
in Network services -> Load balancing -> http-backend
il traffico

in Compute Engine -> VM instances
possiamo visualizzare che l'autoscaling ha creato diverse copie di VM di entrambi gli instance group

	SSL Proxy Load Balancing
	(Google Cloud Load Balancing with SSL Proxy)

- global load balancing for encrypted, non-HTTP traffic
- terminates SSL session at load balancing layer
- IPv4 or IPv6 clients
- Benefits:
	. Intelligent routing
	. Certificate management
	.  Security patching
	. SSL policies

	TCP proxy Load Balancing

- global load balancing for unencrypted, non-HTTP traffic
- terminates TCP sessions at load balancing layer
- benefits:
	. intelligent routing
	. security patching

	NETWORK LOAD BALANCING 
- regional, non-proxied load balancer
- forwarding rules (IP protocol data)
- traffic: UDP, TCP/SSL ports
- Architecture:
	. Backend service-based
	. Target pool-based

Backend service-based architecture
- regional backend service
- defines the behavior of the load balancer and how it distributes traffic to its backend instance groups
- enables new features not supported with legacy target pools
	. non-legacy health checks
	. auto-scaling with managed instance groups
	. connection draining
	. configurable failover policy

Target pool-based architecture
- forwarding rules (TCP and UDP)
- up to 50 per project
- one health check
- instances must be in the same region


Internal TCP/UDP load balancing
- regional, private load balancing
	. VM instances in same region
	. RFC 1918 IP addresses
- TCP/UDP traffic
- reduced latency, simpler configuration
- software-defined, fully distributed load balancing

Internal HTTP(S) load balancing
- regional, private load balancing
	. VM instances in same region
	. RFC 1918 IP addresses
- HTTP, HTTPS or HTTP/2 protocols
- based on open source Envoy proxy

Internal load balancing supports 3-tier web services.

LAB - Configuring an Internal Load Balancer

Let's create two "managed instance group" in us-central1 region,
along with firewall rules to allow http traffic to those instances,
and TCP traffic from GCP healt checker.
Then configure and test and internal load balancer
for those instance groups.

Abbiamo la default network e my-internal-app network, con due subnet: subnet-a, subnet-b.
Le firwall rules ammettono tutte le chiamate sia dall'interno che dall'esterno.

Creiamo una nuova Firewall rule:
name: app-allow-http
network: my-internal-app
direction of traffic: ingress
action on match: allow
target tags: lb-backend
source filter: IP ranges
source IP ranges: 0.0.0.0/0
allow tcp: 80


Creiamo una nuova Firewall rule:
name: app-allow-health-check
network: my-internal-app
direction of traffic: ingress
action on match: allow
target tags: lb-backend
source filter: IP ranges
source IP ranges: 130.211.0.0/22 35.191.0.0/16
allow tcp: all

Creiamo una instance template:
name: instance-template-1
network: my-internal-app
	subnet: subnet-a
	network tag: lb-backend
Metadata
	key: startup-script-url
	value: gs//cloud-training/gcpnet/lib/startup.sh

Creiamo una instance template:
name: instance-template-2
network: my-internal-app
	subnet: subnet-b
	network tag: lb-backend
Metadata
	key: startup-script-url
	value: gs//cloud-training/gcpnet/lib/startup.sh

Creiamo un instance group:
name: instance-group-1
zone: us-central1-a
instance template: instance-template-1
target CPU usage: 80
min # of instances: 1
max # of instances: 5
cool down period: 45 secs

Creiamo un instance group:
name: instance-group-2
zone: us-central1-b
instance template: instance-template-2
target CPU usage: 80
min # of instances: 1
max # of instances: 5
cool down period: 45 secs

Creiamo una VM:
name: utility-vm
zone: us-central1-f
network: my-internal-app
	subnet: subnet-a
primary internal IP: Ephemeral (Custom)
	custom ephemeral IP address: 10.10.20.50

Entriamo in SSH della utility-vm
facciamo il curl della VM di instance-group-1 con l'IP interno:
> curl 10.10.20.2
E facciaml il curl della VM che fa parte di instance-group-2 sempre con l'ip interno:
> curl 10.10.30.2

Configuriamo l'internal Load Balancer.
Network services -> Load balancing -> Create
scegliamo TCP Load Balancing
selezioniamo l'opzione "Only between my VMs"

Backend Configuration
name: my-ilb
backend service name: my-lib
	region: us-central1
	network: my-internal-app
Backends:
	Instance group: instance-group-1
	Instance group: instance-group-2
Health check
	name:my-lib-health-check
	port 80

Frontend Configuration
subnetwork: subnet-b
internal IP: Reserve static internal IP address
	name: my-ilb-ip
	Static IP address: Let me choose
	Custom IP adress: 10.10.30.5
Single port number: 80

Torniamo nell'SSH della utility VM
facciamo più volte il curl dell'IP statico dell'internal load balancer
e vediamo che rispondono diverse VM non sempre la stessa.


Come scegliere quale tipo di Load Balancer utilizzare

Una delle grandi differenze è il supporto 
per i client IPv6.
Only the HTTPS, SSL Proxy and TCP Proxy load balancing services support IPv6 clients.
IPv6 termination for these load balancers enables you to handle IPv6 requests
from your users and proxy them over IPv4 to your backend.



--------------------------------------------------
	INFRASTRUCTURE AUTOMATION

Cloud Deployment Manager 
is an infrastructure automation tool
- repeatable deployment process
- declarative language
- focus on the application
- parallel deployment
- template-driven
		Supported resources:
		- Compute Engine
		- Cloud Firewall Rules
		- Cloud VPN
		- Virtual Private Cloud
		- Cloud Load Balancing
		- Cloud Router

Esample:
Auto mode network with HTTP firewall rule

Automode network template:

	autonetwork.jinja
	resources:
	- name: {{ env["name"] }}
	  type: compute.v1.network
	  properties:
		 autoCreateSubnetworks: true

HTTP firewall rule template:

	firewall.jinja
	resources:
	- name: {{ env["name"] }}
	  type: compute.v1.firewall
	  properties:
	  		network: {{ properties["network"] }}
	  		sourceRanges: ["0.0.0.0/0"]
	  		allowed:
	  		- IPProtocol: {{ properties["IPProtocol"]}}
	  		  ports: {{ properties["Port"] }}

Top level configuration:

	config.yaml
	imports:
	- path: autonework.jinja
	- path: firewall.jinja

	resources:
	- name: mynetwork
	  type: autonetwork.jinja

	- name: mynetwork-allow-http
	  type: firewall.jinja
	  properties:
	    network: $(ref.mynetwork.selfLInk)
	    IPProtocol: TCP
	    Port: [80]

__________________________________________
	Infrastructure as code tool for GCP
There are other instrastructure automation tool in addition to Deployment Manager 
that you can use in GCP.
You can also use:
- Terraform
- Chef
- puppet
- Ansible
- Packer

All of these tools allow you to trat your instrastructure like software,
which helps you decrease costs, reduce risk and deploy faster
by capturing instrastructure as code.

LAB - Automating the Deployment if Infrastructure using Deployment Manager 

APIs and Services -> Library 
-> Cloud Deployment Manager V2 API -> API enabled
da Cloud Shell
> mkdir dminfra
> cd mdinfra
> gcloud deployment-manager types list | grep network
> gcloud deployment-manager types list | grep firewall
> gcloud deployment-manager types list | grep instance

launch code editor

new file: config.yaml
	
	imports:
	- path: instance-template.jinja

	resources:

	#create the auto-mode network
	- name: mynetwork
	  type: compute.v1.network
	  properties:
	    autoCreateSubnetwork: true

	#create the firewall rule
	- name: mynetwork-allow-http-ssh-rdp-icmp
	  type: compute.v1.firewall
	  properties: 
	    network: ${ref.mynetwork.selfLink}
	    sourceRanges: ["0.0.0.0/0"]
	    allowed:
	    - IPProtocol: TCP
	      ports: [22,80,3389]
	    -IPProtocol: ICMP

	#create the mynet-us-vm instance
	- name: mynet-us-vm
	  type: instance-template.jinja
	  properties: 
	    zone: us-central1-a
	    machineType: n1-standard-1
	    network: ${ref.mynetwork.selfLink}
	    subnetwork: regions/us-central1/subnetworks/mynetwork

	#create the mynet-eu-vm instance
	- name: mynet-eu-vm
	  type: instance-template.jinja
	  properties: 
	    zone: europe-west1-d
	    machineType: n1-standard-1
	    network: ${ref.mynetwork.selfLink}
	    subnetwork: regions/us-central1/subnetworks/mynetwork

save

new file: instance-template.jinja

	resources:
	- name: {{ env["name"] }}
	  type: compute.v1.instance
	  properties: 
	    machineType: zones/{{ properties["zone"] }}/machineTypes/{{ properties["machineType"] }}
	    zone: {{ properties["network"] }}
	    networkInterfaces:
	    - network: {{ properties["network"] }}
	      subnetwork: {{ properties["subnetwork"] }}
	      accessConfigs:
	      - name: External NAT
	        type: ONE_TO_ONE_NAT

	    disks:
	    - deviceName: {{ env["name"] }}
	      boot: true
	      autoDelete: true
	      initializeParams:
	        sourceImage: https://www.googleapis.com/compute/v1/projects/debian-cloud/global/images/family/debian-9

save

> gcloud deployment-manager deployments create dminfra 
	--config-config.yaml 
	--preview

> gcloud deployment-manager deployments update dminfra

Possiamo vedere i deployments andando da console in Deployment Manager

In VPC Netowork -> VPC networks 
vediamo mynetwork

In VPC Network -> Firewall rules
vediamo mynetwork-allow-http-ssh-rdp-icmp

In Compute Engine -> VM instances
vediamo le virtual machines:
mynet-eu-vm, mynet-us-vm
esse sono entrambe nella network "mynetwork"

copiamo l'ip interno della prima VM ed entriamo in SSH della seconda VM

> ping -c 3 10.132.0.2


LAB - Automating the Deployment if Infrastructure using Terraform









































