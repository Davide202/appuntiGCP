Cloud Computing has 5 fundamental attributes:

1- computer resources are on-demand  and self service
no human interventio needed to get resources

2 - broad network access
resources are accessible over a network from any location

3 - Resource pooling
Provider shares resources to customers from a large pool
allowing them to benefit from
economies of scale.
Customer don't have to know or care about the exact physical location of these resources.

4 - Rapid elasticity
Get more resources quickly as needed
Resources themselves are elastic.
Customers who need more resources can get the rapidly when they need less
that can scale back.

5 - Measured service
Pay only for what you consume
Customers pay for only what they use or reserve as they go.
If they stop using resources, they simply stopped paying.


Google Cloud offers a range od services

Compute 
	Compute  Engine (run vm in the cloud)
	Kubernetes Engine (GKE)
	App Engine
	Cloud Function

Building your own database solution
	puoi installare il tuo database in Compute Engine
	e in GKE.

Or use a managed service:
	Storage:	Cloud Bigtable
				Cloud Storage
				Cloud SQL
				Cloud Spanner
				Datastore
	Big Data:	BigQQuery
				Pub/Sub
				Dataglow
				Dataproc
				Notebooks
	Machine Learning:
				Vision API
				Vertex AI
				Speech-to-Text API
				Cloud Translation API
				Cloud Natural Language API

Resource Management
	Global, Multi-region, Region, Zone

Zonal resources operate exclusively in a single zone
	
Physical organization
	
	Global: HTTP(S) load balancer
		Regional:	Regional GKE cluster
			Zonal:	Persistendt Disk, GKE node

	Global: Virtual Private Cloud (VPC)
		Regional: Datastore
			Zonal:	Compute Engine instance, Persistent Disk

Logical organization
	
	Organization
		Folder
			Project:	GKE
			Project:	Cloud Storage Bucket, Compute Engine Instance
			Project: App Engine Service

Le policy per i permessi si ereditano gerarchicamente
dall'alto verso il basso.

How billing works
. Billing account pays for project resources
. A Billing account is linked to one or more projects
. Charged automatically or invoiced every month 
	or at threshold limit
. Subaccounts can be used for separate billing for projects

How to keep you billing under control:
- Budgets and alerts
- Billling export
- Reports

Quotas are helpful limits
	previene da un utilizzo alto 
	che può essere provocato da un errore 
	o da un attacco hacker.
	Ci sono due tipi:
Rate quota:
	GKE API: 1000 requests per 100 seconds
	reset after specific time
Allocation quota:
	5 networks per project

Many quotas are changeable


Cloud SDK:
	. gcloud
	. kubectl
	. gsutil
	. bq

LAB 

Storage -> create new bucket name: project_id

Compute Engine -> create an instance
name: first-vm
zone: us-central1-c
Boot Disk: Debian GNU/Linux 9
Service account: Compute Engine default service account
Allow default access
Firewall: Allow HTTP traffic
create

IAM and Admin -> Service accounts -> create
Service account name: test-service-account
Role: Project editor
Create Key: Key type: JSON
create -> download locally

Cloud Shell
> MY_BUCKET_NAME_1=project_id
> MY_BUCKET_NAME_2=$MY_BUCKET_NAME_1-2
> echo $MY_BUCKET_NAME_2

Upload JSON credential file
> ls -l credentials.json

Create new bucket:
> gsutil mb gs://$MY_BUCKET_NAME_2

Create VM:
> MY_REGION=us-central1
> gcloud compute zones list | grep $MY_REGION
> MY_ZONE=us-central1-a
> gcloud config set compute/zone $MY_ZONE
> MY_VMNAME=second-vm
> gcloud compute instances create $MY_VMNAME --machine-type "n1-standard-1" --image-project "debian-cloud" --image-family "debian-9" --subnet "default" 

> gcloud compute instances list

Create Service Account:
> gcloud iam service-accounts create test-service-account2 --display-name "test-service-account2"

Diamo al Service Account il Project Viewer Role:
> gcloud projects add-iam-policy-binding $GOOGLE_CLOUD_PROJECT --member serviceAccount:test-service-account2@{GOOGLE_CLOUD_PROJECT}.iam.gserviceaccount.com --role roles/viewer

Copiamo un'immagine esterna in un bucket:
> gsutil cp gs://cloud-training/ak8s/cat.jpg cat.jpg

> gsutil cp cat.jpg gs://$MY_BUCKET_NAME_1

> gsutil cp gs://$MY_BUCKET_NAME_2/cat.jpg gs://$MY_BUCKET_NAME_2/cat.jpg 

> gsutil acl set private gs://$MY_BUCKET_NAME_1/cat.jpg

> gcloud config list

> gcloud auth activate-service-account --key-file credentials.json

> gcloud config list

> gsutil cp gs://$MY_BUCKET_NAME_1/cat.jpg ./cat-copy.jpg
-> private exception

> gsutil cp gs://$MY_BUCKET_NAME_2/cat.jpg ./cat-copy.jpg

Let's change who were authenticated as back to our GCP account.
> gcloud config set account gnarlkjn_student@qwiklabs.net
> gsutil cp gs://$MY_BUCKET_NAME_1/cat.jpg ./cat-copy.jpg
-> works!

> gsutil iam ch allUsers:objectViewer gs://$MY_BUCKET_NAME_1
Adesso possiamo verificare che l'immagine nel bucket ha un link pubblico di accesso.

Nell'editor di Cloud Shell vediamo i file:
cat-copy.jpg, cat.jpg, copy2-of-cat.jpg,
credentials_(1).json, credentials.json

Cloniamo un repository git:
> git clone https://github/googlecodelabs/orchestrate-with-kubernetes.git

creaiamo un file html che contiene l'immagine del gatto dentro alla home directory 

copiamo tale file nella VM
installiamo prima un web server nella VM
quindi entriamo nella SSH della VM
> sudo apt-get update
> sudo apt-get install nginx

Torniamo in Cloud Shell e spostiamo il file html nella vm:
> gcloud compute scp index.html firs-vm:index.nginx-debian.html --zone=us-central1-c

Torniamo in SSH:
spostiamo il file dalla home directory
alla nginx's document root directory
> sudo cp index.nginx-debian.html /var/www/html

Cliccando sull'external IP della VM
che contiene il web server nginx
nel quale abbiamo messo come home page
il file html che punta all'immagine del gatto..
vediamo il gatto!


_________________________________________________________
https://app.pluralsight.com/course-player?clipId=954c35a5-273e-4c11-b6a0-d0438e4cbdb7


INTRODUCTION TO CONTAINERS AND KUBERNETES

--Hypervisors create and manage virtual machines

Dedicated server
	Application code
	Dependencies
	Kernel
	Hardware

Virtual machine
	Application code
	Dependencies
	Kernel
	Hardware + Hypervisor

--Running multiple apps on a single VM

Dedicated server
	Application code
	Dependencies
	Kernel
	Hardware
x 2

Virtual machine
	App1 App2
	Dependencies
	Kernel
	Hardware + Hypervisor


--The VM-centric way to solve this problem

Virtual machine
	Application code
	Dependencies
	Kernel
	
Virtual machine
	Application code
	Dependencies
	Kernel

Hardware + Hypervisor


--User space abstraction and containers

Virtual machine
	
	Container:
	- Application
	- User Space
	- Dependencies

	Container:
	- Application
	- User Space
	- Dependencies

	Container Runtime
	Kernel
	Harware + Hypervisor

Image: Application + Dependency

Containers use a varied set of Linux technologies:
	Processes
	Linux namespaces
	cgroups
	Union fine systems

Esempio di Dockerfile
	
	FROM ubuntu:18.04
	COPY ./app
	RUN make /app
	CMD python /app/pp.py

Cloud Build
	
	from:
	- Cloud Storage
	- git repository
	- Cloud Source Repositories

	to:
	. Cloud Functions
	. GKE
	. App Engine

You can upload the container in the Container Registry.

_________________________________________________________

https://app.pluralsight.com/course-player?clipId=95f8f40d-b7ea-484b-981e-d64b7d0aebeb

LAB - Working with Cloud Build

Controllare che 
Cloud Build API e Container Registry API siano abilitati
cercando in:
Navigation menu -> APIs & Services -> Dashboard -> Enable APIs and Services

+ Building containers with Dockerfile and Cloud Build
da Cloud Shell:


> nano quickstart.sh
#!/bin/sh
echo "Hello, world! The time is $(date)."

Save the file and close nano by pressing the CTRL+X key, then press Y and Enter.


> nano Dockerfile
FROM alpine
COPY quickstart.sh /
CMD ["/quickstart.sh"]

Save the file and close nano by pressing the CTRL+X key, then press Y and Enter.

In Cloud Shell, run the following command to make the quickstart.sh script executable.

> chmod +x quickstart.sh

In Cloud Shell, run the following command to build the Docker container image in Cloud Build.

> gcloud builds submit --tag gcr.io/${GOOGLE_CLOUD_PROJECT}/quickstart-image .

Don't miss the dot (".") at the end of the command. The dot specifies that the source code is in the current working directory at build time.


In the Google Cloud Console, on the Navigation menu (Navigation menu), click Container Registry > Images.

The quickstart-image Docker image appears in the list


---
+ Building Containers with a build configuration file and Cloud Build

Cloud Build also supports custom build configuration files. In this task you will incorporate an existing Docker container using a custom YAML-formatted build file with Cloud Build.

In Cloud Shell enter the following command to clone the repository to the lab Cloud Shell.

> git clone https://github.com/GoogleCloudPlatform/training-data-analyst

Create a soft link as a shortcut to the working directory.

> ln -s ~/training-data-analyst/courses/ak8s/v1.1 ~/ak8s

Change to the directory that contains the sample files for this lab.

> cd ~/ak8s/Cloud_Build/a

A sample custom cloud build configuration file called cloudbuild.yaml has been provided for you in this directory as well as copies of the Dockerfile and the quickstart.sh script you created in the first task.

In Cloud Shell, execute the following command to view the contents of cloudbuild.yaml.

> cat cloudbuild.yaml

You will see the following:

steps:
- name: 'gcr.io/cloud-builders/docker'
  args: [ 'build', '-t', 'gcr.io/$PROJECT_ID/quickstart-image', '.' ]
images:
- 'gcr.io/$PROJECT_ID/quickstart-image'


This file instructs Cloud Build to use Docker to build an image using the Dockerfile specification in the current local directory, tag it with 

gcr.io/$PROJECT_ID/quickstart-image 

($PROJECT_ID is a substitution variable automatically populated by Cloud Build with the project ID of the associated project) and then push that image to Container Registry.

In Cloud Shell, execute the following command to start a Cloud Build using cloudbuild.yaml as the build configuration file:

> gcloud builds submit --config cloudbuild.yaml .

The build output to Cloud Shell should be the same as before. When the build completes, a new version of the same image is pushed to Container Registry.

In the Google Cloud Console, on the Navigation menu (Navigation menu), click Container Registry > Images and then click quickstart-image.

Two versions of quickstart-image are now in the list.

In the Google Cloud Console, on the Navigation menu (Navigation menu), click Cloud Build > History.

Two builds appear in the list.

Click the build ID for the build at the top of the list.

The details of the build, including the build log, are displayed.

+ Building and Testing Containers with a build configuration file and Cloud Build

The true power of custom build configuration files is their ability to perform other actions, in parallel or in sequence, in addition to simply building containers: running tests on your newly built containers, pushing them to various destinations, and even deploying them to Kubernetes Engine. In this lab, we will see a simple example: a build configuration file that tests the container it built and reports the result to its calling environment.

In Cloud Shell, change to the directory that contains the sample files for this lab.

> cd ~/ak8s/Cloud_Build/b

As before, the quickstart.sh script and the a sample custom cloud build configuration file called cloudbuild.yaml has been provided for you in this directory. These have been slightly modified to demonstrate Cloud Build's ability to test the containers it has build. There is also a Dockerfile present, which is identical to the one used for the previous task.

In Cloud Shell, execute the following command to view the contents of cloudbuild.yaml.

> cat cloudbuild.yaml
You will see the following:

steps:
- name: 'gcr.io/cloud-builders/docker'
  args: [ 'build', '-t', 'gcr.io/$PROJECT_ID/quickstart-image', '.' ]
- name: 'gcr.io/$PROJECT_ID/quickstart-image'
  args: ['fail']
images:
- 'gcr.io/$PROJECT_ID/quickstart-image

In addition to its previous actions, this build configuration file runs the quickstart-image it has created. In this task, the quickstart.sh script has been modified so that it simulates a test failure when an argument ['fail'] is passed to it.

In Cloud Shell, execute the following command to start a Cloud Build using cloudbuild.yaml as the build configuration file:

> gcloud builds submit --config cloudbuild.yaml .

You will see output from the command that ends with text like this:

Output (do not copy)

Finished Step #1
ERROR
ERROR: build step 1 "gcr.io/ivil-charmer-227922klabs-gcp-49ab2930eea05/quickstart-image" failed: exit status 127
----------------------------------------------------------------------------------------------------------------------------------------------------------------
ERROR: (gcloud.builds.submit) build f3e94c28-fba4-4012-a419-48e90fca7491 completed with status "FAILURE"

Confirm that your command shell knows that the build failed:

> echo $?

The command will reply with a non-zero value. If you had embedded this build in a script, your script would be able to act up on the build's failure.

End your lab
____________________________________________________________
https://app.pluralsight.com/course-player?clipId=4bc4508c-fafa-4e39-89a2-36a22c0d3e58

	CONTAINER MANAGEMENT

Kubernetes is a container centric management environment.
Google originated id and then donated it to the open source community.
Now it's a project of the vendor neutral Cloud Native Computing Foundation.

It automates the deployment, scaling, load balancing,
logging, monitoring, and other management features of
containerized applications.
These are the features that are characteristic of a typical
Platform as a Service (PaaS) solutions.

Kubernetes also facilitates the fewatures of an infrastucture as a service,
souch as allowing a wide range of user preferences and configuration flexibility.

Kubernetes supports declarative configurations.
When you administer your infrastructure declaratively,
you describe the desired state you want to achieve,
instead of issuing a series of commands to achieve the desired state.

Kubernete's job is to make the deployed system conform to
your desired state and the keep it there in spite of failures.
Declarative configuration saves you work,
because the system is desired state is always documented.
It also reduces the risk of error.

Communities also allows imperative configuration in which
you issue commands to change the system state,
but administering kubernetes that scale imeratively
will be a big missed opportunity.

One of he primary strengths of kubernetes is its ability
to automatically keep a system in a stat that you declare.
Experience Kubernetes administators use imperative configuration
only for quick temporary fixes and as a tool in building a declarative configuration.

	KUBERNETES FEATURES
1- Supports both stateful and stateless applications,
	batch jobs and deamon tasks.
2- Autoscaling
	Service Discovery
	Load Balancer
	Self Healing 
	Zero Downtime Deployment
3- Resource limits
4- Extensibility
5- Portability

Managed Kubernetes service
auto-repair, auto-upgrade 
pod and cluster autoscaling
Cloud Logging, Cloud Monitoring 



Introducing to Google Kubernetes Engine

Kubernets is powerful, but managing the infrastructure is a full-time job.

Google Kubernets Engine is a managed service for Kubernetes within Google Cloud.

GKE will help you deploy, manage and scale kubernetes environments
for your containerized application on GCP.
More specifically, GKE is a component of the GCP compute offering,
it makes i easy to bring your kubernetes workloads into the cloud.

GKE is fully managed,
which means that you don't have to provision the underlying resources.
GKE uses a container optimize operating system.
These operating systems are maintained by Google,
are optimized to scale quickly and with a minimal resource footprint.

When you use GKE,
you start by directing the service to instantiate a kubernetes system for you.
This system is called a cluster.
GKE is auto upgrade feature is enabled to ensure that your clusters are automatically upgraded with 
the latest and greatest version of kubernetes.

The virtual machines that host your containers inside of the GKE cluster are called NODES.
If you enable GKE auto repair feature,
the service will automatically repair unhealthy nodes for you.
It will make periodic health checks on each node in the cluster.
If a node is determined to be unhealthy and requires repair,
GKE will drain the node.
In other words, it will cause its workload to gracefully exit
and then recreate that node.

Just as kubernetes supprts scaling workloads,
GKE supports scaling the cluster itself.
GKE seamlessly integrates with Google Cloud Build
and Container Registry.
This allows you to automate deployment using private container images 
that you've securely stored in contaner registry.

GKE also integrates with Google's Identity and Access Management (IAM),
which allows you to control access through the use of the accounts and role permissions.

Stackdriver is Google Cloud System for monitoring and management for services, containers,  applications and infrastructure.
GKE integrates with Stackdriver monitoring to help you understand your application's performance.
GKE is integradet with Google Virtual Private Cloud (VPC)
and makes use of GCO networking features.

The GCP Console provides insights into GKE clusters
and resources and allows you to view inspect and delete resources in those clusters.

You might be aware that open source kubernetes contains a dashboard, 
it takes a lot of work to set it up securely
but the GCP Console is a Dashboard for your GKE clusters and workloads
that you don't have to manage
and it's more powerful that the kubernetes dashboard.


	Comparing Google Cloud computing solutions

Compute Engine: IaaS
	fully customizable virtual machines
	persistent disks and optiona local SSDs
	global load balancing and autoscaling
	per-second billing
	use cases:
		1- complete control over the OS and virtual hardware
		2- well suited for lift-and-shift migrations to the cloud
		3- most flexible compute solution, often used when a managed solution is too restrictive
	--
	Long-lived VMs
	Physical servers
	One container per VM

GKE: Hybrid
	fully managed kubernete platform
	supports cluster scaling, persistent disks, automated upgrades and auto node repairs.
	built-in integration with Google Cloud services
	portability across multiple environments:
	- hibrid computing
	- multi-cloud computing
	use cases:
	1- containerized applications
	2- Cloud-native distributed systems
	3- Hybrid applications
	--
	rich administration of container workloads
	on-premises kubernetes

App Engine: PaaS
	provides a fully managed, code-first platform.
	stramlines application deployment and scalability
	provides support for popular programming languages and application runtimes
	you can also run container workloads
	supports integrated monitoring, logging,  diagnostics.(Stackdriver)
	simplifies version control, canary testing and rollbacks.
	use cases:
	1- website
	2- mobile app and gaming backends
	3- RESTful APIs
	--
	containers run by the service
	no-ops

Cloud Run: Stateless
	enables stateless containers
	abstracts away infrastructure management
	automatically scales up and down
	Open API and runtime environment
	use cases:
	1- deploy stateless containers that listen for requests or events
	2- build applications in any language using any frameworks and tools 
	--
	staeless container
	managed


Cloud Functions: Server logic
	event-driven, serverless compute service
	automatic scaling with highly available and fault-tolerant desing
	charges apply only when your code runs
	triggered based on events in Google Cloud services, HTTP endpoints and Firebase
	use cases:
	1- supporting microservice architecture
	2- serverless application backends
		mobile and IoT backends
		integrate with third-party services and APIs
	3- intelligent applications
		virtual assistant and chat bots
		video and image analysis
	--
	no-ops

_____________________________________________________________
https://app.pluralsight.com/course-player?clipId=68e46a89-dbd5-4776-877b-09bbf07d3e7d


	Kubernetes Concepts:

Kubernetes object model
Each thing Kubernetes managers is represented
by an object and you can view and change objects attributes and state.

Principle of declarative management
Kubernetes axpects you to tell it what you want the state 
of the objects under its management to be and it will work
to bring that state into being and keep it there.

A Kubernetes object is defined as a persistent entity that
represent the state of something running in a cluster:
its desired state and its current state.

Various kinds of objects represent containerized applications,
the resources that are available to them and the policies that
affect their behavior.

Kubernetes objects have two important elements:
1- Objec spec (object specification)
	desired state described by us
2- Object status
	current state described by Kubernetes

Containers in a Pod share resources
In a Pod there are:
	Shared networking
	Containers
	Shared storage



Kubernetes assigns each Pod a unique IP addess
Every container within a Pod shares the network name space
including IP address and net reports.
Containers within the same Pod can communicate through
localhost: 127.0.0.1
Pod can also specify a set of storage volumes to be shared 
amongst his containers.

Example
Running three nginx containers in a single Pod
	You want three nginx containers running all the time
	You declare objects that represent those containers
	Kubernetes launghes those objects and maintains them

Desired state compared to current state
	Desired state (Kubernetes objects)
	Current state
	Kubernetes Control Plane, Remediation actions



---
https://app.pluralsight.com/course-player?clipId=540f47f9-d579-4bd4-88d8-8c937a29f4b9

Kubernetes Control Plane
---

++Cooperationg processes make a Kubernetes cluster work
________________________
Cluster
	Control plane (VM)
		kube-APIserver <- kubectl
			<- etcd (cluster database, its job is reliably store the state of the cluster, cluster configuration data and more dynamic information such as what nodes are part of the cluster, what pod should be running and where they should be running)
			<- kube-scheduler (is responsible for scheduling pods onto nodes, to do taht it evaluates the requirements of each individual pod and selecting which node is most suitable...)
			<- kube-controller-manager (it continuously monitors the state of the cluster through kube api server...)
			<- kube-cloud-manager
	Node (VM)
		Kubelet -> kube-APIserver
		Kube-proxy
________________________

The single componet that you interact with derectly
is called the Kube-APIserver.
This componet's job is to accept commands that view or chage the
state of the cluster including launghing pods.
In the specialization,
you will use the "kubectl" command frequently.
This command's job is to connect to the kube app server
and communicate with using the kubernetes API.
kube-APIserver also authenticates incoming requests,
determines whether they are authorized and valid 
and managers admission control.
But it's not just kubectl that talks with kube-APIserver,
in fact any query or change to at the cluster state must be addressed
to the kube-APIserver.

etcd is the clusters database.
Its job is to reliably store the state of the cluster.
This includes all of the cluster configuration data and more 
dynamic information such as 
what nodes are part of the cluster,
what Pods should be running,
where they should be running.

You'll never interact directly with etcd.
Instead kube-APIserver interacts with the database
on behalf of the rest of the system.

Kube-scheduler is responsible for scheduling Pods onto nodes.
To do that, it evaluates the requirements of each individual Pod
and selectiong which node is most suitable
but it doesn't do the work of actually lauching tho Pods
on the nodes.
Instead, whenever it discover a Pod object that doesn't yet have
an assignment to a node,
it chooses a node and simply writes the name of that node into the pod object.

Another component of the system is reponsible then
for launching the departs and you'll see it very soon.
But how does kube-scheduler decide where to run a Pod,
it knows the stat of all node and it will obey contraints
that you define on where a pod may run based on hardware, software and policy.

For example, you might specify a certain Pod is only allowed run on nodes with a certain amount of memory.
You can also define affinity specifications,
which calls groups of Pods to prefer running on the same node.
Or anti affinity specifications, 
which ensure that Pods do not run on the same node..

kube-controller-manager has a broader job,
it continuously monitors the state of the cluter through
kube-APIserver.
Whenever the current state of the cluster doesn't match the
desired state, kube-controller-manager will attempt to make
changes to achieve the desired state.
It's called the controller-manager because many kubernetes objects
are managed by loops of code called controllers.
These loops of code handled the process of remediation.
Controllers will be very useful for you.
To be specific,
you'll learn to use cerain kinds of kubernetes controllers to manage workloads.

For example,
remember our problem of keeping three nginx Pods always running,
we can gather them together into a controller object
called a deployment that not only keeps the running,
but also lets us scale them and bing the together under the frontend.

Other kinds of controllers have system level responsibilities,
for example, node controller's job is to monitor and respond
when a node is offline.
kube-cloud-manager manages controllers that interact with
the underlying cloud providers.
For example,
if you manually launch a kubernetes cluster 
on Google Compute Engine, kube-cloud-manager would be 
responsible for bringing in Google Cloud features like
Load Balancers and storage volumes
when you need them.

Each node runs a small family of control plane components to.
For example, each node runs a kubelet.
You can thing of a kubelet as kubernetes agent on each node,
when the kube-APIserver wants to start a Pod on a node,
it connects to that nodes kubelet.

kubelet uses the container runtime to start the pod
and monitors its lifecycle, 
including readiness and liveliness probes
and reports back to kube-APIserver.

The container runtime is the software that knows
how to launch a container from a container image.
The world of kubernetes offers several choices for containers
run times, but the Linux distribution that GKE uses for its node
launches containers using containerd (https://containerd.io/),
the runtime component of Docker.

Kube-proxy's job is to maintain the network connectivity
among the Pods in the cluster.
In open source kubernetes it does so
using the firewall capabilities of IP tables which are built
into the Linux kernel.

---
https://app.pluralsight.com/course-player?clipId=aeba77cb-8d8d-47c3-93c6-7af9adb1e00a

Google Kubernetes Engine Concepts
---

GKE manages all of the control playing components for us.
It still exposes an IP address to which we send all of our kubernetes API requests,
but GKE takes the responsibility for provisioning and managing all of the control plane infrastructure behind it.
It also abstracs away having a separate control plane.
The responsibilities of the control plane are absorbed by Google Cloud and you were not separately billed for your control plane.

In any Kubernetes environments nodes are created externally by cluster administrators, not by kubernetes itself.
GKE automates this process for you,
it launches Compute Engine Virtual Machine instances and registers them as nodes.
You can manage no settings directly from the Cloud Console.
You pay per hour of life of your nodes,
not counting the control plane. 
Because nodes running Compute Engine,
you can choose your node machine type when you create your cluster.
By default, the node machine type is n1-standard-1,
which provide one virtual CPU and 3.75 gb of memory.
Google Cloud offers a wide variety of Compute Engine options.

You can customize the virtual machine.

Use node pools to mnage different kinds of nodes.

You can also select multiple node machine types
by creating multiple node pools.
A node pool is a subset of nodes within a cluster that share a configuration,
such as the amount of memory, their CPU generation.
Node pools also provide an easy way to ensure that 
workloads run on the right hardware within your cluster,
you just label them with a desired node pool.
Node pools are GKE feature rather than a kubernetes feature.
You can build an analogist mechanism
within open source kubernetes
ut you'd have to maintain it yourself,
you can enable automatic node upgrades,
automatica node repairs and cluster autoscaling
at this node pool level.
Here's a word of caution.

Some of each node CPU and memory are needed to run 
the GKE and kubernetes component that let it work as part of your cluster.
For example,
if you allocate nodes with 15GB of memory, 
not quite all of that 15GB will be available for use by pods.

Zonal versus regional clusters
__________________________________________
	ZONAL CLUSTER
Zone
	Cluster
		Control plane
		Nodes
__________________________________________
	REGIONAL CLUSTER
Region
	Cluster
		Zone
			Control plane
			Nodes
		Zone
			Contrl plane
			Nodes
		Zone
			Contrl plane
			Nodes
___________________________________________			


By default a cluster launches on a single Google Cloud 
compute zone with three identical nodes, all in one node pool.
The number of nodes can be changed during or after
the creation of the cluster.
Adding more nodes and deploying multiple replicas of an application will improve an application availability
but only up to a point.

What happens if an entire compute zone goes down?
You can address this concern by using a GKE regional cluster.
Regional clusters have a single API endpoint for the cluster.
However, its control planes and nodes are spread across multiple Compute Engine zones within a region.

Regional cluster ensure that the availability of the application
is maintained across multiple zones in a single region.
In addigion, the availability of the control plane
is also maintained, so the both applications
and manaement functionality can withstand the loss of one
or more but not all zones.

By default, a regional cluster is spread across three zones,
each containing one control plane and three nodes.
These numbers can be increased or decreased.

For example, if you have five nodes in zone one,
you will exactly the same number of nodes 
for each of the other zones.
For a total of 15 nodes.

Once you build a zonal cluster,
you cannot convert it into a regiona cluster or vice versa.

A regional or zonal GKE cluster can also be set up as a private cluster.
The entire cluster that is the control plane
and its nodes are hidden from the public internet.
Cluster control plane can be accessed by Google Cloud products such as Cloud Logging or Cloud Monitoring
through an internal IP address.
They also can be accessed by authorized networks,
through external IP address.
Authorized networks are basically IP address reanges
that are trusted to access to control plane.
In addition, nodes can have a limited outbound access through private Google access,
which allows them to communicate with other Google Cloud services.

For example, nodes can pull container images from Google container registry without needing external IP addresses.

---
https://app.pluralsight.com/course-player?clipId=aeba77cb-8d8d-47c3-93c6-7af9adb1e00a

	Kubernetes Object Management
---

All kubernetes objects are identified by a unique name
and a unique identifier.

Example: running three nginx containers.

Il primo modo che abbiamo è definire i tre pod:

Objects are defined in a YAML file.

--
apiVersion: apps/v1
kind: Pod
metadata:
	name: nginx
	labels:
		app: nginx
spec:
	containers:
	- name: nginx
	  image: nginx:latest
--
This YAML file defines a desired state for a pod,
its name and a specific container image for it to run. 

Your manifest files have certain required fields.

"apiVersion" describes which kubernetes API version is used
to create the object,
the kubernetes protocols version so as to help maintain
backwards compatibility.

"kind" defines the object you want,
in this case a pod.

"metadata" helps identify the object using name, unique id and on optional name space.
You can define several related objects in the same YAML file
and it is a best practive to do so.

One file is often easier to manage than several.

Another even more important tip,
you should save your YAML file in version controlled repositories.
This practice makes it easier to track and manage changes 
and to back out those changes when necessary.
It's also a big help when you need to recreate or restore a cluster.

Many GCP custemers use Cloud Source repositories for this purpose, because that service lets them control the permissions of those files in the same way as their other GCP resources.

When you create a kubernetes object,
you name it with a string,
names must be unique.
Only one object of a particular kind can have a particlulare name at the same time in a kubernetes name space.

However, if an object is deleted, its name can be reused.

Each object generated throughout the life of a cluster
has unique id generated by kubernetes.
This means that no two objects will have tha same "uid"
throughout the life of a cluster.

"labels" are key-vale pairs with which you tag your objects during or after their creating.
Labes help you identify and organize objects and subsets of objects.

--
apiVersion: apps/v1
kind: Deployment
metadata:
  name: nginx
  labels:
    app: nginx
    env: dev
    stack: frontend
spec:
  replicas: 3
  selector:
    matchLabels
    app: ninx
--

Labels can be matched by label selectors

Admin issued a command
> kubectl get pods -selector=app=nginx

One way to bring three nginx web server into being,
would be to declare three pod objecs,
each with its own section of YAML.

kubernetes default scheduling alghorithm prefers to spread
the workload evenly across the nodes available to it
so we'd get a situation like this one looks good.

Pods have a life cycle:
born -> running -> broken -> dies

++Pods and Controller Objects

Controller object types:
- Deployment
- StatefulSet
- DaemonSet
- Job

Deployments are great choice for long lived software components like web servers, especilly when we want to manage them as a group.

In our example, when cube scheduler schedules pods for a deployment,
it notifies the kube-APIserver.
These changes are constantly monitored by controllers,
especially by the deployment controller.
The deployment controller will monitor and maintain three nginx pods.

If one of those pods fails,
the deployment controller will recognize the difference between the current state and the desired state
and we'll try to fix it, by launching a new pod.

Instead if using multiple YAML manifest or files for each pod, you use a single deployment yaml to launch three replicas of the same container.

A deployment insurers that a defined set of pods is running at any given time.
Within its objects back, you specify how many replica pods you want,
how pods should run
which containers should run within these pods
and which volumes should be mounted.

Based on these templates, controllers maintain the pods
desired state within a cluster.

--
apiVersion: apps/v1
kind: Deployment
metadata:
  name: nginx-deployment
  labels:
    app: nginx
spec:
  replicas: 3
  template:
    metadata:
      labels:
        app: nginx
    spec:
      containers:
      - name: nginx
        image: nginx:latest
--

Deployments can also do a lot more than this,
which you will see later.





Allocating resource quotas

Multiple projects run on a single cluster
how can I allocate resource quotas??

Kubernetes allows you to abstract a single physical cluster into multiple clusters
known as "name spaces".
Name spaces provide scope for naming resources such as pods, deployments and controllers.

Il name space di default è "Default".
Your workload resources will use this name space by default (Pods, Deployments).

Di default esistono anche altri due name space:

"Kube-system" name space for objects created by the kubernets system itself (ConfigMap, Secrets, Controllers, Deployments).

When you use a "kubectl" command, by default items in the "kube-system" name space are excluded,
but you can choose to view its contents explicitly.

"kube-public" name space for object that are publicly readable to all users.
kube-public is a tool for disseminating information to 
everything running in a cluster,
you're not required to use it, bu it can come in handy
especiallywhen everything running in a cluster is 
relaed to the same goal and needs information in common.

Best practice tip: namespace-neutral YAML
You can appl a resource to a name space when creating it
using a command line name space flag or 
you can specify a name space in the yaml file for the resource.
Whenever possible, apply name spaces at the command line level!!!
This practice makes your yaml files ore flexible.

For example

kubectl -n demo apply -f mypod.yaml

apiVersion: v1
kind: Pod
metadata:
  name: mypod
  namespaces: demo

-----------

A note about Deployments and ReplicaSets
The example nginx Deployment you saw in the previous video was simplified. In practice, you
would launch a Deployment object to manage your desired three nginx Pods, just as the video
described. Note, though, that the Deployment object would create a ReplicaSet object to
manage the Pods. The diagram in the video leaves out this detail.
You will work with Deployment objects directly much more often than ReplicaSet objects. But
it's still helpful to know about ReplicaSets, so that you can better understand how Deployments
work. For example, one capability of a Deployment is to allow a rolling upgrade of the Pods it
manages. To perform the upgrade, the Deployment object will create a second ReplicaSet
object, and then increase the number of (upgraded) Pods in the second ReplicaSet while it
decreases the number in the first ReplicaSet.


A note about Services
Services provide load-balanced access to specified Pods. There are three primary types of
Services:
● ClusterIP: Exposes the service on an IP address that is only accessible from within this
cluster. This is the default type.
● NodePort: Exposes the service on the IP address of each node in the cluster, at a
specific port number.
● LoadBalancer: Exposes the service externally, using a load balancing service provided by
a cloud provider.
In Google Kubernetes Engine, LoadBalancers give you access to a regional Network Load
Balancing configuration by default. To get access to a global HTTP(S) Load Balancing
configuration, you can use an Ingress object.
You will learn more about Services and Ingress objects in a later module in this learning path.


Controller objects to know about
This reading explains the relationship among several Kubernetes controller objects:
● ReplicaSets
● Deployments
● Replication Controllers
● StatefulSets
● DaemonSets
● Jobs
A ReplicaSet controller ensures that a population of Pods, all identical to one another, are
running at the same time. Deployments let you do declarative updates to ReplicaSets and Pods.
In fact, Deployments manage their own ReplicaSets to achieve the declarative goals you
prescribe, so you will most commonly work with Deployment objects.
Deployments let you create, update, roll back, and scale Pods, using ReplicaSets as needed to
do so. For example, when you perform a rolling upgrade of a Deployment, the Deployment object
creates a second ReplicaSet, and then increases the number of Pods in the new ReplicaSet as it
decreases the number of Pods in its original ReplicaSet.
Replication Controllers perform a similar role to the combination of ReplicaSets and
Deployments, but their use is no longer recommended. Because Deployments provide a helpful
"front end" to ReplicaSets, this training course chiefly focuses on Deployments.
If you need to deploy applications that maintain local state, StatefulSet is a better option. A
StatefulSet is similar to a Deployment in that the Pods use the same container spec. The Pods
created through Deployment are not given persistent identities, however; by contrast, Pods
created using StatefulSet have unique persistent identities with stable network identity and
persistent disk storage.
If you need to run certain Pods on all the nodes within the cluster or on a selection of nodes, use
DaemonSet. DaemonSet ensures that a specific Pod is always running on all or some subset of
the nodes. If new nodes are added, DaemonSet will automatically set up Pods in those nodes
with the required specification. The word "daemon" is a computer science term meaning a
non-interactive process that provides useful services to other processes. A Kubernetes cluster
might use a DaemonSet to ensure that a logging agent like fluentd is running on all nodes in the
cluster.
The Job controller creates one or more Pods required to run a task. When the task is completed,
Job will then terminate all those Pods. A related controller is CronJob, which runs Pods on a
time-based schedule.
Later modules in this learning path will cover these controllers in more depth.

---
https://app.pluralsight.com/course-player?clipId=312c5d08-bc39-46f5-b0ad-979caf40bfb6


LAB - Deploying Google Kubernetes Engine
---

 on the Navigation menu, click Kubernetes Engine > Clusters.

 Click Create to begin creating a GKE cluster. In the GKE Standard cluster option select configure in the next screen.

 Change the cluster name to standard-cluster-1 and zone to us-central1-a. Leave all the values at their defaults and click Create.

 Modify GKE clusters
It is easy to modify many of the parameters of existing clusters using either the Google Cloud Console or Cloud Shell. In this task, you use the Google Cloud Console to modify the size of GKE clusters.

In the Google Cloud Console, click NODES at the top of the details page for standard-cluster-1.
In Node Pools section and click default-pool.
In the Google Cloud Console, click RESIZE at the top of the Node Pool Details page.
Change the number of nodes from 3 to 4 and click RESIZE.

In the Google Cloud Console, on the Navigation menu, click Kubernetes Engine > Clusters.
When the operation completes, the Kubernetes Engine > Clusters page should show that standard-cluster-1 now has four nodes.

Deploy a sample workload
In this task, using the Google Cloud console you will deploy a Pod running the nginx web server as a sample workload.

In the Google Cloud Console, on the Navigation menu( Navigation menu), click Kubernetes Engine > Workloads.
Click Deploy to show the Create a deployment wizard.

Click Continue to accept the default container image, nginx:latest, which deploys 3 Pods each with a single container running the latest version of nginx.

Scroll to the bottom of the window and click the Deploy button leaving the Configuration details at the defaults.
When the deployment completes your screen will refresh to show the details of your new nginx deployment.


_____________________________________________
---
apiVersion: "apps/v1"
kind: "Deployment"
metadata:
  name: "nginx-1"
  namespace: "default"
  labels:
    app: "nginx-1"
spec:
  replicas: 3
  selector:
    matchLabels:
      app: "nginx-1"
  template:
    metadata:
      labels:
        app: "nginx-1"
    spec:
      containers:
      - name: "nginx-1"
        image: "nginx:latest"
---
apiVersion: "autoscaling/v2beta1"
kind: "HorizontalPodAutoscaler"
metadata:
  name: "nginx-1-hpa-ofkc"
  namespace: "default"
  labels:
    app: "nginx-1"
spec:
  scaleTargetRef:
    kind: "Deployment"
    name: "nginx-1"
    apiVersion: "apps/v1"
  minReplicas: 1
  maxReplicas: 5
  metrics:
  - type: "Resource"
    resource:
      name: "cpu"
      targetAverageUtilization: 80
_________________________________________________

apiVersion: apps/v1
kind: Deployment
metadata:
  annotations:
    deployment.kubernetes.io/revision: "1"
  creationTimestamp: "2021-12-03T09:57:11Z"
  generation: 1
  labels:
    app: nginx-1
  managedFields:
  - apiVersion: apps/v1
    fieldsType: FieldsV1
    fieldsV1:
      f:metadata:
        f:labels:
          .: {}
          f:app: {}
      f:spec:
        f:progressDeadlineSeconds: {}
        f:replicas: {}
        f:revisionHistoryLimit: {}
        f:selector: {}
        f:strategy:
          f:rollingUpdate:
            .: {}
            f:maxSurge: {}
            f:maxUnavailable: {}
          f:type: {}
        f:template:
          f:metadata:
            f:labels:
              .: {}
              f:app: {}
          f:spec:
            f:containers:
              k:{"name":"nginx-1"}:
                .: {}
                f:image: {}
                f:imagePullPolicy: {}
                f:name: {}
                f:resources: {}
                f:terminationMessagePath: {}
                f:terminationMessagePolicy: {}
            f:dnsPolicy: {}
            f:restartPolicy: {}
            f:schedulerName: {}
            f:securityContext: {}
            f:terminationGracePeriodSeconds: {}
    manager: GoogleCloudConsole
    operation: Update
    time: "2021-12-03T09:57:11Z"
  - apiVersion: apps/v1
    fieldsType: FieldsV1
    fieldsV1:
      f:metadata:
        f:annotations:
          .: {}
          f:deployment.kubernetes.io/revision: {}
      f:status:
        f:availableReplicas: {}
        f:conditions:
          .: {}
          k:{"type":"Available"}:
            .: {}
            f:lastTransitionTime: {}
            f:lastUpdateTime: {}
            f:message: {}
            f:reason: {}
            f:status: {}
            f:type: {}
          k:{"type":"Progressing"}:
            .: {}
            f:lastTransitionTime: {}
            f:lastUpdateTime: {}
            f:message: {}
            f:reason: {}
            f:status: {}
            f:type: {}
        f:observedGeneration: {}
        f:readyReplicas: {}
        f:replicas: {}
        f:updatedReplicas: {}
    manager: kube-controller-manager
    operation: Update
    time: "2021-12-03T09:57:18Z"
  name: nginx-1
  namespace: default
  resourceVersion: "3934"
  uid: 648f7fda-f12a-4151-ab4b-5830d4852dfe
spec:
  progressDeadlineSeconds: 600
  replicas: 3
  revisionHistoryLimit: 10
  selector:
    matchLabels:
      app: nginx-1
  strategy:
    rollingUpdate:
      maxSurge: 25%
      maxUnavailable: 25%
    type: RollingUpdate
  template:
    metadata:
      creationTimestamp: null
      labels:
        app: nginx-1
    spec:
      containers:
      - image: nginx:latest
        imagePullPolicy: Always
        name: nginx-1
        resources: {}
        terminationMessagePath: /dev/termination-log
        terminationMessagePolicy: File
      dnsPolicy: ClusterFirst
      restartPolicy: Always
      schedulerName: default-scheduler
      securityContext: {}
      terminationGracePeriodSeconds: 30
status:
  availableReplicas: 3
  conditions:
  - lastTransitionTime: "2021-12-03T09:57:18Z"
    lastUpdateTime: "2021-12-03T09:57:18Z"
    message: Deployment has minimum availability.
    reason: MinimumReplicasAvailable
    status: "True"
    type: Available
  - lastTransitionTime: "2021-12-03T09:57:11Z"
    lastUpdateTime: "2021-12-03T09:57:18Z"
    message: ReplicaSet "nginx-1-754ddbcd6c" has successfully progressed.
    reason: NewReplicaSetAvailable
    status: "True"
    type: Progressing
  observedGeneration: 1
  readyReplicas: 3
  replicas: 3
  updatedReplicas: 3

______________________________________________________
apiVersion: v1
kind: Pod
metadata:
  creationTimestamp: "2021-12-03T09:57:11Z"
  generateName: nginx-1-754ddbcd6c-
  labels:
    app: nginx-1
    pod-template-hash: 754ddbcd6c
  managedFields:
  - apiVersion: v1
    fieldsType: FieldsV1
    fieldsV1:
      f:metadata:
        f:generateName: {}
        f:labels:
          .: {}
          f:app: {}
          f:pod-template-hash: {}
        f:ownerReferences:
          .: {}
          k:{"uid":"85665bd0-452d-419b-9f3d-69ea2ebeebd7"}:
            .: {}
            f:apiVersion: {}
            f:blockOwnerDeletion: {}
            f:controller: {}
            f:kind: {}
            f:name: {}
            f:uid: {}
      f:spec:
        f:containers:
          k:{"name":"nginx-1"}:
            .: {}
            f:image: {}
            f:imagePullPolicy: {}
            f:name: {}
            f:resources: {}
            f:terminationMessagePath: {}
            f:terminationMessagePolicy: {}
        f:dnsPolicy: {}
        f:enableServiceLinks: {}
        f:restartPolicy: {}
        f:schedulerName: {}
        f:securityContext: {}
        f:terminationGracePeriodSeconds: {}
    manager: kube-controller-manager
    operation: Update
    time: "2021-12-03T09:57:11Z"
  - apiVersion: v1
    fieldsType: FieldsV1
    fieldsV1:
      f:status:
        f:conditions:
          k:{"type":"ContainersReady"}:
            .: {}
            f:lastProbeTime: {}
            f:lastTransitionTime: {}
            f:status: {}
            f:type: {}
          k:{"type":"Initialized"}:
            .: {}
            f:lastProbeTime: {}
            f:lastTransitionTime: {}
            f:status: {}
            f:type: {}
          k:{"type":"Ready"}:
            .: {}
            f:lastProbeTime: {}
            f:lastTransitionTime: {}
            f:status: {}
            f:type: {}
        f:containerStatuses: {}
        f:hostIP: {}
        f:phase: {}
        f:podIP: {}
        f:podIPs:
          .: {}
          k:{"ip":"10.8.3.3"}:
            .: {}
            f:ip: {}
        f:startTime: {}
    manager: kubelet
    operation: Update
    time: "2021-12-03T09:57:18Z"
  name: nginx-1-754ddbcd6c-zl8g9
  namespace: default
  ownerReferences:
  - apiVersion: apps/v1
    blockOwnerDeletion: true
    controller: true
    kind: ReplicaSet
    name: nginx-1-754ddbcd6c
    uid: 85665bd0-452d-419b-9f3d-69ea2ebeebd7
  resourceVersion: "3932"
  uid: 3c5140ef-eb92-4652-a40e-43537e713c48
spec:
  containers:
  - image: nginx:latest
    imagePullPolicy: Always
    name: nginx-1
    resources: {}
    terminationMessagePath: /dev/termination-log
    terminationMessagePolicy: File
    volumeMounts:
    - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
      name: kube-api-access-glnzt
      readOnly: true
  dnsPolicy: ClusterFirst
  enableServiceLinks: true
  nodeName: gke-standard-cluster-1-default-pool-0c0e9925-dg52
  preemptionPolicy: PreemptLowerPriority
  priority: 0
  restartPolicy: Always
  schedulerName: default-scheduler
  securityContext: {}
  serviceAccount: default
  serviceAccountName: default
  terminationGracePeriodSeconds: 30
  tolerations:
  - effect: NoExecute
    key: node.kubernetes.io/not-ready
    operator: Exists
    tolerationSeconds: 300
  - effect: NoExecute
    key: node.kubernetes.io/unreachable
    operator: Exists
    tolerationSeconds: 300
  volumes:
  - name: kube-api-access-glnzt
    projected:
      defaultMode: 420
      sources:
      - serviceAccountToken:
          expirationSeconds: 3607
          path: token
      - configMap:
          items:
          - key: ca.crt
            path: ca.crt
          name: kube-root-ca.crt
      - downwardAPI:
          items:
          - fieldRef:
              apiVersion: v1
              fieldPath: metadata.namespace
            path: namespace
status:
  conditions:
  - lastProbeTime: null
    lastTransitionTime: "2021-12-03T09:57:11Z"
    status: "True"
    type: Initialized
  - lastProbeTime: null
    lastTransitionTime: "2021-12-03T09:57:18Z"
    status: "True"
    type: Ready
  - lastProbeTime: null
    lastTransitionTime: "2021-12-03T09:57:18Z"
    status: "True"
    type: ContainersReady
  - lastProbeTime: null
    lastTransitionTime: "2021-12-03T09:57:11Z"
    status: "True"
    type: PodScheduled
  containerStatuses:
  - containerID: containerd://18d10412673093c15e425517362849a76b4ef6351da197569504eb5e7c367e32
    image: docker.io/library/nginx:latest
    imageID: docker.io/library/nginx@sha256:9522864dd661dcadfd9958f9e0de192a1fdda2c162a35668ab6ac42b465f0603
    lastState: {}
    name: nginx-1
    ready: true
    restartCount: 0
    started: true
    state:
      running:
        startedAt: "2021-12-03T09:57:17Z"
  hostIP: 10.128.0.5
  phase: Running
  podIP: 10.8.3.3
  podIPs:
  - ip: 10.8.3.3
  qosClass: BestEffort
  startTime: "2021-12-03T09:57:11Z"
-_____________________________________________________


https://app.pluralsight.com/course-player?clipId=02003bc2-e5b2-45de-aaf0-e64e0fa6df87

Migrate for Anthos
---


Migrate for Anthos moves VMs to containers,
existing applications into a kubernetes environments.
The best thing about this is the process is automated.

Your workloads can be on premises or another cloud providers.

Moves workload compute to container immediately (<10min).

Data can be migrated all at once or "streamed" to the cloud intil the app is live in the cloud.
___________________________________________________
Migrate for Compute Engine
	Migrate Manager (Compute Engine)
	Edge Nodes (Compute Engine)
	Cache (Cloud Storage)

on-premises/cloud
	Local Compute
	Local Compute
	Local Compute

Migrate for Anthos
	Processing Cluster (GKE)
		CRD
		SC
		CSI
		....
____________________________________________________

First, let's inspect the archiecture required for migration.
The first step is to allow Migrate for Compute Engine
to create a pipeline for streaming or migrating the data from on premises or another cloud provider into Google Cloud.

Migrate for Compute Engine is a tool that allows you to 
bring you existing applications into VMs on Google Cloud.

Migrate for Anthos is then installed on GKE processing cluster
and is composed of many Kubernetes resources.
Migrate for Anthos is used to generate deployment artifacts.
Some of these artifacts, like your kubernetes configurations
and the Docker files are used to create the VM wrapping container.
This container goes into Cloud Storage,
the container images themselves are stored in the Container Registry.

After the deployment assets are created,
they can be used to deploy your application into a target cluster.
You simply apply the generator configuration and
it creates all the necessary Kubernetes elements on the target cluster.

A Migration is a multi-step processor

1- First configure processing cluster,
you need to create processing cluster.

2- After that you install the Migrate for Anthos components
onto that cluster.

Next you need add a migration source.
You can migrate from VMware, AWS, Azure or Google Cloud.

3- Generate and review plan.
You will need to create a migration object
with the details of the migration that you're performing.
This will generate a plan template for you in a yaml file.
You may need to alter this configuration file to create the level of customization that you desire.


4- Generate artifacts.
When the plan is ready, you will need to generate 
the artifacts for the migration.
This means generating the container images of your applications
and the yaml file is required for the deployment.

5- Test.
After your migration artifacts have been generated,
they need to be tested.
Both the container images and the deployments will be tested at this stage.

6- Deploy.
Finally, if the tests are successful,
you can use the generated artifacts to deploy your application 
into your production clusters.





Example:

Setup the processing cluster:

> gcloud container --project $PROJECT_ID 
	cluster create $CLUSTER_NAME
	--zone $CLUSTER_ZONE
	--username "amdin"
	--cluster-version 1.14
	--machine-type "n1-standard-4"
	--image-type "UBUNTU"
	--num-nodes 1
	--enable-stackdriver-kubernetes
	--scopes "cloud-platform"
	--enable-ip-alias
	--tags="http-server"

Before you run the command,
you need to make sure that you are a GKE admin to setup the cluster.

You also must have firewall rules in place to allow communications between Migrate for Anthos 
and Migrate for Compute Engine.

After all that's done,
you can create the pocessing cluter.

The example enables a VPC native cluster.

When the processing cluster is up and running,
you need to install Migrate for Anthos
using the command:

> migctl sutup install

This command installs all of the required Kubernetes resources onto the processing cluster for the migration.

Adding a source enables migrations from a specific environment

> migclt source create ce my-ce-src 
	--project my-project 
	--zone zone

The migctl source create command specifies the location
of the application to migrate.

The example is from migrating from Google Compute Engine.

If you're migrating from a VMware backend or another Cloud Provider,
you need to install some additiona packeages.

Now that the infrastructure elements are setup,
the next step is to create a migration plan.

> migctl migration create test-migration
	--source my-ce-src
	--cm-id my-id
	--intent Image

The migctl migration create command will create the migration plan.
This command will define the migration resources to be crerated on the cluster.
You identify the source VM and what data to exclude from the migration.
You can also specify what migration intense you want.

You can specify the following intense:
image, image data, data or PV-based container.
The output of this command is a yaml file that can be further customized.

After creating a migration plan,
you will need to generate the artifacts for the migration.

> migctl migration generate-artifacts my-migration

The migctl migration generate-artifacts command 
will start this process.

This process will first copy files and directories
representing the VM to a container image registry as images.

Migrate for Anthos creates two images,
a reliable image for deployment ino anothe cluster and
a non reliable image layer that can be used to update the container image in the future.

Next, Migrate for Anthos will generate configuration yaml files that you use to deploy the VM to another GKE cluster.

These are copied into a Cloud Storage Bucket
as an intermediate location.

> migctl migration get-artifacts test-migration

You run the migctl migration get-artifacts command
to download the yaml configuration files generated from the last step.

The yaml configuration defines the resources to deploy, such as you are creating a deployment or a stateful set.

Is the deployment of headless service?
Are you using persistent volumes or persistent volume claims?

You can edit the yaml file to customize your deployment.
Examples of customization include enabling load balancing, allowing ingress or defining this size.

> kubectl apply -f deployment_spec.yaml

Finally, you run the kubectl apply command to deploy
the defined specification.

Summary

Kubernetes controllers keep the cluster state matching the desired state.

Kubernetes consists of a family of control plane components,
running on the control plane and the nodes.

GKE abstracts away the control plane.

Declare the state you want using manifest files.


gcloud config set project my-kubernetes-project-304910
gcloud container clusters get-credentials my-cluster --zone us-central1-c --project my-kubernetes-project-304910
	
	
	
-------------------------------------------------

	DEPLOYMENT 
kubectl create deployment hello-world-rest-api 
	--image=in28min/hello-world-rest-api:0.0.1.RELEASE
kubectl get deployment
	
	LOAD BALANCER (SERVICE)
kubectl expose deployment hello-world-rest-api 	
		--type=LoadBalancer 
		--port=8080

kubectl get services
kubectl get services --watch
curl 35.184.204.214:8080/hello-world

	MANUAL-SCALE
kubectl scale deployment hello-world-rest-api 
	--replicas=3
kubectl get deployment 
watch curl http://35.184.204.214:8080/hello-world
kubectl get pods


gcloud container clusters resize my-cluster 
	--node-pool default-pool 
	--num-nodes=2 
	--zone=us-central1-c
	
	AUTO-SCALE
kubectl autoscale deployment hello-world-rest-api 
		--max=4 
		--cpu-percent=70

	HORIZONTAL AUTOSCALING 
kubectl get hpa
	
	AUTOSCALE FOR CLUSTER 
gcloud container clusters update cluster-name	
	--enable-autoscaling
	--min-nodes=1 
	--max-nodes=10
	
	
	CONFIGMAP 
kubectl create configmap hello-world-config 
			--from-literal=RDS_DB_NAME=todos
			
kubectl get configmap 

kubectl describe configmap hello-world-config


	SECRET 
kubectl create secret generic hello-world-secrets-1 
	--from-literal=RDS_PASSWORD=dummytodos

	
kubectl get secret
kubectl describe secret hello-world-secrets-1


kubectl apply -f deployment.yaml
gcloud container node-pools list 
		--zone=us-central1-c 
		--cluster=my-cluster
kubectl get pods -o wide
 
kubectl set image deployment hello-world-rest-api hello-world-rest-api=in28min/hello-world-rest-api:0.0.2.RELEASE
kubectl get services
kubectl get replicasets
kubectl get pods
kubectl delete pod hello-world-rest-api-58dc9d7fcc-8pv7r
 
kubectl scale deployment hello-world-rest-api --replicas=1
kubectl get replicasets
gcloud projects list
 
kubectl delete service hello-world-rest-api
kubectl delete deployment hello-world-rest-api
gcloud container clusters delete my-cluster --zone us-central1-c


---
apiVersion: apps/v1
kind: Deployment
metadata:
  labels:
    app: app-name
  name: app-name
  namespace: default
spec:
  replicas: 3
  selector:
    matchLabels:
	  app: hello-world-rest-api
  template:
    metadata:
	  labels:
	    app: hello-world-rest-api
	spec:
	  containers:
	  - image: hello-world-rest-api:0.0.3.RELEASE
	    name: hello-world-rest-api
---
apiVersion: v1
kind: Service
metadata:
  labels:
    app: hello-world-rest-api
  name: hello-world-rest-api
  namespace: default
spec:
  ports:
  - port: 8080
    protocol: TCP
	targetPort: 8080
  select:
    app: hello-world-rest-api
  sessionAffinity: None
  type: LoadBalancer
---

Deploy a new microservice which needs nodes
with a GPU attached

Attach a new node pool with GPU instances to your cluster:
> gcloud container node-pools create POOL_NAME 
	--cluster CLUSTER_NAME
	
> gcloud container node-pools list
	--cluster CLUSTER_NAME
	--zone=us-central1-c

Deploy the new microservice to the new pool by setting up 
nodeSelector in the deployment.yaml

- nodeSelector:cloud.google.com/gke-nodepool:POOL_NAME
	
Delete the Microservice
- Delete service - kubectl delete service
- delete deployment - kubectl delete deployment 

Delete cluster
- gcloud container clusters delete 

Google Kubernetes Engine (GKE) Cluster

- Cluster: Group of Compute Engine instances:
	- Master Node(s) - manages the cluster
	- Worker Node(s) - run your workloads (pods)

- Master Node (Control plane) components:
 - API Server - Handles all communication for K8S cluster (from nodes and outside)
	- Scheduler - Decides placement of pods
	- Control Manager - Manages deployments and replicasets
	state

- Worker Node components:
	- Runs your pods
	- Kubelet - Manages communication with master node(s) 
 
Cluster Types:

Zonal Cluster:
- Single Zone - Single Control plane. Nodes running in the same zone.
- Multi-zonal - Single Control plane but nodes running in multiple zones 

Regional Cluster:	
Replicas of the control plane runs in multiple zones 
of a given region. 
Nodes also run in the same zones where control plane runs.

Private Cluster:
VPC-native cluster. Nodes only have internal IP addresses

Alpha Cluster:
Clusters with alpha APIs-early feature API's. Used to test new K8S features.


Kubernetes Pods

- smallest deployable unit in Kubernetes 
- a Pod contains one ore more containers
- each Pod is assigned an ephemeral IP address 
- all container in a pod share:
	- Network
	- Storage
	- IP Address
	- Ports and
	- Volumes (Shared persisten disks) 
- Pod statuses:
	Running/Pending/Succeeded/Failed/Unknown 
		
	
> kubectl get pods

> kubectl get deployment -o wide 



Deployment VS ReplicaSet 

A Deployment is created for each microservice:
> kubectl create deployment m1 --image=m1:v1 
Deployment represents a microservice with all its releases(versions).
Deployment manages new releases ensuring zero downtime.

> kubects set image deployment hello-world-rest-api 
	hello-world-rest-api=hello-world-rest-api:0.0.2.RELEASE
	
Replica Set ensures that a specific number of pods are running for a specific microservice version 

> kubectl get replicasets
> kubectl get pods

> kubectl scale deployment m2 --replicas=2

Even if one of the pods is killed,
Replica Set will launch a new one.

Deploy V2 of microservice - Create a new Replica Set 
> kubectl set image deployment m1 m1=m1:v2 
V2 Replica Set is created
Deployment updates V1 Replica Set and V2 Replica Set based on the release strategies

Kubernets - Service
- each Pod has its own IP address:
How do you ensure that external users are not impacted when:
- A Pod fails and is replaced by Replica Set 
- A new release happens and all existing pods of old release are replaced by ones of new release.

- Create Service 
> kubectl expose deployment name --type= LoadBalancer --port=80
- Expose Pods to outside world using a stable IP Address 
- Ensures that the external world does not get impacted as pods go down and come up

- Three Types:
	
	1- ClusterIP: exposes Service on a cluster-internal IP
	use case: you want your microservice only to be available inside the cluster (Intra cluster communication)
	
	2- LoadBalancer: exposes Service externally using a Cloud Provider's Load Balancer.
	use case: you want to create individual Load Balancer's for each microservice.
	
	3- NodePort: Exposes Service on each Node's IP at a static port (NodePort) 
	use case: you do not want to create an external Load Balancer for each microservice, you can create one Ingress component to load balance multiple microservice.
	

Container Registry - Image Repository
fully-managed container registry provided by GCP 
Alternative Docker Hub
Can be integrated to CI/CD tools to publish images to registry. 
You can secure your container images.
Analyze for vulnerabilities and inforce deployment policies.
Naming: HostName/ProkectID/Image:Tag-gcr.io/projectname/helloworld:1

Remember:
Replicate master nodes across multiple zones for high availability.
Some CPU on the nodes is reserved by Control Plane:
1st core - 6%
2nd core - 1%
3rd/4th core - 0.5%
Rest - 0.25%

Creating Docker Image for your microservices(Dockerfile)
	Build Image: docker build -t image:version
	Test id Locally: docker run -d -p 8080:8080 image:version
	Push it to Container Repository: docker push image:version
	
Kubernetes supports Stateful deployments like Kafka, Redis, ZooKeeper
StatefulSet - Set of Pods with unique persistent identities and stable hostname

How do we run services on nodes for log collection or monitoring?
DaemonSet - One Pod on every node for backfround ervices

Enabled by default - Integrates with Cloud Monitoring and Cloud Logging.
Cloud Logging System and Application Logs 
can be exported to BigQuery or Pub/Sub.

You want to keep your costs low and optimize your GKE implementation:
consider Preemptible VMs, appropriate region,
committed-use discount.
E2 machine types are cheaper than N1.
choose right environment to fit your workload type 
(use multiple node pools if needed).

Auto scaling GKE solution:
configure horizontal Pod autoscaler for deployments
and Cluster Autoscaler for node pools.

You wanto to execute untrusted third-party code in Kubernetes Cluster:
create a new node pool with GKE Sandbox.
Deploy untrusted code to Sandbox node pool.

You want enable ONLY internal communication between your microservice deployments in a Kubernetes Cluster:
Create Service of type ClusterIP.

My Pod stays pending:
Most probably Pod cannot be scheduled onto a node 
(insufficient resources).

My Pod stays waiting:
Most probably failure to pull the image.

Command Line

Create Cluster:
> gcloud container clusters create my-cluster
	--zone us-central1-a
	--node-locations us-central1-a,us-central1-b
	
Resize Cluster:
> gcloud container clusters resize my-cluster
	--node-pool my-node-pool
	--num-nodes 10
	
Autoscale Cluster:
> gcloud container clusters update cluster-name 
	--enable-autoscaling 
	--min-nodes=1
	--max-nodes=10 

Delete Cluster 
> gcloud container clusters delete my-cluster 

Adding Node Pool:
> gcloud container node-pools create new-node-pool-name
	--cluster my-cluster 
	
List Images in container registry:
> gcloud container images list 

Description:
> kubectl get pods/services/replicasets

Create Deployment:
> kubectl apply -f deployment.yaml 
> kubectl create deployment 

Create service:
> kubectl expose deployment hello-world-rest-api
	--type=LoadBalancer
	--port=8080
	
Scale Deployment:
> kubectl scale deployment hello-world 
	--replicas 5
	
Autoscale Deployment:
> kubectl autoscale deployment 
	--max 10
	--min 1
	--cpu-percent 80
	
Delete Deployment:
> kubectl delete deployment hello-world 

Update Deployment:
> kubectl apply -f deployment.yaml 

Rollback Deployment 
> kubectl rollout undo deployment hello-world
	--to-revision=1
	
Delete everything:
> kubectl delete service hello-world-rest-api
> kubectl delete deployment hello-world-rest-api

> gcloud projects list 
> gcloud config set project VALUE 

> gcloud container clusters delete my-cluster 
	--zone us-central1-c 

Connect to a Kubernetes Cluster:
> gcloud container clusters get-credentials 




-----------------------------------------------------------------------------------------------------------------------------------------------------
https://app.pluralsight.com/library/courses/architecting-google-kubernetes-engine-workloads-4/table-of-contents

	Architecting with Google Kubernetes Engine:
		Workloads

Kubernetes Operations

Kubectl must be configured first:

Relies on a config file: $HOME/.kube/config 

Config file contains:
- Targett cluster name 
- Credentials for the cluster 

Current config: kubectl config view 

Connecting to a Google Kubernetes Engine cluster:

gcloud container clusters get-credentials CLUSTER_NAME 
	--zone ZONE_NAME 
	
This command writes the configuration information into a config file in the .kube directory in the home directory by default.


	Kubects command Syntax
	
kubectl [command] [TYPE] [NAME] [flags]

command: get/describe/logs/exec/...
TYPE: pods/deployments/nodes/...
	kubectl get pods 

NAME: (name of the object)
	kubectl get pod my-test-app 

flags: 
	kubectl get pod my-test-app -o=yaml 
	kubectl get pods -o=wide (wide format)
	
the kubectl command has many uses:
- create kubernetes objects 
- view objects 
- delete objects 
- view and export conigurations 

	
	Introspection 

get/describe for information about pod and container 

exec for executing command and applications 

logs to see what's happening inside pods


Pod phases: Pending, Running, Succeded, Failed, Unknown, CrashLoopBackOff 



Intercation: exec 

kubectl exec POD_NAME -- COMMAND 

kubectl exec demo -- env 
kubectl exec demo -- ps aux 
kubectl exec demo -- cat /proc/1/mounts 

kubectl exec demo -- ls /
	bin 
	boot
	dev 
	etc
	...
	
	WORKING INSIDE A POD 
kubectl exec -it POD_NAME -- COMMAND 

kubectl exec -it demo -- /bin/bash 
root@demo:/# ls 
bin boot dev etc ... 

kubectl logs POD_NAME 

The logs contain both the standard output and standard error messages that the application within the container have generated.

LAB - CREATE CLUSTER 

export my_zone=us-central1-a
export my_cluster=standard-cluster-1

gcloud container clusters create $my_cluster 
	--num-nodes 3 
	--zone $my_zone 
	--enable-ip-alias

gcloud container clusters resize $my_cluster 
	--zone $my_zone 
	--num-nodes=4

To create a kubeconfig file with the credentials of the current user (to allow authentication) and provide the endpoint details for a specific cluster (to allow communicating with that cluster through the kubectl command-line tool), execute the following command:

gcloud container clusters get-credentials $my_cluster --zone $my_zone

nano ~/.kube/config

kubectl config view

kubectl cluster-info

kubectl config current-context

kubectl config get-contexts

kubectl config use-context gke_${GOOGLE_CLOUD_PROJECT}_us-central1-a_standard-cluster-1

kubectl top nodes

 source <(kubectl completion bash)

In Cloud Shell, type kubectl followed by a space and press the Tab key twice.

In Cloud Shell, type kubectl co and press the Tab key twice.
The shell outputs all commands starting with "co"

kubectl create deployment --image nginx nginx-1

kubectl get pods

export my_nginx_pod=[your_pod_name]

kubectl describe pod $my_nginx_pod


nano ~/test.html
	<html> 
	<header>
	<title>This is title</title>
	</header>
	<body> Hello world </body>
	</html>
	
	
kubectl cp ~/test.html $my_nginx_pod:/usr/share/nginx/html/test.html

kubectl expose pod $my_nginx_pod --port 80 --type LoadBalancer

kubectl get services

curl http://[EXTERNAL_IP]/test.html





git clone https://github.com/GoogleCloudPlatform/training-data-analyst

ln -s ~/training-data-analyst/courses/ak8s/v1.1 ~/ak8s

cd ~/ak8s/GKE_Shell/

new-nginx-pod.yaml:

apiVersion: v1
kind: Pod
metadata:
  name: new-nginx
  labels:
    name: new-nginx
spec:
  containers:
  - name: new-nginx
    image: nginx
    ports:
    - containerPort: 80
	
kubectl apply -f ./new-nginx-pod.yaml

kubectl exec -it new-nginx /bin/bash
	apt-get update
	apt-get install nano
	
	cd /usr/share/nginx/html
	nano test.html

	<html> <header><title>This is title</title></header>
	<body> Hello world </body>
	</html>

	exit

kubectl port-forward new-nginx 10081:80

kubectl logs new-nginx -f --timestamps

END OF THE LAB


	DEPLOMENTS 

Deployment describe the desired state of pods.

	Roll out updates to the Pods.
A new ReplicaSet is created that matches the altered version of the deployment.

	Roll back Pods to previous revision.

	Scale or autoscale Pods.
	
	Well-suited for stateless applications.
	
The desired state is described in a deployment YAML file containing the characteristics of the pods coupled with head operationally run these pods and their life cycle events.

After you submit this file to the kubernetes control plane, it creates a Deplyment Controller which is responsible for converting the desired state into reality and keeping that desired state over time.

Controller is a loop process created by Kubernetes that takes care the routine tasks to ensure the desired state of an object or set objects running on the cluster matches the desired state.

During this process, a ReplicaSet is created, a ReplicaSet is a controler that ensures that a certain number of Pod replicas are running at any give time. 

The Deployment is a high level controller for a Pod that declares a state.
The Deployment configures  a replicated controller to instantiate and maintain a specific version of the pods specified in the deployment.

Example of Deployment:

apiVersion: apps/v1
kind: Deployment 
metadata: 
  name: my-app 
spec:
  replicas: 3
  template:
    metadata:
	  labels:
	    app: my-app 
	spec:
	  containers: 
	  - name: my-app 
	    image: gr.io/demo/my-app:1.0 
		ports: 
		- containerPort: 8080 




















