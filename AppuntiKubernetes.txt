Cloud Computing has 5 fundamental attributes:

1- computer resources are on-demand  and self service
no human interventio needed to get resources

2 - broad network access
resources are accessible over a network from any location

3 - Resource pooling
Provider shares resources to customers from a large pool
allowing them to benefit from
economies of scale.
Customer don't have to know or care about the exact physical location of these resources.

4 - Rapid elasticity
Get more resources quickly as needed
Resources themselves are elastic.
Customers who need more resources can get the rapidly when they need less
that can scale back.

5 - Measured service
Pay only for what you consume
Customers pay for only what they use or reserve as they go.
If they stop using resources, they simply stopped paying.


Google Cloud offers a range od services

Compute 
	Compute  Engine (run vm in the cloud)
	Kubernetes Engine (GKE)
	App Engine
	Cloud Function

Building your own database solution
	puoi installare il tuo database in Compute Engine
	e in GKE.

Or use a managed service:
	Storage:	Cloud Bigtable
				Cloud Storage
				Cloud SQL
				Cloud Spanner
				Datastore
	Big Data:	BigQQuery
				Pub/Sub
				Dataglow
				Dataproc
				Notebooks
	Machine Learning:
				Vision API
				Vertex AI
				Speech-to-Text API
				Cloud Translation API
				Cloud Natural Language API

Resource Management
	Global, Multi-region, Region, Zone

Zonal resources operate exclusively in a single zone
	
Physical organization
	
	Global: HTTP(S) load balancer
		Regional:	Regional GKE cluster
			Zonal:	Persistendt Disk, GKE node

	Global: Virtual Private Cloud (VPC)
		Regional: Datastore
			Zonal:	Compute Engine instance, Persistent Disk

Logical organization
	
	Organization
		Folder
			Project:	GKE
			Project:	Cloud Storage Bucket, Compute Engine Instance
			Project: App Engine Service

Le policy per i permessi si ereditano gerarchicamente
dall'alto verso il basso.

How billing works
. Billing account pays for project resources
. A Billing account is linked to one or more projects
. Charged automatically or invoiced every month 
	or at threshold limit
. Subaccounts can be used for separate billing for projects

How to keep you billing under control:
- Budgets and alerts
- Billling export
- Reports

Quotas are helpful limits
	previene da un utilizzo alto 
	che può essere provocato da un errore 
	o da un attacco hacker.
	Ci sono due tipi:
Rate quota:
	GKE API: 1000 requests per 100 seconds
	reset after specific time
Allocation quota:
	5 networks per project

Many quotas are changeable


Cloud SDK:
	. gcloud
	. kubectl
	. gsutil
	. bq

LAB 

Storage -> create new bucket name: project_id

Compute Engine -> create an instance
name: first-vm
zone: us-central1-c
Boot Disk: Debian GNU/Linux 9
Service account: Compute Engine default service account
Allow default access
Firewall: Allow HTTP traffic
create

IAM and Admin -> Service accounts -> create
Service account name: test-service-account
Role: Project editor
Create Key: Key type: JSON
create -> download locally

Cloud Shell
> MY_BUCKET_NAME_1=project_id
> MY_BUCKET_NAME_2=$MY_BUCKET_NAME_1-2
> echo $MY_BUCKET_NAME_2

Upload JSON credential file
> ls -l credentials.json

Create new bucket:
> gsutil mb gs://$MY_BUCKET_NAME_2

Create VM:
> MY_REGION=us-central1
> gcloud compute zones list | grep $MY_REGION
> MY_ZONE=us-central1-a
> gcloud config set compute/zone $MY_ZONE
> MY_VMNAME=second-vm
> gcloud compute instances create $MY_VMNAME --machine-type "n1-standard-1" --image-project "debian-cloud" --image-family "debian-9" --subnet "default" 

> gcloud compute instances list

Create Service Account:
> gcloud iam service-accounts create test-service-account2 --display-name "test-service-account2"

Diamo al Service Account il Project Viewer Role:
> gcloud projects add-iam-policy-binding $GOOGLE_CLOUD_PROJECT --member serviceAccount:test-service-account2@{GOOGLE_CLOUD_PROJECT}.iam.gserviceaccount.com --role roles/viewer

Copiamo un'immagine esterna in un bucket:
> gsutil cp gs://cloud-training/ak8s/cat.jpg cat.jpg

> gsutil cp cat.jpg gs://$MY_BUCKET_NAME_1

> gsutil cp gs://$MY_BUCKET_NAME_2/cat.jpg gs://$MY_BUCKET_NAME_2/cat.jpg 

> gsutil acl set private gs://$MY_BUCKET_NAME_1/cat.jpg

> gcloud config list

> gcloud auth activate-service-account --key-file credentials.json

> gcloud config list

> gsutil cp gs://$MY_BUCKET_NAME_1/cat.jpg ./cat-copy.jpg
-> private exception

> gsutil cp gs://$MY_BUCKET_NAME_2/cat.jpg ./cat-copy.jpg

Let's change who were authenticated as back to our GCP account.
> gcloud config set account gnarlkjn_student@qwiklabs.net
> gsutil cp gs://$MY_BUCKET_NAME_1/cat.jpg ./cat-copy.jpg
-> works!

> gsutil iam ch allUsers:objectViewer gs://$MY_BUCKET_NAME_1
Adesso possiamo verificare che l'immagine nel bucket ha un link pubblico di accesso.

Nell'editor di Cloud Shell vediamo i file:
cat-copy.jpg, cat.jpg, copy2-of-cat.jpg,
credentials_(1).json, credentials.json

Cloniamo un repository git:
> git clone https://github/googlecodelabs/orchestrate-with-kubernetes.git

creaiamo un file html che contiene l'immagine del gatto dentro alla home directory 

copiamo tale file nella VM
installiamo prima un web server nella VM
quindi entriamo nella SSH della VM
> sudo apt-get update
> sudo apt-get install nginx

Torniamo in Cloud Shell e spostiamo il file html nella vm:
> gcloud compute scp index.html firs-vm:index.nginx-debian.html --zone=us-central1-c

Torniamo in SSH:
spostiamo il file dalla home directory
alla nginx's document root directory
> sudo cp index.nginx-debian.html /var/www/html

Cliccando sull'external IP della VM
che contiene il web server nginx
nel quale abbiamo messo come home page
il file html che punta all'immagine del gatto..
vediamo il gatto!


_________________________________________________________
INTRODUCTION TO CONTAINERS AND KUBERNETES

--Hypervisors create and manage virtual machines

Dedicated server
	Application code
	Dependencies
	Kernel
	Hardware

Virtual machine
	Application code
	Dependencies
	Kernel
	Hardware + Hypervisor

--Running multiple apps on a single VM

Dedicated server
	Application code
	Dependencies
	Kernel
	Hardware
x 2

Virtual machine
	App1 App2
	Dependencies
	Kernel
	Hardware + Hypervisor


--The VM-centric way to solve this problem

Virtual machine
	Application code
	Dependencies
	Kernel
	
Virtual machine
	Application code
	Dependencies
	Kernel

Hardware + Hypervisor


--User space abstraction and containers

Virtual machine
	
	Container:
	- Application
	- User Space
	- Dependencies

	Container:
	- Application
	- User Space
	- Dependencies

	Container Runtime
	Kernel
	Harware + Hypervisor

Image: Application + Dependency

Containers use a varied set of Linux technologies:
	Processes
	Linux namespaces
	cgroups
	Union fine systems

Esempio di Dockerfile
	
	FROM ubuntu:18.04
	COPY ./app
	RUN make /app
	CMD python /app/pp.py

Cloud Build
	
	from:
	- Cloud Storage
	- git repository
	- Cloud Source Repositories

	to:
	. Cloud Functions
	. GKE
	. App Engine

You can upload the container in the Container Registry.

_________________________________________________________
LAB - Working with Cloud Build

Controllare che 
Cloud Build API e Container Registry API siano abilitati
cercando in:
Navigation menu -> APIs & Services -> Dashboard -> Enable APIs and Services

+ Building containers with Dockerfile and Cloud Build
da Cloud Shell:


> nano quickstart.sh
#!/bin/sh
echo "Hello, world! The time is $(date)."

Save the file and close nano by pressing the CTRL+X key, then press Y and Enter.


> nano Dockerfile
FROM alpine
COPY quickstart.sh /
CMD ["/quickstart.sh"]

Save the file and close nano by pressing the CTRL+X key, then press Y and Enter.

In Cloud Shell, run the following command to make the quickstart.sh script executable.

> chmod +x quickstart.sh

In Cloud Shell, run the following command to build the Docker container image in Cloud Build.

> gcloud builds submit --tag gcr.io/${GOOGLE_CLOUD_PROJECT}/quickstart-image .

Don't miss the dot (".") at the end of the command. The dot specifies that the source code is in the current working directory at build time.


In the Google Cloud Console, on the Navigation menu (Navigation menu), click Container Registry > Images.

The quickstart-image Docker image appears in the list


---
+ Building Containers with a build configuration file and Cloud Build

Cloud Build also supports custom build configuration files. In this task you will incorporate an existing Docker container using a custom YAML-formatted build file with Cloud Build.

In Cloud Shell enter the following command to clone the repository to the lab Cloud Shell.

> git clone https://github.com/GoogleCloudPlatform/training-data-analyst

Create a soft link as a shortcut to the working directory.

> ln -s ~/training-data-analyst/courses/ak8s/v1.1 ~/ak8s

Change to the directory that contains the sample files for this lab.

> cd ~/ak8s/Cloud_Build/a

A sample custom cloud build configuration file called cloudbuild.yaml has been provided for you in this directory as well as copies of the Dockerfile and the quickstart.sh script you created in the first task.

In Cloud Shell, execute the following command to view the contents of cloudbuild.yaml.

> cat cloudbuild.yaml

You will see the following:

steps:
- name: 'gcr.io/cloud-builders/docker'
  args: [ 'build', '-t', 'gcr.io/$PROJECT_ID/quickstart-image', '.' ]
images:
- 'gcr.io/$PROJECT_ID/quickstart-image'


This file instructs Cloud Build to use Docker to build an image using the Dockerfile specification in the current local directory, tag it with 

gcr.io/$PROJECT_ID/quickstart-image 

($PROJECT_ID is a substitution variable automatically populated by Cloud Build with the project ID of the associated project) and then push that image to Container Registry.

In Cloud Shell, execute the following command to start a Cloud Build using cloudbuild.yaml as the build configuration file:

> gcloud builds submit --config cloudbuild.yaml .

The build output to Cloud Shell should be the same as before. When the build completes, a new version of the same image is pushed to Container Registry.

In the Google Cloud Console, on the Navigation menu (Navigation menu), click Container Registry > Images and then click quickstart-image.

Two versions of quickstart-image are now in the list.

In the Google Cloud Console, on the Navigation menu (Navigation menu), click Cloud Build > History.

Two builds appear in the list.

Click the build ID for the build at the top of the list.

The details of the build, including the build log, are displayed.

+ Building and Testing Containers with a build configuration file and Cloud Build

The true power of custom build configuration files is their ability to perform other actions, in parallel or in sequence, in addition to simply building containers: running tests on your newly built containers, pushing them to various destinations, and even deploying them to Kubernetes Engine. In this lab, we will see a simple example: a build configuration file that tests the container it built and reports the result to its calling environment.

In Cloud Shell, change to the directory that contains the sample files for this lab.

> cd ~/ak8s/Cloud_Build/b

As before, the quickstart.sh script and the a sample custom cloud build configuration file called cloudbuild.yaml has been provided for you in this directory. These have been slightly modified to demonstrate Cloud Build's ability to test the containers it has build. There is also a Dockerfile present, which is identical to the one used for the previous task.

In Cloud Shell, execute the following command to view the contents of cloudbuild.yaml.

> cat cloudbuild.yaml
You will see the following:

steps:
- name: 'gcr.io/cloud-builders/docker'
  args: [ 'build', '-t', 'gcr.io/$PROJECT_ID/quickstart-image', '.' ]
- name: 'gcr.io/$PROJECT_ID/quickstart-image'
  args: ['fail']
images:
- 'gcr.io/$PROJECT_ID/quickstart-image

In addition to its previous actions, this build configuration file runs the quickstart-image it has created. In this task, the quickstart.sh script has been modified so that it simulates a test failure when an argument ['fail'] is passed to it.

In Cloud Shell, execute the following command to start a Cloud Build using cloudbuild.yaml as the build configuration file:

> gcloud builds submit --config cloudbuild.yaml .

You will see output from the command that ends with text like this:

Output (do not copy)

Finished Step #1
ERROR
ERROR: build step 1 "gcr.io/ivil-charmer-227922klabs-gcp-49ab2930eea05/quickstart-image" failed: exit status 127
----------------------------------------------------------------------------------------------------------------------------------------------------------------
ERROR: (gcloud.builds.submit) build f3e94c28-fba4-4012-a419-48e90fca7491 completed with status "FAILURE"

Confirm that your command shell knows that the build failed:

> echo $?

The command will reply with a non-zero value. If you had embedded this build in a script, your script would be able to act up on the build's failure.

End your lab
____________________________________________________________

	CONTAINER MANAGEMENT

Kubernetes is a container centric management environment.
Google originated id and then donated it to the open source community.
Now it's a project of the vendor neutral Cloud Native Computing Foundation.

It automates the deployment, scaling, load balancing,
logging, monitoring, and other management features of
containerized applications.
These are the features that are characteristic of a typical
Platform as a Service (PaaS) solutions.

Kubernetes also facilitates the fewatures of an infrastucture as a service,
souch as allowing a wide range of user preferences and configuration flexibility.

Kubernetes supports declarative configurations.
When you administer your infrastructure declaratively,
you describe the desired state you want to achieve,
instead of issuing a series of commands to achieve the desired state.

Kubernete's job is to make the deployed system conform to
your desired state and the keep it there in spite of failures.
Declarative configuration saves you work,
because the system is desired state is always documented.
It also reduces the risk of error.

Communities also allows imperative configuration in which
you issue commands to change the system state,
but administering kubernetes that scale imeratively
will be a big missed opportunity.

One of he primary strengths of kubernetes is its ability
to automatically keep a system in a stat that you declare.
Experience Kubernetes administators use imperative configuration
only for quick temporary fixes and as a tool in building a declarative configuration.

	KUBERNETES FEATURES
1- Supports both stateful and stateless applications,
	batch jobs and deamon tasks.
2- Autoscaling
3- Resource limits
4- Extensibility
5- Portability

Introducing to Google Kubernetes Engine

Kubernets is powerful, but managing the infrastructure is a full-time job.

Google Kubernets Engine is a managed service for Kubernetes within Google Cloud.

GKE will help you deploy, manage and scale kubernetes environments
for your containerized application on GCP.
More specifically, GKE is a component of the GCP compute offering,
it makes i easy to bring your kubernetes workloads into the cloud.

GKE is fully managed,
which means that you don't have to provision the underlying resources.
GKE uses a container optimize operating system.
These operating systems are maintained by Google,
are optimized to scale quickly and with a minimal resource footprint.

When you use GKE,
you start by directing the service to instantiate a kubernetes system for you.
This system is called a cluster.
GKE is auto upgrade feature is enabled to ensure that your clusters are automatically upgraded with 
the latest and greatest version of kubernetes.

The virtual machines that host your containers inside of the GKE cluster are called NODES.
If you enable GKE auto repair feature,
the service will automatically repair unhealthy nodes for you.
It will make periodic health checks on each node in the cluster.
If a node is determined to be unhealthy and requires repair,
GKE will drain the node.
In other words, it will cause its workload to gracefully exit
and then recreate that node.

Just as kubernetes supprts scaling workloads,
GKE supports scaling the cluster itself.
GKE seamlessly integrates with Google Cloud Build
and Container Registry.
This allows you to automate deployment using private container images 
that you've securely stored in contaner registry.

GKE also integrates with Google's Identity and Access Management (IAM),
which allows you to control access through the use of the accounts and role permissions.

Stackdriver is Google Cloud System for monitoring and management for services, containers,  applications and infrastructure.
GKE integrates with Stackdriver monitoring to help you understand your application's performance.
GKE is integradet with Google Virtual Private Cloud (VPC)
and makes use of GCO networking features.

The GCP Console provides insights into GKE clusters
and resources and allows you to view inspect and delete resources in those clusters.

You might be aware that open source kubernetes contains a dashboard, 
it takes a lot of work to set it up securely
but the GCP Console is a Dashboard for your GKE clusters and workloads
that you don't have to manage
and it's more powerful that the kubernetes dashboard.


	Comparing Google Cloud computing solutions

Compute Engine: IaaS
	fully customizable virtual machines
	persistent disks and optiona local SSDs
	global load balancing and autoscaling
	per-second billing
	use cases:
		1- complete control over the OS and virtual hardware
		2- well suited for lift-and-shift migrations to the cloud
		3- most flexible compute solution, often used when a managed solution is too restrictive
	--
	Long-lived VMs
	Physical servers
	One container per VM

GKE: Hybrid
	fully managed kubernete platform
	supports cluster scaling, persistent disks, automated upgrades and auto node repairs.
	built-in integration with Google Cloud services
	portability across multiple environments:
	- hibrid computing
	- multi-cloud computing
	use cases:
	1- containerized applications
	2- Cloud-native distributed systems
	3- Hybrid applications
	--
	rich administration of container workloads
	on-premises kubernetes

App Engine: PaaS
	provides a fully managed, code-first platform.
	stramlines application deployment and scalability
	provides support for popular programming languages and application runtimes
	you can also run container workloads
	supports integrated monitoring, logging,  diagnostics.(Stackdriver)
	simplifies version control, canary testing and rollbacks.
	use cases:
	1- website
	2- mobile app and gaming backends
	3- RESTful APIs
	--
	containers run by the service
	no-ops

Cloud Run: Stateless
	enables stateless containers
	abstracts away infrastructure management
	automatically scales up and down
	Open API and runtime environment
	use cases:
	1- deploy stateless containers that listen for requests or events
	2- build applications in any language using any frameworks and tools 
	--
	staeless container
	managed


Cloud Functions: Server logic
	event-driven, serverless compute service
	automatic scaling with highly available and fault-tolerant desing
	charges apply only when your code runs
	triggered based on events in Google Cloud services, HTTP endpoints and Firebase
	use cases:
	1- supporting microservice architecture
	2- serverless application backends
		mobile and IoT backends
		integrate with third-party services and APIs
	3- intelligent applications
		virtual assistant and chat bots
		video and image analysis
	--
	no-ops

_____________________________________________________________
	Kubernetes Concepts:

Kubernetes object model
Each thing Kubernetes managers is represented
by an object and you can view and change objects attributes and state.

Principle of declarative management
Kubernetes axpects you to tell it what you want the state 
of the objects under its management to be and it will work
to bring that state into being and keep it there.

A Kubernetes object is defined as a persistent entity that
represent the state of something running in a cluster:
its desired state and its current state.

Various kinds of objects represent containerized applications,
the resources that are available to them and the policies that
affect their behavior.

Kubernetes objects have two important elements:
1- Objec spec (object specification)
	desired state described by us
2- Object status
	current state described by Kubernetes

Containers in a Pod share resources
In a Pod there are:
	Shared networking
	Containers
	Shared storage

Kubernetes assigns each Pod a unique IP addess
Every container within a Pod shares the network name space
including IP address and net reports.
Containers within the same Pod can communicate through
localhost: 127.0.0.1
Pod can also specify a set of storage volumes to be shared 
amongst his containers.

Example
Running three nginx containers in a single Pod
	You want three nginx containers running all the time
	You declare objects that represent those containers
	Kubernetes launghes those objects and maintains them

Desired state compared to current state
	Desired state (Kubernetes objects)
	Current state
	Kubernetes Control Plane, Remediation actions


++Cooperationg processes make a Kubernetes cluster work
________________________
Cluster
	Control plane (VM)
		kube-APIserver <- kubectl
			<- etcd
			<- kube-scheduler
			<- kube-controller-manager
			<- kube-cloud-manager
	Node (VM)
		Kubelet -> kube-APIserver
		Kube-proxy
________________________

The single componet that you interact with derectly
is called the Kube-APIserver.
This componet's job is to accept commands that view or chage the
state of the cluster including launghing pods.
In the specialization,
you will use the "kubectl" command frequently.
This command's job is to connect to the kube app server
and communicate with using the kubernetes API.
kube-APIserver also authenticates incoming requests,
determines whether they are authorized and valid 
and managers admission control.
But it's not just kubectl that talks with kube-APIserver,
in fact any query or change to at the cluster state must be addressed
to the kube-APIserver.

etcd is the clusters database.
Its job is to reliably store the state of the cluster.
This includes all of the cluster configuration data and more 
dynamic information such as 
what nodes are part of the cluster,
what Pods should be running,
where they should be running.

You'll never interact directly with etcd.
Instead kube-APIserver interacts with the database
on behalf of the rest of the system.

Kube-scheduler is responsible for scheduling Pods onto nodes.
To do that, it evaluates the requirements of each individual Pod
and selectiong which node is most suitable
but it doesn't do the work of actually lauching tho Pods
on the nodes.
Instead, whenever it discover a Pod object that doesn't yet have
an assignment to a node,
it chooses a node and simply writes the name of that node into the pod object.

Another component of the system is reponsible then
for launching the departs and you'll see it very soon.
But how does kube-scheduler decide where to run a Pod,
it knows the stat of all node and it will obey contraints
that you define on where a pod may run based on hardware, software and policy.

For example, you might specify a certain Pod is only allowed run on nodes with a certain amount of memory.
You can also define affinity specifications,
which calls groups of Pods to prefer running on the same node.
Or anti affinity specifications, 
which ensure that Pods do not run on the same node..

kube-controller-manager has a broader job,
it continuously monitors the state of the cluter through
kube-APIserver.
Whenever the current state of the cluster doesn't match the
desired state, kube-controller-manager will attempt to make
changes to achieve the desired state.
It's called the controller-manager because many kubernetes objects
are managed by loops of code called controllers.
These loops of code handled the process of remediation.
Controllers will be very useful for you.
To be specific,
you'll learn to use cerain kinds of kubernetes controllers to manage workloads.

For example,
remember our problem of keeping three nginx Pods always running,
we can gather them together into a controller object
called a deployment that not only keeps the running,
but also lets us scale them and bing the together under the frontend.

Other kinds of controllers have system level responsibilities,
for example, node controller's job is to monitor and respond
when a node is offline.
kube-cloud-manager manages controllers that interact with
the underlying cloud providers.
For example,
if you manually launch a kubernetes cluster 
on Google Compute Engine, kube-cloud-manager would be 
responsible for bringing in Google Cloud features like
Load Balancers and storage volumes
when you need them.

Each node runs a small family of control plane components to.
For example, each node runs a kubelet.
You can thing of a kubelet as kubernetes agent on each node,
when the kube-APIserver wants to start a Pod on a node,
it connects to that nodes kubelet.

kubelet uses the container runtime to start the pod
and monitors its lifecycle, 
including readiness and liveliness probes
and reports back to kube-APIserver.

The container runtime is the software that knows
how to launch a container from a container image.
The world of kubernetes offers several choices for containers
run times, but the Linux distribution that GKE uses for its node
launches containers using containerd (https://containerd.io/),
the runtime component of Docker.

Kube-proxy's job is to maintain the network connectivity
among the Pods in the cluster.
In open source kubernetes it does so
using the firewall capabilities of IP tables which are built
into the Linux kernel.


Google Kubernetes Engine Concepts

GKE manages all of the control playing components for us.
It still exposes an IP address to which we send all of our kubernetes API requests,
but GKE takes the responsibility for provisioning and managing all of the control plane infrastructure behind it.
It also abstracs away having a separate control plane.
The responsibilities of the control plane are absorbed by Google Cloud and you were not separately billed for your control plane.

In any Kubernetes environments nodes are created externally by cluster administrators, not by kubernetes itself.
GKE automates this process for you,
it launches Compute Engine Virtual Machine instances and registers them as nodes.
You can manage no settings directly from the Cloud Console.
You pay per hour of life of your nodes,
not counting the control plane. 
Because nodes running Compute Engine,
you can choose your node machine type when you create your cluster.
By default, the node machine type is n1-standard-1,
which provide one virtual CPU and 3.75 gb of memory.
Google Cloud offers a wide variety of Compute Engine options.

You can customize the virtual machine.

Use node pools to mnage different kinds of nodes.

You can also select multiple node machine types
by creating multiple node pools.
A node pool is a subset of nodes within a cluster that share a configuration,
such as the amount of memory, their CPU generation.
Node pools also provide an easy way to ensure that 
workloads run on the right hardware within your cluster,
you just label them with a desired node pool.
Node pools are GKE feature rather than a kubernetes feature.
You can build an analogist mechanism
within open source kubernetes
ut you'd have to maintain it yourself,
you can enable automatic node upgrades,
automatica node repairs and cluster autoscaling
at this node pool level.
Here's a word of caution.

Some of each node CPU and memory are needed to run 
the GKE and kubernetes component that let it work as part of your cluster.
For example,
if you allocate nodes with 15GB of memory, 
not quite all of that 15GB will be available for use by pods.

Zonal versus regional clusters
__________________________________________
	ZONAL CLUSTER
Zone
	Cluster
		Control plane
		Nodes
__________________________________________
	REGIONAL CLUSTER
Region
	Cluster
		Zone
			Control plane
			Nodes
		Zone
			Contrl plane
			Nodes
		Zone
			Contrl plane
			Nodes
___________________________________________			


By default a cluster launches on a single Google Cloud 
compute zone with three identical nodes, all in one node pool.
The number of nodes can be changed during or after
the creation of the cluster.
Adding more nodes and deploying multiple replicas of an application will improve an application availability
but only up to a point.

What happens if an entire compute zone goes down?
You can address this concern by using a GKE regional cluster.
Regional clusters have a single API endpoint for the cluster.
However, its control planes and nodes are spread across multiple Compute Engine zones within a region.

Regional cluster ensure that the availability of the application
is maintained across multiple zones in a single region.
In addigion, the availability of the control plane
is also maintained, so the both applications
and manaement functionality can withstand the loss of one
or more but not all zones.

By default, a regional cluster is spread across three zones,
each containing one control plane and three nodes.
These numbers can be increased or decreased.

For example, if you have five nodes in zone one,
you will exactly the same number of nodes 
for each of the other zones.
For a total of 15 nodes.

Once you build a zonal cluster,
you cannot convert it into a regiona cluster or vice versa.

A regional or zonal GKE cluster can also be set up as a private cluster.
The entire cluster that is the control plane
and its nodes are hidden from the public internet.
Cluster control plane can be accessed by Google Cloud products such as Cloud Logging or Cloud Monitoring
through an internal IP address.
They also can be accessed by authorized networks,
through external IP address.
Authorized networks are basically IP address reanges
that are trusted to access to control plane.
In addition, nodes can have a limited outbound access through private Google access,
which allows them to communicate with other Google Cloud services.

For example, nodes can pull container images from Google container registry without needing external IP addresses.


	Kubernetes Object Management

All kubernetes objects are identified by a unique name
and a unique identifier.

Example: running three nginx containers.

Il primo modo che abbiamo è definire i tre pod:

Objects are defined in a YAML file.

--
apiVersion: apps/v1
kind: Pod
metadata:
	name: nginx
	labels:
		app: nginx
spec:
	containers:
	- name: nginx
	  image: nginx:latest
--
This YAML file defines a desired state for a pod,
its name and a specific container image for it to run. 

Your manifest files have certain required fields.

"apiVersion" describes which kubernetes API version is used
to create the object,
the kubernetes protocols version so as to help maintain
backwards compatibility.

"kind" defines the object you want,
in this case a pod.

"metadata" helps identify the object using name, unique id and on optional name space.
You can define several related objects in the same YAML file
and it is a best practive to do so.

One file is often easier to manage than several.

Another even more important tip,
you should save your YAML file in version controlled repositories.
This practice makes it easier to track and manage changes 
and to back out those changes when necessary.
It's also a big help when you need to recreate or restore a cluster.

Many GCP custemers use Cloud Source repositories for this purpose, because that service lets them control the permissions of those files in the same way as their other GCP resources.

When you create a kubernetes object,
you name it with a string,
names must be unique.
Only one object of a particular kind can have a particlulare name at the same time in a kubernetes name space.

However, if an object is deleted, its name can be reused.

Each object generated throughout the life of a cluster
has unique id generated by kubernetes.
This means that no two objects will have tha same "uid"
throughout the life of a cluster.

"labels" are key-vale pairs with which you tag your objects during or after their creating.
Labes help you identify and organize objects and subsets of objects.

--
apiVersion: apps/v1
kind: Deployment
metadata:
  name: nginx
  labels:
    app: nginx
    env: dev
    stack: frontend
spec:
  replicas: 3
  selector:
    matchLabels
    app: ninx
--

Labels can be matched by label selectors

Admin issued a command
> kubectl get pods -selector=app=nginx

One way to bring three nginx web server into being,
would be to declare three pod objecs,
each with its own section of YAML.

kubernetes default scheduling alghorithm prefers to spread
the workload evenly across the nodes available to it
so we'd get a situation like this one looks good.

Pods have a life cycle:
born -> running -> broken -> dies

++Pods and Controller Objects

Controller object types:
- Deployment
- StatefulSet
- DaemonSet
- Job








